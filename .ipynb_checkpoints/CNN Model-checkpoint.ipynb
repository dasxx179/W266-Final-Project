{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline CNN Model Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import collections\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "from sklearn.utils import shuffle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import pickle\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "from model import ConvNet, Net\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseTrainData(topicLabels, df):\n",
    "    data = {}\n",
    "    labels = {}\n",
    "    \n",
    "    for dataType in ['train']:\n",
    "        data[dataType] = {}\n",
    "        labels[dataType] = {}\n",
    "        \n",
    "        for topic in topicLabels:\n",
    "            data[dataType][topic] = []\n",
    "            labels[dataType][topic] = []\n",
    "            \n",
    "            for idx, sentence in df.iterrows(): # iterates through each sentence in df we passed\n",
    "                if sentence.category == topic:\n",
    "                    data[dataType][topic].append(sentence.text)\n",
    "                    labels[dataType][topic].append(topicLabels[sentence.category]) # assigns it the value in our dict of topicLabels based on each category\n",
    "            \n",
    "            assert len(data[dataType][topic]) == len(labels[dataType][topic]), \\\n",
    "                    \"{}/{} data size does not match labels size\".format(data_type, topic)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we have in read in the raw training and testing data and formatted it properly, and now we are going to shuffle the records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareTrainData(data, labels):\n",
    "    \"\"\"Prepare training set from L3 Data\"\"\"\n",
    "    \n",
    "    # Combine pro and anti sentences and labels\n",
    "    dataTrain = data['train']['pro-immigration'] + data['train']['anti-immigration'] + data['train']['pro-guns'] + data['train']['anti-guns'] + data['train']['pro-medicare'] + data['train']['oppose-medicare']\n",
    "    labelsTrain = labels['train']['pro-immigration'] + labels['train']['anti-immigration'] + labels['train']['pro-guns'] + labels['train']['anti-guns'] + labels['train']['pro-medicare'] + labels['train']['oppose-medicare']\n",
    "    \n",
    "    # Shuffle sentences and the corresponding labels within the training data\n",
    "    dataTrain, labelsTrain = shuffle(dataTrain, labelsTrain)\n",
    "    \n",
    "    # Return shuffled training data and training labels\n",
    "    return dataTrain, labelsTrain\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareTestData(candidates):\n",
    "    testDict = {}\n",
    "    for candidate in candidates:\n",
    "        df = pd.read_csv('./data/' + candidate + '_cleaned.csv')\n",
    "        convertToList = df.text.to_list()\n",
    "        wordsTest = [sentenceToWords(sentence) for sentence in convertToList]\n",
    "        testDict[candidate] = wordsTest\n",
    "    return testDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to tokenize our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenceToWords(sentence):\n",
    "    nltk.download(\"stopwords\", quiet = True)\n",
    "    # stemmer = PorterStemmer()\n",
    "    \n",
    "    sentence = re.sub(r\"[^a-zA-Z0-9]\", \" \", sentence.lower()) # Convert text to lower case\n",
    "    words = sentence.split() # Split string into words\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
    "    # words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will apply the above method to all of our data and cache the results as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cacheDir = os.path.join(\"../cache\", \"cnn_analysis\")  # where to store cache files\n",
    "os.makedirs(cacheDir, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "def preprocessData(dataTrain, labelsTrain, cacheDir = cacheDir, cacheFile = \"preprocessedData.pkl\"):\n",
    "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
    "    \n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cacheData = None\n",
    "    if cacheFile is not None:\n",
    "        try:\n",
    "            pass\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "        \n",
    "      # If cache is missing, then do the heavy lifting\n",
    "    if cacheData is None:\n",
    "        # Preprocess training and test data to obtain words for each review\n",
    "        # words_train = list(map(review_to_words, data_train))\n",
    "        # words_test = list(map(review_to_words, data_test))\n",
    "        wordsTrain = [sentenceToWords(sentence) for sentence in dataTrain]\n",
    "        \n",
    "        # Write to cache file for future runs\n",
    "        if cacheFile is not None:\n",
    "            cacheData = dict(wordsTrain=wordsTrain, labelsTrain=labelsTrain)\n",
    "            \n",
    "            with open(os.path.join(cacheDir, cacheFile), \"wb\") as f:\n",
    "                pickle.dump(cacheData, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", cacheFile)\n",
    "        \n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        wordsTrain, labelsTrain = (cacheData['wordsTrain'], cacheData['labelsTrain'])\n",
    "        \n",
    "    return wordsTrain, labelsTrain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will construct a feature representation that represents each word as an integer and include the words that appear most frequently.  We will combine all the infrequent words into another category by itself.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDict(data, vocabSize = 50000):\n",
    "    \"\"\"Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer.\"\"\"\n",
    "    # Determine how often each word appears in `data`. Note that `data` is a list of sentences and that a\n",
    "    # sentence is a list of words.\n",
    "    \n",
    "    # A dict storing the words that appear in the reviews along with how often they occur\n",
    "    wordCount = {}\n",
    "    for sentence in data:\n",
    "        for word in sentence:\n",
    "            wordCount[word] = wordCount[word] + 1 if word in wordCount else 1\n",
    "            \n",
    "    # Sort the words found in `data` so that sorted_words[0] is the most frequently appearing word and\n",
    "    # sorted_words[-1] is the least frequently appearing word. \n",
    "    wordCountSorted = sorted(wordCount.items(), key=(lambda item: item[1]), reverse=True)\n",
    "    sortedWords = [item[0] for item in wordCountSorted]\n",
    "    \n",
    "    # This is what we are building, a dictionary that translates words into integers\n",
    "    wordDict = {}\n",
    "    for idx, word in enumerate(sortedWords[:vocabSize - 2]): # The -2 is so that we save room for the 'no word'\n",
    "        wordDict[word] = idx + 2\n",
    "    return wordDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our word dictionary, so let's convert our sentences to integer sequence representation and pad our results to a fixed length.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertAndPad(wordDict, sentence, pad = 500):\n",
    "    NOWORD = 0 # We will use 0 to represent the 'no word' category\n",
    "    INFREQ = 1  # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict\n",
    "    \n",
    "    workingSentence = [NOWORD] * pad\n",
    "    \n",
    "    for wordIdx, word in enumerate(sentence[:pad]):\n",
    "        if word in wordDict:\n",
    "            workingSentence[wordIdx] = wordDict[word]\n",
    "        else:\n",
    "            workingSentence[wordIdx] = INFREQ\n",
    "        \n",
    "    return workingSentence, min(len(sentence), pad)\n",
    "\n",
    "def convertAndPadData(wordDict, data, pad = 500):\n",
    "    result = []\n",
    "    lengths = []\n",
    "    \n",
    "    for sentence in data:\n",
    "        converted, leng = convertAndPad(wordDict, sentence, pad)\n",
    "        result.append(converted)\n",
    "        lengths.append(leng)\n",
    "    \n",
    "    return np.array(result), np.array(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Train the PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainLoader, epochs, optimizer, criterion, device):\n",
    "    totalStep = len(trainLoader)\n",
    "    for epoch in range(epochs):\n",
    "        for idx, (data, labels) in enumerate(trainLoader):\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (id+1) % 100 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                       .format(epoch+1, epochs, id+1, totalStep, loss.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the Model for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Model for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function for Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Immigration articles: train = 900 pro / 385 anti\n",
      "Immigration articles: train = 759 pro / 745 anti\n",
      "Immigration articles: train = 673 pro / 639 anti\n",
      "Counter({0: 900, 2: 759, 3: 745, 4: 673, 5: 639, 1: 385})\n",
      "Sentences (combined): train = 4101\n",
      "That does not, of course, mean that farmers in Kansas and Colorado and California should be stripped of their ability to keep a firearm for pests or hunting on their property.\n",
      "['along', 'overflowing', 'access', 'guns', 'honduras', 'remain', 'high', 'death', 'rates', 'gun', 'violence']\n",
      "Wrote preprocessed data to cache file: preprocessedData.pkl\n",
      "['gun', 'would', 'health', 'immigrants', 'medicare']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight 6 3 5 5, but got 2-dimensional input of size [50, 501] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-24beb7f3a624>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainSampleDL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-24beb7f3a624>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainSampleDL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-938b31e40fdb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, trainLoader, epochs, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Berkeley/W266/W266-Final-Project/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight 6 3 5 5, but got 2-dimensional input of size [50, 501] instead"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Initial Variables\n",
    "    topicLabels = {\n",
    "        'pro-immigration': 0,\n",
    "        'anti-immigration': 1,\n",
    "        'pro-guns': 2,\n",
    "        'anti-guns': 3,\n",
    "        'pro-medicare': 4,\n",
    "        'oppose-medicare': 5\n",
    "    }\n",
    "    candidates = ['Biden', 'Buttigieg', 'Klobuchar', 'Sanders', 'Warren', 'Yang']\n",
    "    Out.clear()\n",
    "    \n",
    "    data, labels = parseTrainData(topicLabels, df = pd.read_csv('./data/L3train_nonOneHot.csv'))\n",
    "    \n",
    "    # Let's check out the length of our training data for each category\n",
    "    print(\"Immigration articles: train = {} pro / {} anti\".format(\n",
    "            len(data['train']['pro-immigration']), len(data['train']['anti-immigration'])))\n",
    "    \n",
    "    print(\"Immigration articles: train = {} pro / {} anti\".format(\n",
    "            len(data['train']['pro-guns']), len(data['train']['anti-guns'])))\n",
    "    \n",
    "    print(\"Immigration articles: train = {} pro / {} anti\".format(\n",
    "            len(data['train']['pro-medicare']), len(data['train']['oppose-medicare'])))\n",
    "    \n",
    "    # Below is our prepared shuffled training data\n",
    "    trainX, trainY = prepareTrainData(data, labels)\n",
    "    print(collections.Counter(trainY))\n",
    "    # Below is our test set\n",
    "    testX = prepareTestData(candidates)\n",
    "    \n",
    "    print(\"Sentences (combined): train = {}\".format(len(trainX)))\n",
    "    \n",
    "    # Let's go check out an example from our training data now\n",
    "    print(trainX[100])\n",
    "    \n",
    "    # Applying the tokenizer to get the stems\n",
    "    print(sentenceToWords(trainX[1]))\n",
    "    # Now we preprocess the Data\n",
    "    trainX, trainY = preprocessData(trainX, trainY)\n",
    "    # Build our word Dict\n",
    "    wordDict = buildDict(trainX)\n",
    "    print(list(wordDict.keys())[:5])\n",
    "    \n",
    "    # Save our wordDict\n",
    "    dataDir = '../data/pytorch' # folder to store our data\n",
    "    if not os.path.exists(dataDir): # check to make sure folder exists\n",
    "        print(\"making dataDir folder...\")\n",
    "        os.makedirs(dataDir)\n",
    "    \n",
    "    with open(os.path.join(dataDir, 'wordDict.pkl'), 'wb') as f:\n",
    "        pickle.dump(wordDict, f)\n",
    "    \n",
    "    # Now we pad all the sentences in our training data\n",
    "    trainXNum, trainXLen = convertAndPadData(wordDict, trainX)\n",
    "    \n",
    "    # Pad all the sentences in our testing data?\n",
    "    \n",
    "    # Save our data locally\n",
    "    pd.concat([pd.DataFrame(trainY), pd.DataFrame(trainXLen), pd.DataFrame(trainXNum)], axis=1) \\\n",
    "        .to_csv(os.path.join(dataDir, 'train.csv'), header=False, index=False)\n",
    "    \n",
    "    # Let's load a small portion of our training dataset for testing\n",
    "    trainSample = pd.read_csv(os.path.join(dataDir, 'train.csv'), header=None, names=None, nrows=250)\n",
    "    \n",
    "    # Turn input df into tensors\n",
    "    trainSampleY = torch.from_numpy(trainSample[[0]].values).float().squeeze()\n",
    "    trainSampleX = torch.from_numpy(trainSample.drop([0], axis = 1).values).long()\n",
    "    \n",
    "    # Build the dataset\n",
    "    trainSampleDS = torch.utils.data.TensorDataset(trainSampleX, trainSampleY)\n",
    "    # Build the dataloader\n",
    "    trainSampleDL = torch.utils.data.DataLoader(trainSampleDS, batch_size=50)\n",
    "    \n",
    "    # Train the model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ConvNet(10).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    train(model, trainSampleDL, 5, optimizer, criterion, device)\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
