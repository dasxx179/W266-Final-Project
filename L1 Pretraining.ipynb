{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the L1 data and putting it into a text file. Replace newline characters with a space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"s3://training-bert/corpus.csv\", nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     It is rather hard to write with eclipse glasse...\n",
       "1     As our nation celebrated Labor Day, giving att...\n",
       "2     We hear quite a bit about survivors’ guilt the...\n",
       "3     The study of the Hebrew language did not come ...\n",
       "4     Social media is blowing up about a culture of ...\n",
       "5     Many preachers took up the Isaiah 55 passage t...\n",
       "6     A trip to the cardiologist is rarely routine. ...\n",
       "7     **The author has been participating with QC Fa...\n",
       "8     Enderly Park is blistering under an unseasonab...\n",
       "9     Enderly Park is blistering under an unseasonab...\n",
       "10    By Nell Green Glory to God in the highest heav...\n",
       "11    Enderly Park is blistering under an unseasonab...\n",
       "12    “If we have no peace, it is because we have fo...\n",
       "13    By Greg and Helms Jarrell Chinua Achebe’s clas...\n",
       "14    Can we feel that there are human beings, very ...\n",
       "15    Flowing like a pink river, scores of women in ...\n",
       "16    Enderly Park is blistering under an unseasonab...\n",
       "17    Enderly Park is blistering under an unseasonab...\n",
       "18    Enderly Park is blistering under an unseasonab...\n",
       "19    Fox News personality Todd Starnes attributed D...\n",
       "20    Canadian Baptists joined other Christian leade...\n",
       "21    Baptists in Puerto Rico have withdrawn support...\n",
       "22    Franklin Graham is dead wrong about immigratio...\n",
       "23    Enderly Park is blistering under an unseasonab...\n",
       "24    The recent uprising in Baltimore has occupied ...\n",
       "25    Enderly Park is blistering under an unseasonab...\n",
       "26    Enderly Park is blistering under an unseasonab...\n",
       "27    Enderly Park is blistering under an unseasonab...\n",
       "28    In my city, Charlotte, N.C., we have reached a...\n",
       "29    The announcement came on Saturday. Just three ...\n",
       "                            ...                        \n",
       "70    Sociologist Nancy Ammerman’s first book was ab...\n",
       "71    QC Family Tree, founded by Greg and Helms Jarr...\n",
       "72    It was not by accident that we chose “Faith & ...\n",
       "73    The concept of being an “illegal” immigrant pr...\n",
       "74    Letters to the Editor\\n\\nWe welcome letters fr...\n",
       "75    When done from an unhealthy place, these new a...\n",
       "76    All photos taken in this photo gallery of QC F...\n",
       "77    Cornelia Hagens, volunteer with the QC Family ...\n",
       "78    Enderly Park is blistering under an unseasonab...\n",
       "79    With little opportunity for youth and children...\n",
       "80    Helms Jarrell, co-director of the QC Family Tr...\n",
       "81    Graham, son of the famous evangelist Billy Gra...\n",
       "82    With little opportunity for youth and children...\n",
       "83    On an intense weeklong trip through the Holy L...\n",
       "84    An English version is available here.\\n\\nLa se...\n",
       "85    The bustle of the past few weeks slows today. ...\n",
       "86    The U.S. Supreme Court has agreed to accept a ...\n",
       "87    The head of the Baptist World Alliance has cri...\n",
       "88    Enderly Park is blistering under an unseasonab...\n",
       "89    Every day Baptist News Global staff works hard...\n",
       "90    Tony disappears behind the abandoned house at ...\n",
       "91    In response to growing revelations of sexual a...\n",
       "92    LGBTQ individuals and movements have become in...\n",
       "93    The bustle of the past few weeks slows today. ...\n",
       "94    Where are the churches willing to model vulner...\n",
       "95    Divorce hurts children. My grandparents’ divor...\n",
       "96    If people are really committed to biblical law...\n",
       "97    With little opportunity for youth and children...\n",
       "98    Besides growing a church with new converts to ...\n",
       "99    A few weeks ago, I saw the renderings of a new...\n",
       "Name: content, Length: 100, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate these results into one file\n",
    "s = df.content.str.cat(sep=' ')\n",
    "s = s.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is rather hard to write with eclipse glasses on this morning, but “I want to be ready,” as the old spiritual intones. Not ever known for culinary process, I even tried to turn Oreos into eclipse cookies. Massive fail!  Like many others, I have anticipated this day with a sense of foreboding and wonder. The foreboding is a lot less than the wonder, to be sure. The foreboding in my case has to do with religious quacks that claim a natural scientific event is an act of God’s judgment or an apocalyptic event signaling the end of time. That this portent, which happens quite regularly, by the way, has to haul this fake theological freight creates a stumbling block to coherent belief. It is a responsible perspective to hold a worldview that attends to the wisdom of both theology and science.  Another of these celestial alignments will happen in 2024, and I suspect many of the same arguments will be in vogue. It will be an election year, so beware! Some will proclaim that God is judging a “godless nation” by blotting out the sun, and who knows but this might be the very last chance we have to repent. Please understand, I am all for repentance — deep, daily and determined. God’s call always beckons us to turn around to traverse a more life-giving pathway, but I demur at the heated rhetoric of an interventionist theology that only sees God active in the cataclysmic.  God is at work in the regularity of seasons, circadian rhythms and evolutionary trajectories. The old language of calling a brutal storm or other inexplicable event an “act of God” is receding, thankfully.  I do recall when Yorkminster Cathedral was hit by lightning in 1980 and insurance adjustors and religious figures debated whether to call it a divine action. Some concluded that it was because of a liberal bishop; others were less convinced. It is hard to believe that this language persists.  Better to see the whole of creation within providential care, even if it is not a tightly controlled oversight. Terrible and beautiful things occur, often all mixed up together. God is not absent from any of them, and the presence of suffering provokes interrogation of a benevolent deity. Human participation in the unfinished, groaning creation sometimes wreaks more havoc; it is hard to calculate the extent of our influence for good or evil.  Wonder far outstrips foreboding, however. I am thrilled by the fervor with which people have planned ahead to secure the best viewing spots. I delight in the ways this astronomical occasion provides teachers with an expansive scientific lesson. I relish the new learning about animal and crop behavior during a solar eclipse, especially as this is a key concern for those of us who live in the plains. I chortle at the kids who tried out their glasses days in advance, just in case. I revel, also, in the capacity of humans to experience wonder. We were made for this, you know.  It was Rudolf Otto, a scholar of religion in the early 20th century, who wrote about the “idea of the holy,” contending that each one of us has an inherent response to the numinous realities of life. By numinous he meant a divine power at work that invites us to a sense of mystery, fear and, perhaps, worship. At the heart of the religious impulse is the sense of grandeur of God’s creation, the idea that something lies behind the phenomena of earthly processes. A pious Christian, Otto sought to defend belief from naturalistic tendencies, the argument that the whole of this world can be explained without a theistic hypothesis.  It is very instructive for human beings to experience themselves as both infinitesimal and significant, with an appropriate understanding of their place in the scale of our universe. This means we understand that we are latecomers, beneficiaries of a long history that preceded us. We are dependent on all the life forms that have gone before, as well as those that continue to companion humanity today. It also means that the glorious beauty that surrounds us points to a vast intelligence beyond our comprehension. Our recognition of both our frailty and magnificent vocation as God’s image bearers kindles wonder that God is mindful of us, as Psalm 8 states.  I hope our experience of this event illumines a spirit of contemplation as we marvel at God’s handiwork and the perdurance of an expanding universe. I also hope that we find ever more constructive ways to hold both science and faith in our heart. We must read both books — of Scripture and of nature. Holding them together makes for an informed sense of wonder.  Related opinion:  Genius hesitates, both in science and religion | Scott Dickison As our nation celebrated Labor Day, giving attention to the role and dignity of workers, we should also consider the role of human agency in accomplishing divine labor. Theologians always interrogate such things! Surely the work of Christians is more than simply fueling the engine of capitalism; meaningful work also participates in God’s intention for the world. Yet, determining how God is at work in this world is one of the hardest theological challenges.  Think about the urgent crises confronting us. People of faith pray for deliverance, trusting God to hold back the waters of the sea or help them elude their enemies pushing them over the border in Myanmar or rid them of the malignancy growing in their bodies or quell the rising tide of white supremacy. Fervent prayer may not create the conditions for which they pray; however, many continue to trust that God’s providence will prevail. We must ask: through what instrumentality?  Reading narratives of deliverance in Scripture evokes hope for God’s mighty acts to be victorious once again. Many preachers and Sunday school teachers have followed the lectionary texts from Exodus in this extended season after Pentecost. We have noted the trickery of Shiphrah and Puah, the resistance of Pharaoh’s daughter and Miriam, and God’s call of Moses. We have pondered the extended saga of Israel in Egypt, questioning why deliverance was long delayed.  In these early chapters, the writer declares that God has “heard their groaning,” and “remembered God’s covenant,” “seen the misery of the people,” and has “come down to rescue them from the power of Egypt.” The suffering of the people touches the heart of God, although God leans the plans for deliverance upon humans who are themselves part of the oppressed. The means by which God has come down to rescue does not seem very sturdy, and how God will be involved is at question.  God’s commission to Moses is for him to go to Pharaoh and “bring my people out of Egypt.” God’s promise is very simple: “I will be with you,” and the proof that it is truly God who sends him is this: “When you have brought the people out of Egypt, you shall all worship God here on this mountain” (Exodus 3:12). What? It is only after the liberation is accomplished that Moses will know who propelled him into this high stakes mission? Oh my!  I often hear persons wonder out loud why God does not work in our day as God worked in biblical times. It appears that God does indeed work in the same way, inviting people to shared responsibility for God’s handiwork, which we inhabit and stain and heal. I believe that God is always calling humanity to do the needed holy work and that God is the power behind the actions of those courageous enough to trust God.  In a world where things can go terribly wrong — such as the human evocation of climate change that wreaks havoc — God is using every means to mend creation. The incarnation of Jesus teaches that God’s primary means of conducting redemptive work is through a partnership with humans who were tasked at the beginning to tend God’s handiwork. A long, grinding and luminous history of evolution antedates the human arrival, albeit a particular stewardship is required of those whom God has granted dominion.  Kathryn Tanner reminds us that God works in history at a different level than humans. For Tanner, divine and human agency are not in competition with one another. Because God is not in the same order of being as creatures, God’s power is universally extended and is at work in all things. Thus, there is no zero-sum game that suggests the more God is at work, the less humans can do — and vice versa.  Tanner, rather, points us to a renewed vision of how the incarnation determines how divine and human agency can be at work in the same person, who is a paradigm for how God chooses to accomplish the divine purpose. She calls us to think about God as “gift giver,” who not only imbues the Christ with holy presence to transform the horizon of human hopes, but makes possible human participation in Christ toward the same goal of redemption. Her theological vision that Christ is the key to what God is doing everywhere in the world guides our thinking about how human work and godly work always interface. Through God’s humility, we are always ingredient to saving work.  In times of challenge, trusting that God is at work empowering humans to work for the good of all is reassuring. It also prompts courageous action. While it is common to think that we are waiting on God, actually both God and others are waiting on us. We hear quite a bit about survivors’ guilt these days. A neighbor’s house has a tree through the roof, while yours was spared. A soldier walks behind another; the one in the lead steps on a landmine, and the follower does not suffer loss of limbs. The hurricane skirts your hometown, and the adjoining county is hard hit. The rains come in due season to your land; not too far away drought is ravaging crops. Because you were born a Buddhist in Myanmar, you do not suffer the degradation of the Rohingya people.  Survivors may be grateful for the good fortune they experienced, but they are wise to consider at what cost to others. It is hard not to construct a “spared for a reason” narrative; however, it diminishes a sense of divine providence for those not spared. Even more horrific is the disregard for the value of lives of a different ethnicity. Survivors’ guilt is to be preferred over callous indifference, yet the sense of being specially “blessed” by God may lead to presumption.  On Sunday, the lesson from the Hebrew Bible was the story of Israel crossing the Red Sea with the army of Egypt in hot pursuit. Written obviously from the perspective of the liberated, the story is grisly in detailing how the Lord fought against Egypt on behalf of Israel. God instructs Moses, “Stretch out your hand over the sea, so that the water may come back upon the Egyptians, upon their chariots and chariot drivers” (Exodus 14:26). Moses follows this directive, and the entire army is drowned. Not one of them remained.  The rabbis have long struggled with this text. God’s preservation of the people of covenant seems to have blood cost. Their further travel through the wilderness brings them to the land of promise, which is already occupied, a major inconvenience for both Canaanites and Israelites. More violence will ensue.  Codifying this narrative of election in Scripture then extended to Christians, many of whom became supersessionist, i.e., arguing that now their election in Christ eclipsed the Jews. Even when the humbler Christian approach acknowledged that they have been grafted into God’s enduring covenant with Israel, this claim still argues for a preferential treatment that elevates this Judeo-Christian trajectory over against all the rest of the world’s people.  Doctrines of election are a delicate matter, and it is hard to understand God’s love of the whole world through the lens of particularity that the Exodus story reveals. Does divine favoritism not make it hard for outsiders to find their place in the story of liberation? Christians have always argued that the purpose of the Abrahamic tradition of covenant was to bless the nations, as well as the historic people of God.  Some rabbis have suggested that the covenant with Israel was an experiment in divine-human intimacy. Would a people trust the Holy One enough to follow instruction and live in faithful patterns of worship and justice? Could Israel’s experience of Exodus become paradigmatic of God’s salvific purpose for all?  As we struggled with this text in my Sunday school class, an older man posed this question: “So who are God’s people?” I responded, “Everyone.” The follow-up question in my mind was, “When life is so hard, how do they know God is for them, that they are God’s own?”  This is where survivors come in, it seems to me. Those who are spared must not simply rejoice in their seeming “chosenness,” but must use every resource to alleviate the suffering of others. Guilt may not be the best source of motivation, but if it spurs compassion, it is constructive.  Christians believe that we come to know God’s favor through Christ and that he is the key to what God is doing throughout the world. What if the means through which God favors the suffering comes through those who already know of the expanse of God’s election in him? As Kathryn Tanner writes, through Christ “God gives the world God’s very own life” and saves us by establishing “the closest possible relationship with us.” The divine favor resting upon Jesus is a gift for the whole world.  Just as blaming God for every cataclysmic event founders, so does expecting God to provide miraculously all the means of recovery. God always uses humanity in the redemptive project, and the only privilege Christians have is their responsibility for acting in the name of the one they have come to know in Jesus Christ.  This approach does not require triumphalistic swagger or a certitude that quashes other perceptions of truth. Rather, it requires the humility of those called to participate with God in bringing this world to its true end. There is so much healing work to be done, and it is urgent. The study of the Hebrew language did not come easily to me. Perhaps it was because I waited until my final year of seminary to take it; perhaps it was because it met at 3 p.m.; perhaps it was because I sat near a window; perhaps it was because our professor was so bored teaching at this elementary level — I am sure I can come up with some other excuses. I did pass, even making an A, but only because I memorized the Book of Jonah, a particular interest of our instructor.  As I have been teaching my way through Exodus with my Sunday school class, I certainly wish I had loved the Hebrew language more. I am sure that particular nuances elude me, yet translations do capture the richness of the narratives. Specifically, I want to know more about the issue of “testing.”  Exodus itself is a patchwork of stories, which were gathered and edited over many years. These stories are complex reflections on how God’s purpose will be accomplished with flawed human actors, and how human choices will impact the character of the divine. Wandering, while being led? It is very hard to map how the wilderness journey transpired, and clearly the people made the trek more difficult through obstinacy and lack of faith.  The wilderness was a dangerous place, and the way God through Moses led was open to suspicion. A frequent refrain of the congregation of Israel was, “Have you brought us out here to kill us?” Moses usually deflected and suggested that their complaint was against God, not him (Exodus 17:2). Because he claimed to be sent by God to shepherd this people, he had to take the heat. He was a convenient target — like our pastors.  God tested the people, and the people tested God. Neither seemed pleased about this abrasion in their relationship, yet it was an unavoidable reality as a covenant was being forged in a context of peril. Could God trust the people to follow the appointed leadership of Moses? Could the people trust that they were accompanied by God’s own presence? Their insistent question was, “Is the Lord among us or not?” We can only imagine the ways in which that same query is being voiced in the daily heart-rending devastations.  They found themselves at Rephidim without water, and once again God provided through an unexpected means. God instructed Moses to take his staff, the one he had used to turn the Nile into blood, was now to provide life-giving water. God’s own presence was in front of him at Horeb, and through Moses’ action of striking the rock, abundant water flowed. The place where this occurs portrays testing and quarreling, Massah and Meribah, and becomes a cautionary note about how not to behave toward God. Israel’s remarkable lack of faith was on display in full force.  It seemed that Moses believed that God had the right to test the people, but they should have refrained from testing God. After all, God has prerogatives that do not belong to human beings. Brueggemann says that this text warns against a utilitarian view of God, in which the divine “is judged by the desired outcomes for the asking community.” In other words, human measures should not presume to assess the adequacy of God. Dictating how God must respond reduces the sovereign one to our level, a risky proposition, indeed. While immanent among us, God is also working in ways that transcend our comprehension.  Jesus’ own experience in the wilderness raises this theological question once again. When the devil tempted him to throw himself down from the pinnacle of the temple in order to prove his identity in a spectacular way, Jesus quoted, “Do not put the Lord your God to the test” (Deuteronomy 6:16). The rest of the verse is “as you did at Massah.”  This faith venture is not easy, and we are beckoned to trust what we cannot see. Walking by faith and not sight produces a bit of anxiety, even for the mature in Christ. We echo the treasured words of Thomas Merton in Thoughts in Solitude:  “My Lord, God, I have no idea where I am going. I do not see the road ahead of me. I cannot know for certain where it will end.”  Thankfully, the one who was seen and touched by earliest believers, the very sacrament of God’s presence with humanity, walks just ahead of us, marking out the pathway. It is this reality that leads Merton to conclude his prayer with these words:  “Therefore I will trust you always though I may seem to be lost and in the shadow of death. I will not fear, for you are ever with me, and you will never leave me to face my perils alone.” Social media is blowing up about a culture of sexual harassment and assault. The sordid tale of Harvey Weinstein’s pattern of predatory behavior against women has opened up a larger conversation about the almost universal experience of sexual exploitation of women. We have been ogled, called “babes,” touched without permission and raped. This does not just occur in the entertainment industry, but in every profession, including ministry.  Many clergywomen have voiced their experiences of violation, often perpetrated by friends in seminary, senior pastors and judicatory leaders — many of whom were publicly affirming of women in ministry. Baptists are not absent from this tale of abuse, and our refusal to talk about it forthrightly has only given license to predators. I am grateful that Baptist Women in Ministry is working on a major project addressing sexual abuse.  Far too many of us have a story of unwanted attention and worse, usually entailing a major power differential. Male colleagues have propositioned their female colleagues; some male pastors have taken advantage of vulnerable counselees; men have felt free to comment on women’s appearance in a sexually suggestive way; male professors have sought hugs and endearments from female graduate students or younger women colleagues; and, women have been silenced when speaking about these behaviors. Not surprising, the cycle has gone on. That is why so many Baptist women in ministry are posting “Me, too.”  There have been some serial predators in moderate Baptist circles, and only in conversation with other women in ministry have we discovered our similar stories of untoward actions by these men. They did not lose their ministerial credentials, and often they moved on to even more lucrative professional pursuits.  Experiences such as these bring shame to women, vocational redirection for some, and deepened awareness of how systems protect abusers. When a female pastor lodged a complaint about a pastoral colleague on the same staff (multiple women of the church had complained of his behavior toward them), many of the congregation took his side and departed when he left. They believed him incapable of such, and they did not trust the word of the female pastor. Why would she even need to bring it up, they wondered.  Patriarchy had certainly conditioned them to believe the male pastor instead of the female, even though she had higher rank on the pastoral staff. Congregants were more upset with her for calling for the discipline of this minister than upset with him for his violation of ministry ethics.  I have my own stories, as will many readers of this column. As a seminary student, a married doctoral student who was my Greek instructor began to stop by my study carrel in the library, inviting me to go out with him. He would say things like, “If only I had met you before I married,” and other inappropriate things. I refused his invitations, and I am not sure if my grade was lowered from an A to an A- because of that.  I was once invited for a weekend of preaching, and the pastor came to pick me up at the airport alone. I had thought his wife was coming, too. He kept asking if I wanted to go shopping or stop and get a drink. I managed to persuade him that I really needed to finish preparation for the varied events of the weekend. I kept my distance the rest of the weekend, making sure I was not alone with him. I later learned of other women who had experienced this kind of behavior — and more — from him.  As more women enter the ranks of ministry, some men have reacted to what they see as a threatening incursion with renewed diminishment of women, often through sexualizing them inappropriately. Women must call this out for what it is: predatory behavior. In a time when the president of the United States can brag about sexual assault, many men are emboldened to act on their worst impulses.  I am thankful that many are speaking clearly about a culture of sexual predation. This conversation is hardly finished, and many are finding their voices. Many preachers took up the Isaiah 55 passage this past Sunday, proclaiming the abundant provision of God for all that sustains life.  All you who are thirsty, come to the water!  You who have no money, come buy food, and eat!  Come, buy wine and milk, without money, without price!  The invitation is almost too good to be true! Who gets to purchase the finest food and drink without ever pulling out the credit card? Not only does the prophet speak of the basic physical nourishment we require, but also speaks of listening to God “that your soul may live” (55:3b). Many live unaware that our deepest longings go beyond what we eat and drink; hunger and thirst for the Holy One are inscribed into every human being. The problem is that we forget, over and over.  This portion of Isaiah announces that the time of desolation is over and that God may be sought — and found (v.6). God promises abundance, and renewal will come to a place covered with briers and thorns. Myrtle and cypress trees will flourish, and they will signal God’s goodness and restorative power. The landscape will mirror the renewing work God will do with the covenant people. As Irenaeus wrote, “The glory of God is humanity fully alive.”  Weariness with life stalks many in our time, and rather than turning to the source of renewal, we often simply numb ourselves with “food that does not satisfy” or distracting entertainment. Recently we have been shocked by the social media fueled suicide of an adolescent boy, goaded by the promptings of an adolescent girl. Teenage suicide is on the rise (and it is not just boys), and one cannot help but wonder at the level of emptiness so many experience. The incessant postings of peers who seem, by comparison, to have more friends, fun and family leaves others with a sense of diminishment that cannot be surmounted.  God’s invitation is nothing less than a beckoning to receive what God alone can provide: grace. Grace receives us as we are and renews the human spirit. Grace reminds us of who we are created to be and supplies the enervating work of the Spirit that we might fully live. Without grace, despair eclipses hope, and persons wonder if they can endure.  I have just enjoyed the extravagance of two weeks away in the mountains of northern New Mexico. The Idlewild Community, near Eagle Nest, provides a “preacher cabin” for visiting ministers. If you preach at the worship service, you have the privilege of staying in this rustic setting for the better part of a week. Since I preached two Sundays, I got to stay a bit longer. I am grateful that my school encouraged this investment of time, and I trust my soul will live with a greater sense of abundance.  The first word of the Rule of St. Benedict is “listen,” and this is what I have attempted to do in this time. I have listened to rushing streams flowing over creek stones; I have listened to wind ruffling wild flowers; I have listened to mule deer stirring in the brush; I have listened to rain on a tin roof; I have listened to bats squeaking under the eaves; I have listened to Scripture; and, I have listened to the sound of a “fine silence,” as Elijah testified. God has spoken through these media, and it has been good.  Most important has been an attentiveness to grace that is an intrinsic part of Sabbath. Wendell Berry says this of Sabbath: “… the field is tilled and left to grace.” Recognizing the limitation of effort and the need for rest requires a level of relinquishment, which most of us resist. Meister Eckhart used the concept of Seinlassen in his mystical writings; it is an attitude of “letting be,” of being in a state of receptivity to what might be born through us. He encourages humans to become the silent place of God’s presence.  Tethered to our devices, we presume that if we go off the grid dire things may happen. I had little wi-fi connectivity in the “preacher’s cabin,” and wonder of wonders, my school, church and family all survived just fine in my absence. Humility trusts that God’s grace will produce harvest even when our labor is suspended. The word “Shabbat” comes from the root Shin-Bet-Tav, meaning to cease. The Sabbath keeps the people more than people keep the Sabbath, as the Jewish adage reminds us.  Soon the rhythms of summer will be behind us, and the regularity of academic and ecclesial schedules will order our days and weeks. The memory of this time of renewal will linger, I trust, and remind me to return to the source of life, where one may drink deeply. God’s invitation is ever present, and abundant grace is offered. A trip to the cardiologist is rarely routine. Usually a precipitating episode or a prior procedure prompts the appointment, and we are eager to receive a reassuring assessment. We know that the condition of the heart determines our health, and we listen with rapt attention to learn how we are really doing.  This past winter while teaching at Conception Abbey, I had a viral infection that mimicked some sort of heart trouble. Being taken to the infirmary in a male monastery was exciting for all concerned — monks and patient alike. I got to see parts of the monastery that are off-limits for women, so I dutifully closed my eyes (well, most of the time). Thankfully, all checked out well after a trip to a regional hospital and a follow-up with my cardiologist. Truly, I gained a new appreciation for my heart.  The Bible speaks of the heart frequently, making it the most common anthropological term in the Scripture. It is the vital center of a person, and it supplies the nourishment that is life-giving. In our time, we think the brain to be the center of human activity, directing the rhythms of an individual’s body; from the ancients’ viewpoint, the heart was the central organ that moved the rest of the body.  Scripture also speaks of the heart as the source of emotional, intellectual and moral activities. Thus, it plays a significant role in spiritual health, as the heart gathers up one’s whole being. It is the locus of faith; it orients a person’s life, and we should be wary of giving our heart to other than God. As Martin Luther said, belief is “that upon which we lean our heart.”  The Gospel reading for this coming Sunday includes the parable of the sower. Jesus speaks of varied responses to the word of the coming reign of God, and he warns how hard it is to be faithful to it, for “the cares of the world and the lure of wealth choke the word, and it yields nothing” (Matthew 13:22). The word intends to rehabilitate the human heart, which can be “hardened,” “wicked,” “godless,” in biblical parlance.  When one broadcasts seed rather than carefully spaced planting, seeds can end up in varied contexts. Good soil can encourage growth, but rocky soil or a thorny patch or a trodden path make it very hard for the seed to bear plentiful yields. Varied elements conspire to keep the seed (the word) from accomplishing its true intent, to renew the heart by grace.  Responding to the word of God is not an easy thing, for it puts us at odds with the dominant culture of our world, which is all about acquisition, comfort and personal security. Jesus warns that the claim of this word is difficult: “When anyone hears the word of the basileia (the reign of God) and does not understand it, the evil one comes and snatches away what is sown in the heart …” (v. 19).  Consumerist Christianity has performed a convenient work-around so as to ease the burden of Jesus’ claim. We have relegated his warning to the status of “interim ethics.” Jesus could only expect his followers to be radical in their fidelity because the world would soon end; he would return and rescue them from the challenge of living in a world of competing claims. He surely could not have meant for people to live in the economics of grace long term.  Wallace Hartsfield II suggests to his preaching students at our seminary that one who proclaims must find the original claim of the text on the hearer, the claim for the hearer today and the claim upon the preacher. This parable would make more sense in an agricultural setting; however, its pungent insights remain and call hearers of the word to examine their response and think about what issues forth from the heart, as well its chief longing.  I recently had the opportunity to write a blurb for an important book authored by Elizabeth Hinson-Hasty. In her forthcoming text, The Problem of Wealth, she challenges assumptions about why people are in poverty and calls for a new vision of community requiring economic justice. By delineating the problem when theology and economics are not integrally related, she is echoing the teaching of the parable. Wealth is the problem, she contends, for it tramples compassion for the economic other. Once again, it is a matter of the heart.  When the word of God takes lodging in our hearts, we are beckoned toward generative living, little satisfied with only the world’s goods. Where the heart is located and what fruit it bears makes all the difference for the treasure that is God’s emerging reign. **The author has been participating with QC Family Tree in tracing the steps of the Freedom Riders of the Civil Rights Movement. Learn more about the trip here and here. Remembering that it happened once, We cannot turn away the thought,… Enderly Park is blistering under an unseasonable September heat, and Frank Byers saunters across Tuckaseegee Road to the rec center where he likes to play cards with his neighbors. He doesn’t use the crosswalk, but in many ways he’s earned… Enderly Park is blistering under an unseasonable September heat, and Frank Byers saunters across Tuckaseegee Road to the rec center where he likes to play cards with his neighbors. He doesn’t use the crosswalk, but in many ways he’s earned… By Nell Green Glory to God in the highest heaven, and on earth peace, good will among people. (Luke 2:14, NRSV) Our first Christmas on the field was in 1986. From then on, every year we host at least one… Enderly Park is blistering under an unseasonable September heat, and Frank Byers saunters across Tuckaseegee Road to the rec center where he likes to play cards with his neighbors. He doesn’t use the crosswalk, but in many ways he’s earned… “If we have no peace, it is because we have forgotten that we belong to each other.” –Mother Teresa It is Friday night, and 125 friends, new and old, crowd the yard at QC Family Tree. We are gathered to… By Greg and Helms Jarrell Chinua Achebe’s classic novel Things Fall Apart centers around the life of Okonkwo, the powerful leader of his clan and their village, one of nine in the Umuofia region of Nigeria. Okonkwo is a fearsome… Can we feel that there are human beings, very close to us, created in God’s image, who are being buried? If we cannot, perhaps we need to be silent and attentive in order to listen to their stories of suffering due to spiritual, social, economic, and/ or physical conditions that are foreign to us. Flowing like a pink river, scores of women in their trademark headgear marched all over the world, just as they did a year ago. Carrying signs with urgent messages, the generations took to the streets to bring attention to the many concerns women bear. A few men marched, also.  Over the past year, the dam holding back accumulated grievances has broken. Women are insulted at being paid less; women are outraged by the sexual harassment they have endured; women are wearied from the power differential they experience in nearly every sphere. Women believe that they have a rightful claim to be treated equally. Women vote; women are breadwinners; and women are gifted for governing, especially in this fractious time when male egos pose barriers to compromise for the common good. It is past time for recalibrating the calculus of gender in the workplace, the public square, the home and the church.  Some have suggested that this is nothing short of a revolution, and the sensitivities these marches are stoking are having an impact on the social landscape. Men are examining their actions, what they say, and maybe even some of their assumptions about women.  This past week I happened to arrive at the mailbox at the same time the regular carrier was there. He said, “I owe you an apology for something I said to you last summer.” I was puzzled and asked, “For what?” He recalled having remarked that I looked hot; I recall that it was a very warm day. He was concerned that he had offended me. It was not a sexual flirtation, and I did not take it as such. Yet the fact that he interrogated himself about the possible misunderstanding and then brought it up after several months speaks volumes about the growing sensitivity to how the genders interact.  The threshold of pain is palpable in our national psyche. The unrelenting cycle of threatening “breaking news” wears down our capacity for compassion, hope and patience. Yet, pain can be a fertile soil for seeds to sprout and new life to come. As Mary Tyler Moore put it, “Pain nourishes courage. You can’t be brave if you’ve only had wonderful things happen to you.” She is right, and our times call for courage.  The patriarchal chaos of the presidency has prompted an unprecedented number of women to run for public office. Black women, in particular, are leading the way, astonished that the majority of white women voted for the current president. Women contend that their voices are required to bring good governance to the regnant system, all but broken. Yet women are often the brunt of jokes in the Old Boys club of the Senate or the House, seen as interlopers to be tolerated but not welcomed as colleagues. The deal-cutting usually excludes them. Further, statehouses hold few women governors; as of 2017, 22 states have never had a female.  As a New York Times article put it Sunday, “Gender can alienate as well as energize.” Levels of education, race, whether a woman is rural or urban, all contribute to the possibility that gender concerns can galvanize an ongoing movement — or not. Even though the hard work of gender equality in modern times began in the ’60s, it is not finished. Rarely do persons voluntarily relinquish power, and the pressure is building.  While other women were marching, I was meeting with members of Central Baptist Theological Seminary’s Women’s Leadership Initiative and their mentors for training on how to address race and justice issues. That will be part of the necessary work ahead as we find ways to work together for a more just society, not expecting black women to carry a disproportionate burden for making things right. There is much to do beyond marching!  As I attended a Martin Luther King Jr. rally last week, I observed that there was a concerted effort to get people registered to vote. This most basic civic privilege is often thought to be a futile pursuit, especially when districts are shifted and new forms of identification required. Voting remains a critical expression of voice and power, as the recent defeat of Roy Moore at the hands of women, primarily black, demonstrates.  Scholarly writers and common parlance suggest that the future belongs to women. I agree if it is a balancing of power that ensures safety in the workplace, equality in compensation, and new value accorded to the leadership practices that women bring.  Globally, girls still lag behind in educational opportunities, as Malala has so courageously brought to the world’s attention. Educating women and girls raises the level of the whole community, as the findings of many studies document. Gender equality is even a more urgent issue for developing nations, and those with privilege wisely join in advocating for them. Together, we march into the future. Enderly Park is blistering under an unseasonable September heat, and Frank Byers saunters across Tuckaseegee Road to the rec center where he likes to play cards with his neighbors. He doesn’t use the crosswalk, but in many ways he’s earned… Enderly Park is blistering under an unseasonable September heat, and Frank Byers saunters across Tuckaseegee Road to the rec center where he likes to play cards with his neighbors. He doesn’t use the crosswalk, but in many ways he’s earned… Enderly Park is blistering under an unseasonable September heat, and Frank Byers saunters across Tuckaseegee Road to the rec center where he likes to play cards with his neighbors. He doesn’t use the crosswalk, but in many ways he’s earned… Fox News personality Todd Starnes attributed Donald Trump’s election to “divine intervention” aided by prayer rallies led by evangelist Franklin Graham in an American Pastor’s Network radio interview Feb. 9.  Starnes, a Fox News personality for more than a decade who early in his career wrote for the Southern Baptist Convention news service Baptist Press, said that he interviewed the head of both the Billy Graham Evangelistic Association and Samaritan’s Purse for his new book The Deplorables’ Guide to Making America Great Again on Thursday’s installment of Stand in the Gap Today.  “He told me we were at a moral tipping point, and I’m not sure if people understand what was literally at stake and how close we were to losing the country, to losing the culture, if Hillary Clinton had won,” said Starnes, whose previous books include God Less America published in 2014. “Losing the Supreme Court, not just for an election cycle but for generations, that’s how much was at stake here.”  Starnes said he believes Graham’s message “resonated with Christians across the country” as the evangelist shared it with an estimated 230,000 people during his Decision America Tour urging voters to “pray, vote and engage in the political process” during the summer and fall.  “I would not discount the power of what Franklin Graham did,” said Starnes, recognized by pollster George Barna as one of the top 10 media influencers for conservative evangelicals in the 2016 presidential election.  “He did not endorse, but what he did do in 2016 was stage massive prayer gatherings at every single state capital in the country,” Starnes said. “I believe that we experienced divine intervention last November. I believe that God was giving us a second chance. We’ve been given a second chance, and Christians, we’ve got to stand up and we’ve got to get it right.”  Graham, son of 98-year-old retired evangelist Billy Graham, said in December he didn’t believe it was the Russians who intervened in Trump’s election but God.  “I don’t have any scientific information,” Graham reportedly told a crowd at Trump’s final “thank you” rally in Mobile, Ala. “I don’t have a stack of emails to read to you, but I have an opinion: I believe it was God. God showed up. He answered the prayers of hundreds of thousands of people across this land who had been praying for this country.”  Graham recently came under criticism for defending President Trump’s executive order barring citizens of seven Muslim-majority countries from entering the United States for 90 days and suspending the admission of all refugees for 120 days subsequently blocked by federal courts.  The Baptist Churches of Puerto Rico responded by withdrawing endorsement for Graham’s Feb. 10-12 Festival of Hope evangelistic rally in San Juan, terming the preacher’s endorsement of Trump’s anti-immigration stance “contrary to the values of the Kingdom.”  Graham continued to defend the Trump travel ban on social media, posting Feb. 11 on Facebook that as head of a humanitarian organization working in more than 100 countries, including most on the banned list, “I feel I have something to say about this issue.”  “We are working to help thousands of refugees every day in different countries,” Graham described the work of Samaritan’s Purse. “Like the Good Samaritan Jesus told about in the Bible, we help those who have been hurt along life’s road. But that doesn’t mean we don’t need to make the borders of our own country secure.”  “We shouldn’t be naïve,” Graham said. “Just because we give medical care to ISIS fighters doesn’t mean I would want to allow any one of them to immigrate to the United States. That would be crazy.”  Graham said taking time to vet who is being allowed to enter America isn’t too much to ask.  “We need to know who they are,” he said. “God does tell us to help the stranger and those in need; but God doesn’t tell us to expose our cities, homes and lives to hostile people.”  Starnes began his journalistic career in newspapers. In 2000 he landed a job as staff writer for Baptist Press and later moved to assistant editor.  Baptist Press fired him in 2003 after he interviewed Secretary of Education Rod Paige and quoted him as saying he personally preferred private Christian education over public schools. After a controversy that nearly cost the secretary his job, Paige claimed that Starnes quoted him out of context.  Seven years later his former employer welcomed Starnes back as keynote speaker at the 2010 Baptist Press Collegiate Journalism Conference.  After leaving Baptist Press Starnes turned briefly to public relations as director of university communications at Baptist-affiliated Union University in Jackson, Tenn. He switched to radio, moving to California before eventually landing in New York as overnight anchor for Fox News Radio.  Today Starnes is heard daily on hundreds of radio stations around the nation. He writes a column, hosts a digital show for FoxNews.com and appears regularly on Fox News Channel and Fox Business Network.  Starnes describes himself as “a gun toting, chicken eating son of a Baptist” with a penchant for finding stories about conservative Christians who feel discriminated against in America’s culture wars.  Critics attribute his appeal to Fox’s key demographic of older, white Christian conservatives to his use of over-the-top rhetoric and indifference toward checking the facts in his stories.  Previous stories:  Baptists withdraw support for Franklin Graham rally in Puerto Rico  Franklin Graham says God uses ‘extreme vetting,’ welcomes confirmation of attorney general Canadian Baptists joined other Christian leaders in an open letter criticizing evangelist Franklin Graham’s upcoming visit to Vancouver, British Columbia, released Feb. 24.  Clergy from churches representing more than 60 percent of the Christians in the metro area said they “fear contentious and confrontational political and social rhetoric” used by Graham to criticize Muslims and LGBT persons and defend Donald Trump “has the potential to overshadow the message of Jesus and incite hostility in our highly charged social climate.”  The letter comes in the aftermath of a mass shooting at a mosque in Quebec that killed six and wounded 19 on Jan. 29, two days after President Trump’s executive order barring travelers from seven Muslim majority countries from entering the United States.  “Regrettably, Franklin Graham’s public comments appear to compromise Jesus’s mission of love and justice for all,” the letter said. “He has made disparaging and uncharitable remarks about Muslims and the LGBTQ+ community, while portraying the election, administration and policies of U.S. President Donald Trump as intrinsically aligned with the Christian Church.”  Church leaders said examples “incendiary speech” by Graham compiled in a separate document “do not convey the spirit of Christ that we would hope to see preached by an ambassador of the Gospel to Canada.”  Among other things, Graham is quoted as saying all Muslims should be banned from the U.S., Islam is a “very evil and wicked religion” and the outcome of the recent presidential election was due to the “hand of God,” the letter said, “giving the impression that the Christian church as an institution is partisanly aligned with an administration and its policies.”  Signers of the letter included Laura Nelson, president of the board of Canadian Baptists of Western Canada and pastor of Olivet Baptist Church, and Jeremy Bell, executive minister of the organization of 183 Baptist churches in four western provinces, the Yukon and the Northwest Territories.  The Canadian Baptists of Western Canada is affiliated with the Baptist World Alliance and North American Baptist Fellowship. Two of the letter signers identify as CBWC pastors. One of them, Pastor Tim Dickau of Grandview Calvary Baptist Church, signed a previous open letter last summer criticizing Graham as “a polarizing figure” whose “ungracious and bigoted remarks have the potential to generate serious negative impact on the Christian witness in Vancouver.”  The religious leaders said they worked behind the scenes for nine months hoping to persuade the planning committee for the March 3-5 Festival of Hope to find a more suitable speaker. They intended to release their letter Feb. 21 but agreed to postpone it to give Graham time to reply in writing.  They said Graham responded Feb. 23, promising to avoid controversial statements while in Vancouver but not retracting harmful comments made in the past. They encouraged Graham to also release his reply to the public.  The faith leaders said they bear no ill will toward the Festival of Hope committee but “simply believe it is a mistake to think Franklin Graham’s political stances are immaterial to his presenting the gospel.”  Earlier this month leaders in the Baptist Churches of Puerto Rico withdrew their support for Graham’s Feb. 10-12 Festival of Hope evangelistic rally in San Juan, terming Graham’s endorsement of Trump’s immigration policies “contrary to the values of the Kingdom.”  Graham shows no sign of toning down his political views. On Feb. 27 he commented on Facebook about a news story on George W. Bush’s daughter Barbara planning to speak at a fundraiser for Planned Parenthood.  “Planned Parenthood is the #1 abortion provider in the United States,” Graham said. “Raising funds for this organization is like raising money to fund a Nazi death camp — like Auschwitz, except for innocent babies in their mother’s wombs! Reports say they perform over 300,000 abortions per year. And this is the organization whose employees were caught on video trying to sell baby body parts over wine. Disgusting.”  Previous stories:  Pastors oppose Franklin Graham crusade in Canada  Baptists withdraw support for Franklin Graham rally in Puerto Rico Baptists in Puerto Rico have withdrawn support for Franklin Graham’s Feb. 10-12 Festival of Hope evangelistic rally in San Juan in protest of the evangelist’s endorsement of anti-immigration policies espoused by President Donald Trump.  The executive minister and the president of the Baptist Churches of Puerto Rico issued a statement Feb. 4 saying Graham’s endorsement of Trump’s policies “are for us contrary to the values of the Kingdom.”  “The Baptist Churches of Puerto Rico historically affirms that our standard of faith and conduct is the Bible,” Executive Minister Roberto Dieppa-Báez and President Margarita Ramirez said in the statement written in Spanish.  “From the Old Testament to the New Testament, God continually calls us to justice, to love, peace and mercy and, above all, to accompany the marginalized, foreigners, widows and orphans,” they said according to an Internet translation.  The Baptist leaders said Trump’s immigration policies “attack the life of our neighbor, and Jesus has always called us to love even enemies and to be our brother’s keeper.”  The release said the Baptist Churches of Puerto Rico board of directors decided to withdraw their support for the event — being held at the same stadium where Graham’s father, Billy Graham, preached to more than 175,000 people during the San Juan Global Mission in 1995 — “for reasons of conscience.”  The Baptist leaders said individual churches and pastors remain free to make up their own mind about whether to participate.  Franklin Graham, who succeeded his father as CEO of the Billy Graham Evangelistic Association in 2002, recently defended Trump’s executive order blocking refugees from Syria from entering the country and barring immigrants from seven predominantly Muslim nations deemed high risk by the White House, saying for him it is “not a Bible issue.”  “It’s not a biblical command for the country to let everyone in who wants to come, that’s not a Bible issue,” Graham said in a Huffington Post article published Jan. 25.  “We want to love people, we want to be kind to people, we want to be considerate, but we have a country and a country should have order and there are laws that relate to immigration and I think we should follow those laws,” Graham told associate religion editor Carol Kuruvilla. “Because of the dangers we see today in this world, we need to be very careful.”  The Baptist Churches of Puerto Rico is one of 34 regions affiliated with American Baptist Churches USA and the only one that is fully Hispanic. As of 2011 the region numbered 112 churches with more than 25,000 members.  The region’s leaders said they do not intend to undermine the Festival of Hope but issued their statement in order “to affirm our testimony in favor of the poor, marginalized and foreigners, among others.”  “Let us continue in prayer so that the gospel of Jesus can be proclaimed and lived in all our earth,” the Baptist leaders said.  It isn’t the first time that Graham’s outspoken views on controversial issues such as homosexuality, Islam and immigration have caused a stumbling block for an evangelistic event. Last fall five pastors in Canada — two of them Baptists — said publicly they would not be supporting Graham’s March 2017 Festival of Hope crusade in Vancouver, labeling him a poor witness for the gospel.  In addition to his role with the Billy Graham Evangelistic Association, Graham runs Samaritan’s Purse, an international relief agency with programs including Operation Christmas Child, an annual event that recruits volunteers in churches and other organizations to fill shoeboxes with Christmas gifts for impoverished children in more than 150 countries and territories around the world.  A Baptist News Global columnist recently suggested moderate Baptist churches that participate in Operation Christmas Child unwittingly give Graham a platform to spout his political views.  “Do you really want to send a dose of hatred along with that shoebox of Christmas trinkets?” Mark Wingfield, associate pastor of Wilshire Baptist Church in Dallas, wrote Jan. 28. “Does handing out Christmas gifts counterbalance Graham’s declaration that many of those who receive them would not be welcome in America?”  “There are other — and better — ways to support refugees and children in need around the world,” said Wingfield, a member of the Baptist News Global board of directors. “In reality, sending a shoebox of odds and ends is less effective than providing food and clothing and shelter and education.  “If you are outraged by Franklin Graham’s misrepresentation of Christian doctrine, channel your support somewhere else, like World Vision or any of the reputable denomination-based relief agencies,” Wingfield said.  The Charlotte Observer reported in 2015 that between his two jobs Graham received compensation totaling more than $880,000 in 2013. That made him the highest-paid CEO of any international relief agency based in the United States, but it was less than the $1.2 million he received in 2008.  Last year the IRS reclassified the Billy Graham Evangelistic Association as an “association of churches” so that the charity is no longer required to file a financial disclosure form that includes top salaries of executives. The sister organization Samaritan’s Purse also requested reclassification for the same reasons but reportedly did not receive an immediate response.  Previous stories:  Pastors oppose Franklin Graham crusade in Canada  BWA criticizes Muslim travel ban Franklin Graham is dead wrong about immigration, refugees and the Bible. And his comments will have deadly consequences.  And by the way, if you support Samaritan’s Purse and its Operation Christmas Child next December, you will be supporting Graham’s kind of twisted Christian witness. More on that in a moment.  On top of previous outlandish comments about Muslims, the son of America’s most famous evangelist last week rushed to the defense of Donald Trump, saying the president’s hardline ban on immigration was just fine because it is “not a biblical issue.”  File this under the category of “alternative facts.”  Although let’s give the younger Graham a little slack because on one point he’s right: The Bible does not discuss American immigration policy. In the Old Testament, we read about the nation of Israel, and America is not Israel.  But here’s what God does say in Deuteronomy 10, when the Children of Israel have just escaped bondage in Egypt: “For the Lord your God is God of gods and Lord of lords, the great God, mighty and awesome, who is not partial and takes no bribe, who executes justice for the orphan and the widow, and who loves the strangers, providing them food and clothing. You shall also love the stranger, for you were strangers in the land of Egypt.”  That’s one of 92 times the Hebrew word we would translate to “immigrant” in English appears in the Old Testament. And in every case, the admonition is for acceptance and welcome and kindness.  The New Testament doesn’t conceive of a political nation, but Jesus’ directives are given toward individuals who are to act in Christ-like ways wherever they live. So, for example, in Matthew 25 we hear these words of Jesus about the kind of behavior God rewards: “I was a stranger and you welcomed me.”  So, yes, the Bible does not give directions on American foreign policy, even though too many evangelical Christians act like it does. American foreign policy is not, strictly speaking, a biblical issue.  But how a nation full of Christians — and a government filled with professing Christians — should treat immigrants is indeed a biblical issue. And there’s no equivocation that the biblical mandate is for welcome, not building walls, not closing borders, not seeking our own welfare to the detriment of strangers in dire need. You cannot make a biblical case for the policies enacted by Donald Trump and tacitly approved by Congress’ silence.  As a result of this policy, people will die. Immigrants who need to flee horrendous situations will be trapped and will die. And then others will die because America’s protectionism will breed more terrorism and more hatred. How in God’s name is any of that biblical?  Franklin Graham has a voice not only because he is his father’s son but because of his work through Samaritan’s Purse. No doubt, this relief agency does good work and not everyone who works there agrees with Graham. But remember that he gets a hearing because of the scope of that organization.  Evangelical Christians across America enable this platform — and Graham’s mean declarations — by supporting Samaritan’s Purse financially and by teaching their children to support it through Operation Christmas Child. Do you really want to send a dose of hatred along with that shoebox of Christmas trinkets? Does handing out Christmas gifts counterbalance Graham’s declaration that many of those who receive them would not be welcome in America?  There are other — and better — ways to support refugees and children in need around the world. In reality, sending a shoebox of odds and ends is less effective than providing food and clothing and shelter and education. If you are outraged by Franklin Graham’s misrepresentation of Christian doctrine, channel your support somewhere else, like World Vision or any of the reputable denomination-based relief agencies.  Let’s plant seeds of hope, faith and love that will take root and produce a harvest of righteousness and not just dangerously wrong theology. Enderly Park is blistering under an unseasonable September heat, and Frank Byers saunters across Tuckaseegee Road to the rec center where he likes to play cards with his neighbors. He doesn’t use the crosswalk, but in many ways he’s earned… The recent uprising in Baltimore has occupied our screens and dominated our conversations for more than a week now. Protestors there pricked the consciences of the nation in their cries for justice for Freddie Gray. At some point, a small… Enderly Park is blistering under an unseasonable September heat, and Frank Byers saunters across Tuckaseegee Road to the rec center where he likes to play cards with his neighbors. He doesn’t use the crosswalk, but in many ways he’s earned… Enderly Park is blistering under an unseasonable September heat, and Frank Byers saunters across Tuckaseegee Road to the rec center where he likes to play cards with his neighbors. He doesn’t use the crosswalk, but in many ways he’s earned… Enderly Park is blistering under an unseasonable September heat, and Frank Byers saunters across Tuckaseegee Road to the rec center where he likes to play cards with his neighbors. He doesn’t use the crosswalk, but in many ways he’s earned… In my city, Charlotte, N.C., we have reached a general consensus concerning housing and affordability: we all agree that there is not nearly enough of affordable housing, and we would all like for someone else to do something about it. The announcement came on Saturday. Just three days before the Alabama special election that transfixed the nation, and on the same day that President Trump fact-checked the Washington Post’s Dave Weigel, Iraq’s prime minister declared victory in the war against ISIS. Iraq — with indispensable American help — has regained control of its cities and its border with Syria. ISIS has been reduced to a shadow of its former self.  The victory isn’t confined to Iraq. American-allied forces control ISIS’s former capital in Syria, and the world’s largest jihadist army is gone. Bands of insurgents still prowl the countryside, and ISIS cells exist across the world, but the war against the “caliphate” is over. It’s been won.  So why does no one seem to care?  It was exactly three years ago that the Middle East was in crisis. The ISIS blitzkrieg had brought Iraq to its knees. Jihadists controlled immense sections of Iraq and Syria. Abu Bakr al-Baghdadi spoke from Mosul’s Great Mosque, declared himself “Caliph Ibrahim,” and called on Muslims across the world to join him in his jihad.  They answered his call by the thousands. They flocked to Syria and Iraq from North Africa, Europe, and Asia. Britain was rocked by reports that more of its Muslim residents had joined ISIS than joined the British military. ISIS initiated genocide. It threatened the Kurds. It threatened Baghdad. Americans old enough to remember the fall of Saigon began to wonder: Was history repeating itself?  x  For veterans of the Iraq War like me, these were extraordinarily painful months. Friends died over there. Others lost limbs or suffered terrible wounds. Every man and woman who served in Iraq sacrificed something, even if it was “only” a year of their life. And now our nation looked like a bystander to a calamity. Through withdrawal, we’d squandered the military victory of the Surge. Through withdrawal, we’d empowered our enemies.  Even after the Obama administration intervened to stave off total defeat, ISIS still dominated the narrative. It helped unleash a wave of terror in Europe. Terror investigations and attempted terror attacks spiked in the U.S. Even while under American and allied aerial assault, ISIS still held territory. It still controlled major cities.  Remember how debates about ISIS dominated the presidential primaries? Remember how Donald Trump and Ted Cruz ratcheted up their rhetoric until they both seemed to promise that they’d commit war crimes like carpet bombing and torture to defeat the deadly threat? ISIS was often the most important and most prominent story in the world.  Now, however, the caliphate is a smoking ruin. It courted conflict with the great powers. It craved Armageddon, and it got its wish. No one knows ISIS’s exact casualty figures, but its fighters have died by the tens of thousands. I’ve spoken to men who were directly involved in the air campaign, and they have told me that the public doesn’t yet understand the sheer scale and ultimate effectiveness of the American attacks.  Yes, we withdrew from Iraq too soon. Yes, our counteroffensive against ISIS unfolded slowly. But we fought back, we trained and equipped allies, and we won.  This is one of the best stories of the young Trump administration. While many of the battles were fought under Obama, Trump pursued the enemy relentlessly. He delegated decision-making to commanders in the field, they fought within the laws of war, and they prevailed. Trump promised to defeat ISIS, and he has delivered a tremendous victory.  Part of the blame still rests with us. Let’s be honest: Panic and fear make for a better story than victory and peace.  So why isn’t this bigger news?  Part of the blame, of course, rests with Trump himself. Peruse his Twitter feed for a moment. Aside from the occasional boast about the economy, Trump uses his favorite instrument to wage war on “fake news” and to pursue personal vendettas. It’s hard to blame the press for not reporting more extensively on the war when the president himself is directing its attention elsewhere. He’s the first president in my lifetime who somehow seems determined to distract the public from good news by creating his own bad news.  We’re also understandably wary of “mission accomplished” moments. Jihadists, including ISIS jihadists, are still out there seeking to kill Americans. And we shouldn’t minimize that reality in acknowledging the momentous accomplishment of the Caliphate’s defeat.  But part of the blame still rests with us. Let’s be honest: Panic and fear make for a better story than victory and peace. I hear all the time from friends who ask me to “write more about good news.” Yet I write about good news all the time, and those pieces are often among my least-read articles. Perhaps I’m simply bad at writing about good things. Or perhaps the public has less appetite for the positive.  Either way, it’s time for this to change. Americans died in the fight against ISIS. They restored American military victory in Iraq, preserving the gains of the men and women who fought there years before. In the process, they defeated one of most vicious and evil enemies our nation has ever faced. They helped retake cities and liberate the oppressed. They won a war. It’s a victory worth a celebration.  READ MORE:  Did Trump Beat ISIS?  The Fall of Raqqa Is a Marvelous, Bipartisan American Victory  As ISIS Falls, Don’t Forget the Lessons of the Recent Past  — David French is a senior writer for National Review, a senior fellow at the National Review Institute, an attorney, and a veteran of Operation Iraqi Freedom. Google, the most powerful search engine in the world, is now displaying fact checks for conservative publications in its results.  No prominent liberal site receives the same treatment.  And not only is Google’s fact-checking highly partisan — perhaps reflecting the sentiments of its leaders — it is also blatantly wrong, asserting sites made “claims” they demonstrably never made.  When searching for a media outlet that leans right, like The Daily Caller (TheDC), Google gives users details on the sidebar, including what topics the site typically writes about, as well as a section titled “Reviewed Claims.”  Vox, and other left-wing outlets and blogs like Gizmodo, are not given the same fact-check treatment. When searching their names, a “Topics they write about” section appears, but there are no “Reviewed Claims.”  In fact, a review of mainstream outlets, as well as other outlets associated with liberal and conservative audiences, shows that only conservative sites feature the highly misleading, subjective analysis. Several conservative-leaning outlets like TheDC are “vetted,” while equally partisan sites like Vox, ThinkProgress, Slate, The Huffington Post, Daily Kos, Salon, Vice and Mother Jones are spared.  Occupy Democrats is apparently the only popular content provider from that end of the political spectrum with a fact-checking section.  Big name publications like The New York Times, The Washington Post, and the Los Angeles Times are even given a column showcasing all of the awards they have earned over the years.    The Robert Mueller fact check (pictured above) is a case in point for Google’s new feature.  Ostensibly trying to sum up the crux of the post, the third-party “fact-checking” organization says the “claim” in a DC article that special Counsel Robert Mueller is hiring people that “are all Hillary Clinton supporters” is misleading, if not false.  The problem is that TheDC’s article makes no such claim. Their cited language doesn’t even appear in the article. Worse yet, there was no language trying to make it seem that the investigation into the Trump administration and Russia is entirely comprised of Clinton donors. The story simply contained the news: Mueller hired a Hillary Clinton donor to aid the investigation into President Donald Trump.  Still, the Washington Post gave the claim, which came from Trump himself, its official “Three Pinocchios” rating. The method applies to several other checks. Claims concocted or adulterated by someone outside the TheDC are attributed to TheDC, in what appears to be a feature that only applies to conservative sites.  Examples of such misattribution and misrepresentation are aplenty.  For instance, using Snopes.com, an organization with highly dubious fact-checking capabilities, Google’s platform shows an article by TheDC to have a so-called “mixture” of truth.    The “claim” made, according to Snopes.com and Google, is “a transgender woman raped a young girl in a women’s bathroom because bills were passed…”  A quick read of the news piece shows that there was no mention of a bill or any form of legislation. The story was merely a straightforward reporting of a disturbing incident originally reported on by a local outlet. (RELATED: Why is Google CEO Eric Schmidt Technically Serving In The Department Of Defense?)  And like Snopes, another one of Google’s fact-checking partners, Climate Feedback, is not usually regarded as objective.    Snopes and Google also decided to “fact-check” an obviously tongue-in-cheek article in which a writer for TheDC pokes fun at a professor saying the solar eclipse in 2017 was naturally racist.  Even Vox pointed out the absurdity of the educator’s literary tirade on Mother Nature’s purported racial prejudice, and the damage it might have done to real arguments of apparent racism. While Snopes got some flak for its choice, no one seems to have noticed the absurdity of the world’s go-to search engine providing fact-checks to purposefully irreverent content, rather than hard news stories.  Overall, such inclusion embodies Google’s fact-checking services, which, as many presciently feared, are biased, if not also downright libelous. (RELATED: Silicon Valley Seems To Love Burying Conservative News)  Google acknowledged it received an inquiry from TheDCNF, but did not fully respond by time of publication.  Follow Eric on Twitter  Send tips to [email protected].  Freedom of Speech Isn’t Free  The Daily Caller News Foundation is working hard to balance out the biased American media. For as little as $3, you can help us. Make a one-time donation to support the quality, independent journalism of TheDCNF. We’re not dependent on commercial or political support and we do not accept any government funding. Yet another ludicrous conspiracy theory overshadows issues of real significance  Paul Joseph Watson  Infowars.com  August 8, 2013  The Internet is ablaze with yet another baseless conspiracy theory that only serves to distract from real cover-ups and issues of genuine significance — the hoax that NSA whistleblower Edward Snowden recently warned of a “solar flare killshot’ set to wipe out hundreds of millions of people in September.  According to the original article, “Edward Snowden, hacker-fugitive and former National Security Agency (NSA) contractor, revealed Tuesday that a series of solar flares is set to occur in September, killing hundreds of millions of people. Documents provided by Snowden prove that, as of 14 years ago, Central Intelligence Agency (CIA) remote viewers knew that the event was inevitable. Ever since, the world’s governments have quietly been trying to prepare for the sweeping global famine to result.”  The story has since gone viral, appearing on countless forums and websites and attracting thousands upon thousands of comments.  Like all popular hoaxes, the story has a thin veneer of authenticity. The sun is set to exhibit some strange behavior as a result of a magnetic field flip that occurs around once every 11 years. This will cause a “feverish” peak in solar activity, leading to more solar flares and coronal mass ejections, potentially disrupting satellites and power grids on earth but not leading to the slaughter of hundreds of millions of people.  Given that there are so many actual cover-ups, scandals and issues of real significance that we could be concentrating on, it’s really quite disappointing that so many people have wasted their energy on something that is so transparently false. The hoax also serves to distract from Snowden’s authentic revelations about NSA surveillance.  The source of the story can be traced back to ‘The Internet Chronicle’, which if you read the site’s ‘about’ page is clearly a satirical website which disseminates false and overtly ludicrous stories.  The website jokes that it originated, “After crash landing in an asteroid December 30th, 1976, the alien husks of Chronicle editors rapidly adapted to Earth climates and bacterial flora. They are able to survive naturally in the wild and reproduce freely.”  “Receiving tweets twice daily via fax from religious advisers impregnates our right-wing furnace of hate, enabling us to bring you the most horrifyingly sensational headlines you’ve ever seen, or your money back — guaranteed!” states the website.  And if that wasn’t enough to convince you that the website is a hoax, just take a look at the other headlines featured on the front page, many of which are based on bizarre manufactured stories involving Edward Snowden.  – Vatican Forces Storm Dome of the Rock, Pope names Snowden Messiah  – Edward Snowden killed by drone strike on Ecuadorian embassy  – Snowden Reveals Second Shooter in Zimmerman Case  – Snowden Granted Sainthood  – NSA Whistleblower Ed Snowden: Saddam Hussein did WTC 7  – Obama Pardons Edward Snowden  Other hoax stories appear with headlines like ‘Paula Deen commits suicide at 66â€² and ‘Wendy’s unveils new hacker theme for restaurants’.  The gullibility of thousands upon thousands of people in falling for such a patent hoax because they didn’t spend 30 seconds checking the source only provides the mainstream media with more ammunition to shoot down very real cover-ups and conspiracies, such as the death of Michael Hastings, by lumping in genuine issues with baseless conspiracy theories such as the Edward Snowden ‘solar killshot’ hoax.  *********************  Paul Joseph Watson is the editor and writer for Infowars.com and Prison Planet.com. He is the author of Order Out Of Chaos. Watson is also a host for Infowars Nightly News.  This article was posted: Thursday, August 8, 2013 at 7:38 am  Tags: science  Related Articles  –>  Republished from: Infowars MOUNTAIN VIEW, CA - OCTOBER 27: Google CEO Eric Schmidt looks on during a conversation with U.S. Speaker of the House Nancy Pelosi October 27, 2008 at Google headquarters in Mountain View, California. Pelosi and Schmidt engaged in a conversation and answered questions as part of the Google speaker series. (Photo by Justin Sullivan/Getty Images)  Eric Schmidt, chief executive of Google parent company Alphabet, was appointed chairman of a Department of Defense program in 2016 that was established by former President Barack Obama’s administration.  A staunch supporter of the Democratic Party and critic of President Donald Trump, Schmidt still continues to lead the Defense Innovation Board (DIB), even well after the new administration took over in January. This begs the question: should Schmidt’s history of partisan advocacy and condemnation of Trump be a worrisome prospect for the current White House?  Regardless of the answer, Trump’s retention of Schmidt may be emblematic of more than political divisiveness within the current administration, like from “Obama holdovers.” In fact, it may be the opposite of unwanted internal discord and a sign of an underlying ethos for the Trump administration — diversity of thought.  “When you look at the composition of the initial Economic Advisory Board, it kind of reminds me of what Trump is doing here,” Justin Danhof, general counsel of the National Center for Public Policy Research and director of the Free Enterprise Project, told The Daily Caller News Foundation. “He’s putting together a team of rivals. If you want to get to the best ideas, you can’t have everyone in the room thinking the same thing.”  Alphabet announced on Thursday that Schmidt will be stepping down in January, marking the seeming end to a 17-year-career at Google and its parent company. It’s quite likely that he will maintain economic and personal interests in the business, especially since he’s expected to stay on as a member of Alphabet’s board of directors and serve as an adviser.  The DIB was formally created in March 2016, and former Secretary of Defense Ash Carter later announced that Schmidt would be heading the organization. Other specifically chosen board members include famous astrophysicist Neil deGrasse Tyson, LinkedIn cofounder Reid Hoffman, Wharton School of Business professor Dr. Adam Grant, Instagram COO Marne Levine, University of Texas chancellor and former Special Operations Command commander Navy Adm. William McRaven, among several others.  Some, like David Williams, president of the Taxpayers Protection Alliance, saw Schmidt’s taking of the helm as somewhat disconcerting given the “open-door policy” the Obama administration allegedly had with one of the country’s most powerful companies.  “There needs to be a massive amount of oversight with this Board,” Williams told TheDCNF. “If the DOD is contemplating new technology to address a new defense need, will it be skewed toward a technology that benefits Google or one of the companies? Massive potential for conflicts of interest with real taxpayer implications.”  And it’s not just personal business affairs that are concerning to some — Schmidt’s own brand of politics could conceivably come into play, whether intentionally or subconsciously. He was spotted wearing a staff badge during then-Democratic candidate for president Hillary Clinton’s election night party. Not long after, Schmidt reportedly told an audience of employees that Trump is “going to do these evil things as they’ve done in the immigration area and perhaps some others.”  In correlation with his statements to employees, Alphabet also reportedly spearheaded the funding efforts for the legal brief signed by nearly 100 companies that objected to Trump’s temporary immigration ban. (RELATED: Google-Funded Think Tank Fires Analyst For Criticizing Google, Says Report)  But due to the work of the DIB — which mainly centers around projects like modernizing military bases and ensuring defense systems are sufficiently up to date — it appears that personal politics likely has a minimal impact.  “The Defense Innovation Board is focused on supporting the Department of Defense on issues, such as AI, data analytics, software acquisition, and shaping the culture of the DOD workforce. These issues know no partisan boundaries,” Navy Cmdr. Patrick Evans, who does press operations for the Pentagon, told TheDCNF. “DOD cares about enhancing lethality, strengthening alliances and partnerships, and reforming the Department. DIB is about advancing the Department and serving the American public, not politics.”  Google declined to comment on the record and referred TheDCNF to the DOD for any statements or insight.  A top representative for Google was sure to add, though, that Schmidt serves on the board in a personal respect, separate from his work at Alphabet or its subsidiaries.  Conversely, Schmidt could arguably be more than a good fit for the DIB given its overarching goals.  The program “seeks to advise the department on areas that are deeply familiar to Silicon Valley companies, such as rapid prototyping, iterative product development, complex data analysis in business decision making, and organizational information sharing,” Evans explains. “Then-Secretary Carter selected the board to represent a cross-section of America’s most innovative industries, drawing on technical and management expertise from across the country.”  Furthermore, Alphabet is a tech conglomerate that, along with other functions, serves as a corporate umbrella for several subsidiaries including Google. Most notably, Alphabet includes X lab, which serves as an incubator for startups, meaning the larger holding company also acts as a hub for technological research and development.  Williams says that “the idea of the DIB isn’t a bad one” because updating the government’s services and infrastructure is needed.  “The only concern is that there’s too much of an emphasis on tinkering rather than fundamental reforms, like ending big failing spending programs,” he added, implying that innovation may distract from more necessary changes, like deep cuts to costly expenditures.  Danhof says the DIB is a great idea, even with the prospect of it backfiring for Trump if board members resign out of political protest.  “If you think about how laggard the U.S. government is when it comes to innovation and technology breakthroughs, if you look at the backlog that’s at the VA [Department of Veteran Affairs], if Eric Schmidt could do even one thing to improve something like that, then I think that this should be cheered as a great success,” Danhof asserted. “President Trump should be cheered for thinking outside the box and going with someone who has spoken out against him, who is a political rival in a sense in that he supports the other party full lock, stock, and barrel. Trump is willing to look past all of that to try to find the best people to help this government operate better.”  While also commending Obama for the board’s creation, Danhof said having Schmidt stay on the board is a true sign of Trump’s acute business acumen since any good business operator or owner doesn’t hire people merely because they think the same way and are likable.  “You hire someone who’s the best person for the job,” Danhof continued. “If it’s how we technologically innovate, why wouldn’t you think of one of the founders of Google, even if he is against you in almost everything you say, think, or do in a political standpoint?”  Also WATCH:  Follow Eric on Twitter  Send tips to [email protected].  The Daily Caller News Foundation is working hard to balance out the biased American media. For as little as $3, you can help us. Freedom of speech isn’t free. Make a one-time donation to support the quality, independent journalism of TheDCNF. We’re not dependent on commercial or political support and we do not accept any government funding.  Content created by The Daily Caller News Foundation is available without charge to any eligible news publisher that can provide a large audience. For licensing opportunities of our original content, please contact [email protected]. The results are in: Planned Parenthood remains America’s biggest abortion provider. And it’s not even close.  The organization — which recently celebrated its centennial anniversary — has just released its annual report for the 2016–17 fiscal year. The numbers confirm what rational observers have long known: Planned Parenthood exists to provide abortions, and not much else.  Amid the gloom, there is one glimmer of positive news. Last year, the group provided fewer abortions than it had the year before, though the number is still staggering: 321,384 abortion procedures in one year alone. In 2015–16, Planned Parenthood reported 328,348 abortions.  A decrease in abortions, of course, isn’t Planned Parenthood’s victory to celebrate — not that the group is trying to take credit for the drop, or celebrating it at all. The U.S. abortion rate has dropped steadily since 1980, so the decline in Planned Parenthood–administered abortions is much more the result of a greater number of women deciding to carry pregnancies to term than it is an indication of the group’s efforts to de-emphasize abortion.  And yet Planned Parenthood continues to portray itself as a solicitous women’s health organization that just so happens to occasionally perform an abortion or two, when it’s absolutely necessary. This report is no exception, and the astonishing figure of 321,384 is treated as nothing more than a footnote.  Aided by Hollywood, complicit media, and Democratic politicians and activists, Planned Parenthood cloaks the truth beneath empty rhetoric about resisting the hostile Trump administration. But key facts in this report unveil the brutal reality: Planned Parenthood is a corporation that exists primarily to profit from abortion.  Aided by Hollywood, complicit media, and Democratic politicians and activists, Planned Parenthood cloaks the truth beneath empty rhetoric about resisting the hostile Trump administration.  Here are the top myths Planned Parenthood peddles that are debunked by information from its own annual report.  1.) Abortion is just 3 percent of Planned Parenthood’s business.  This statistic has been rated false by left-leaning outlets, including Slate and the Washington Post. The claim rests on a deceptive tactic: calculating abortion procedures as a fraction of its “services” — defined by the group as “discrete clinical interactions” — rather than as a fraction of its clients.  Take an example. A woman walks into a Planned Parenthood clinic. She takes a pregnancy test, meets with a counselor, and chooses to have an abortion procedure. While she’s there, she also receives an STI test and a breast exam and is handed birth control on her way out the door. Planned Parenthood would count each of these “discrete interactions” — six in total — as a service, so abortion would be only 16 percent of that woman’s visit.  Applying this method to an entire year of “services,” Planned Parenthood inflates its numbers to make abortion look like a vanishingly small part of what it does. The actual numbers in the report tell a drastically different story.  Look, for instance, at how the 321,384 abortion procedures dwarf adoption referrals (3,889) and prenatal services (7,762). Planned Parenthood performed 83 abortions for every one adoption referral last year. And its prenatal services have dropped steadily every year since 2009, from over 40,000 that year to just under 8,000 last year. Hardly “comprehensive women’s health care.”  Planned Parenthood performed 83 abortions for every one adoption referral last year.  What’s more, Planned Parenthood performs about one-third of annual U.S. abortions, making it the single largest abortion provider in the country, by far. As the most expensive procedure provided, abortion underwrites a huge portion of the group’s bottom line. Surely this is why the national organization routinely imposes abortion quotas on its regional affiliates and rewards clinics that exceed their abortion targets.  Planned Parenthood president, Cecile Richards, has inadvertently underscored the group’s repeated attempts to downplay its commitment to abortion. Richards met with Ivanka Trump last year to discuss the group’s future, and Trump suggested that Planned Parenthood split into two financially distinct groups, one of which offered no abortion, allowing it to receive government money without compromising taxpayers’ consciences. Richards refused, calling it naïve and saying that Trump failed to understand how central abortion is to Planned Parenthood’s mission.  2.) Millions of women wouldn’t have health care if Planned Parenthood didn’t exist.  The report indicates that Planned Parenthood saw 2.4 million clients in the last fiscal year. But, as has been shown by the group’s own figures, it doesn’t provide those clients with very many actual health-care services. According to the report, the only significant services offered, besides abortion, are STI and HIV tests, contraception, and pregnancy tests.  Last year, it provided only 235,000 well-woman exams and 32,000 “family practice services,” despite executives’ repeated claims that the group is an average health-care provider offering a vast selection of regular health-care services.  The report also hints, but doesn’t outright acknowledge, that Planned Parenthood closed dozens of clinics over the last year. Its 2015–16 report cited “nearly 650 health centers,” while this year’s report highlights “more than 600.”  According to American Life League’s annual survey, Planned Parenthood closed 32 clinics last year, leaving the group with just 597 facilities. This means that several states have only a handful of Planned Parenthood centers, and some have as few as one or two. And yet we are supposed to believe that these paltry clinics provide essential care to women who would otherwise have nowhere to go.  In reality, there exist over 13,000 federally qualified health-care centers across the country that offer a wide array of actual health-care procedures, not just a tiny selection of reproductive services. These centers outnumber Planned Parenthood facilities 20 to one. It is beyond absurd to suggest that American women would have nowhere to turn for health care if not to Planned Parenthood.  3.) Planned Parenthood both needs and deserves government funding.  A plurality of Planned Parenthood’s revenue last year came from government reimbursements and grants, totaling more than half a billion dollars. The group is, understandably, fixated on retaining and expanding that funding — surely in large part for the PR cover it provides. Planned Parenthood’s political-action wing spends millions each year lobbying to increase its federal funding, essentially taking government coin and using it to push for even more.  The group’s executives have recently taken to crusading against the Hyde Amendment, which bars federal funding from paying for abortion procedures. (The fungibility of money, of course, means that any federal money given to Planned Parenthood aids the group in providing abortions.)  But the group could very easily continue to exist without federal money. For one thing, the Trump administration has been great for business. The group’s private donations went up by nearly $100 million last year, bringing the tally to more than half a billion dollars from wealthy benefactors. If Planned Parenthood were ever actually defunded, you can bet that number would skyrocket even higher.  And there’s no question that Planned Parenthood is rolling in excess cash. Its net assets at the end of last fiscal year totaled more than $1.6 billion, up from $1.5 billion the year before. The group directs more than $174 million annually to management salaries, an ever-increasing number.  Planned Parenthood’s PAC is among the most powerful lobbying groups in American politics, shelling out $40 million last year for “public policy” and investing upwards of $175 million in such nebulous categories as “movement building,” “strengthening and securing Planned Parenthood,” and “engaging communities.”  If the group can manage to scrounge up enough cash for these activities — and continue to sit comfortably on assets in excess of $1 billion — surely it can bear its own operating costs. Given its commitment to abortion, and its lack of commitment to much else in the way of actual health care, it should be required to do just that.  The public square is rife with misinformation and outright lies about what Planned Parenthood does. Its own annual report should end any debate. It is, first and foremost, a corporation that profits from the routine and casual slaughter of unborn children. For the sake of political prudence, it must be defunded. For the sake of justice, it must be shut down.  READ MORE:  No, the DOJ Investigation of Planned Parenthood Is Not a Witch Hunt  Kill the Child, Spare It No Pain — Cold ‘Justice’ in Texas  Population Control, the Undying Dream of the Left  — Alexandra DeSanctis is a William F. Buckley Jr. Fellow in Political Journalism with the National Review Institute. Planned Parenthood now operates less than 600 abortion clinics in the U.S. after 32 clinics closed in 2017, according to a report published Thursday.  Planned Parenthood operates 597 facilities in the country as of Dec. 1, compared to 700 facilities it operated in 1973, according to LifeNews. The abortion giant opened five new clinics in 2017, however, it closed 32.  The organization ran 938 clinics in 1995 when Planned Parenthood was at its peak operational level, according to an American Life Leagues (ALL) Report about Planned Parenthood released Thursday. Planned Parenthood has closed 36 percent of their clinics since 1995.  Planned Parenthood has 356 facilities that commit surgical or medical abortions, 166 facilities that do surgical abortions, and 190 facilities that do medical abortions but not surgical. All 597 Planned Parenthood facilities, however, distribute abortion inducing drugs.  Annual reports from 2006 through 2015 show that Planned Parenthood closed 24.9 percent of its facilities and saw its customer base drop by 23.6 percent during that 10-year stretch. North Dakota and Wyoming also have zero Planned Parenthood medical facilities.  Ten states account for 60 percent of all Planned Parenthood facilities in America: California, New York, Washington, Texas, Ohio, Pennsylvania, New Jersey, Florida, Wisconsin, and Michigan.  Planned Parenthood operates 48.8 percent of all surgical and medical abortion facilities in the United States, according to ALL’s report.  Follow Grace on Twitter.  The Daily Caller News Foundation is working hard to balance out the biased American media. For as little as $3, you can help us. Freedom of speech isn’t free. Make a one-time donation to support the quality, independent journalism of the Daily Caller News Foundation. We’re not dependent on commercial or political support and we do not accept any government funding.  Content created by The Daily Caller News Foundation is available without charge to any eligible news publisher that can provide a large audience. For licensing opportunities of our original content, please contact [email protected]. Senate Minority Whip Dick Durbin, D-Ill., confirmed on Friday that President Trump called Africa nations “shitholes,” and said the president’s comments were “hate-filled, vile and racist.”  Durbin attended a bipartisan meeting with Trump at the White House on Thursday, where senators gathered with the president to discuss a deal on the Deferred Action for Childhood Arrivals program.  “In the course of his comments, he said things which were hate-filled, vile and racist,” Durbin told reporters Friday. “I understand how powerful [those words] are, but I cannot believe in the history of the White House, in that Oval Office, any president has ever spoken the words that I personally heard our president speak yesterday.”  BREAKING: Democrat Sen. Durbin, who was in meeting with Pres. Trump: \"He said these hate-filled things.\" https://t.co/yUHQuZIOCm pic.twitter.com/s9fMhtcguR— MSNBC (@MSNBC) January 12, 2018  The Washington Post and other outlets reported Thursday that during the discussion on a DACA deal, Trump questioned why the U.S. is “having all these people from shithole countries come here?”  At the time, the president was criticizing a plan from the Gang of Six senators to reinstate the temporary protected status programs for El Salvador, Haiti, and some African nations.  Durbin told reporters Friday that, of the press reports that have included the president’s comments, “I have not read one of them that’s inaccurate.”  He then reiterated the conversation the senators had with Trump. While discussing the TPS programs, Durbin said he told Trump those primarily impacted by TPS were from El Salvador, Honduras, and Haiti.  “When I mentioned that fact to him, he said, ‘Haitians? Do we need more Haitians?’” Durbin recalled. “And then he went on and he started to describe the immigration from Africa that was being protected in this bipartisan measure. That’s when he used the vile and vulgar comments, calling the nations they come from \\'shitholes.\\' The exact word used by the president, not just once, but repeatedly.”  Trump’s reported comments set off a storm of criticism among Democratic and Republican lawmakers, who are calling on the president to apologize.  In tweets Friday morning, Trump denied making derogatory remarks about Haiti, and accused Democrats of fabricating his comments.  “Never said anything derogatory about Haitians other than Haiti is, obviously, a very poor and troubled country. Never said ‘take them out.’ Made up by Dems. I have a wonderful relationship with Haitians. Probably should record future meetings - unfortunately, no trust!” Trump tweeted. A few conservative critics predicted early on that the recent obsession with fake news would lead eventually to right-of-center newsrooms being lumped with the bad actors.  It looks increasingly likely that those critics were onto something.  Harvard University\\'s library website currently features a page titled, \"Fake News, Misinformation, and Propaganda.\"  The page URL even has the word \"fake\" in it.  This portion of the library\\'s website, which can be found in the \"research guides\" section, is intended to give students, \"a brief introduction to the spread of misinformation of all kinds and tools for identifying it, and reading the news with a more informed eye,\" according to the site.  The \"fake news\" page includes the aptly titled subsection: \"False, Misleading, Clickbait-y, and Satirical \\'News\\' Sources (Huge list of fake news sites).\"  That subsection links to a massive online database, which lists nearly every legitimate conservative-leaning news groups alongside actual hoax websites.  The Washington Free Beacon, Independent Journal Review, the Daily Caller and the Washington Examiner are all on the list. Each is tagged with identifying labels, including \"clickbait,\" \"bias,\" \"political\" and \"unreliable\" or a combination of the the four.  The list also includes conservative commentary sites like TheBlaze, RedState and the Weekly Standard.  Fox News is not included on the database.  Legacy media organizations, including the Washington Post and the New York Times, are also excluded from the list, despite that they\\'ve bungled several recent Trump-related \"scoops.\"  The online database linked by Harvard\\'s library features a handful of well-known left-leaning news sites, including Raw Story and Think Progress. However, several notable left-of-center newsrooms, including Mother Jones, Vox.com, Media Matters and the Huffington Post are conspicuously absent. Several well-known liberal commentary sites, including Salon, Slate and the New Republic, are likewise missing from the list.  The database of \"fake, false, or regularly misleading websites,\" is not a Harvard creation. It was compiled last year by Merrimack College assistant professor Melissa Zimdars, who maintains it is being misrepresented by the press and others.  \"[I]t\\'s not a fake news database,\" she told the Examiner, adding that the project was intended originally for just her students.  Zimdars defended the database, saying, \"there are quite a few left-of-center and far-left websites including in my resources, and I\\'m not sure why they are continually and purposefully disregarded to make points about my \\'bias\\' (maybe you\\'re just unfamiliar with the websites, which may point to some of your own biases?).\"  She said she has done her best to include websites from both the left and right side of the aisle, because she is concerned most about \"informational integrity.\"  \"That being said,\" she explained, \"there are more conservative or right of center websites included in my resource partially because there are more of them (and partially because, perhaps, more of them have been submitted to me for analysis).\"  \"They\\'ve done a better job of constructing an alternative and robust mediasphere,\" Zimdars added.  Along with clearly fake websites, her project, which was taken offline temporarily but is available now for viewing, features sites she claims, \"may circulate misleading and/or potentially unreliable information\" or \"sometimes use clickbait-y headlines and social media descriptions.\"  The Washington Free Beacon, which is listed under \"bias,\" has won kudos from even left-of-center critics for its dogged reporting. Three years ago, for example, the Free Beacon obtained and published a list of donor pledges to the progressive group Democracy Alliance. After that, it unearthed and published rare audio of Hillary Clinton discussing how she defended an accused child rapist as his court-appointed attorney.  Independent Journal Review, which is labeled both \"bias\" and \"unreliable,\" most recently scooped everyone with a report confirming that Trump\\'s Supreme Court justice nominee would be Judge Neil Gorsuch.  The Daily Caller, which scored a hat-trick by being labeled \"political,\" \"clickbait\" and \"unreliable,\" is responsible for uncovering the bombshell story that Michael Flynn, President Trump\\'s former national security adviser, was paid more than $500,000 to represent Turkey\\'s government even as he campaigned with the then-GOP nominee.  The Washington Examiner, which earned the titles \"political\" and \"unreliable,\" has been at the forefront of uncovering the secret service\\'s many scandals, ethical lapses and professional missteps.  Zimdars defended her system of classification, telling the Examiner, \"not every website is labeled as fake news. The tags political, unreliable, and even clickbait actually describe generally credible and verifiable content that supports a particular political perspective (tag: political), but that sometimes uses sensational/emotionally charged headlines or language (tag: clickbait) and should be read in conjunction with other sources (tag: unreliable).\"  It\\'s worth noting there\\'s a difference between \"fake news\" and biased or sloppy journalism. There\\'s a difference between a \"report\" claiming Pope Francis endorsed Donald Trump, which has no basis in reality, and journalists botching the details of a breaking news event or injecting political bias into their stories.  One problem with her database is that it groups 100 percent false stories, which are knowingly fabricated from thin air, with coverage she deems either too political, too clickbait-y or generally unreliable. There\\'s an obvious difference, however, between what the Daily Caller does and what a group of fake news teens in Macedonia do. Perhaps separate lists would help.  Zimdars concurred there is a big difference between a news group flubbing a story and websites \"consistently (or purposefully) [publishing] misleading information.\"  \"I agree that the resource should be expanded to include even more sources to demonstrate the continuum of information to misinformation and disinformation, from credible sources of information to propaganda and fake news,\" she concluded. \"Hopefully, I will be able to do that soon.\"  She stressed above all else that the database is not a list of \"fake news.\"  Someone should tell that to Harvard.  Her project first went viral last year after Hillary Clinton\\'s stunning and historic loss to Trump. It\\'s making a resurgence now following reports that Harvard is leaning on it as a resource.  As it turns out, Harvard isn\\'t even the first school to do this. City University of New York and Radford University have already instructed their students that the database should be used as a tool for identifying false information online.  A spokesperson for the Harvard library did not respond to the Examiner\\'s request for comment. California’s Democrat-controlled Legislature is moving forward with putting a constitutional amendment on the June ballot to double the corporate tax rate.  Assemblymen Kevin McCarty [D-Sacramento] and Phil Ting [D-San Francisco] announced on Jan. 18 that they will sponsor a constitutional amendment to add a 7 percent corporate tax surcharge for any profit over $1 million.  Although the Tax Foundation listed Iowa with the highest corporate tax rate at 12 percent and California only the 7th ranked at 8.84 percent, many companies use deductions to cut California’s effective tax rate to the alternative tax minimum of 6.65 percent.  But California has also won Chief Executive’s booby-prize for the Worst State for Business each of the last 6 years. The 500-top CEO survey found a “variety of measures of tax and regulatory regime” make California the most expensive state to do business.  Despite this already deeply negative environment for business, Democrats Ting and McCarty want California to charge an extra 7 percent corporate surtax that would not be subject to business deductions. Their goal is to grab the cash California corporations are saving from President Trump’s Tax Cuts and Jobs Act and spend it on social services.  The White House has been trumpeting that the corporate tax cut from 35 percent to 21 percent, has already resulted in 2 million workers getting “Trump Bonus” or “Trump Pay Raise;” the National Association of Manufacturers found their member’s optimism levels at “the highest [point] in the survey’s 20-year history;” and the Council of Economic Advisers estimates the economy will grow by “an additional 3 percent.”  But Assemblymen McCarty spoke for all of California Democrat social justice warriors when he told the Sacramento Bee, “We’ve seen enough billionaire justice from the presidency.” He added, “It’s time for middle-class tax justice.”  As a recent Vanity Fair article proclaimed, “Republican Tax Plan Is An Elaborate Middle Finger To Liberals.” According to comments by Heritage Foundation economist Stephen Moore, the tax reform limitation on state and local tax deductions (SALT) will result in a direct negative economic hit to liberal blue states, especially California.  Golden State’s taxpayers claim SALT at the highest deduction per claimant of any state at $36,802. For many Democrats, Trump’s tax cut will create a $12,000 state tax increase.  Ting and McCarty are also furious that a tax reform measure is meant to stop illegal aliens by now requiring parents who claim the child tax credits to produce valid Social Security numbers. Eliminating the widespread use of bogus Individual Taxpayer Identification numbers will eliminate hundreds of billions of California’s rampant welfare and Medicaid entitlement fraud.  If Assembly Constitutional Amendment 22 is approved by two-thirds of both the California Assembly and Senate, it could be placed on the June ballot. Ting and McCarty predict that it may increase corporate taxes by up to $17 billion and the money could be spent on expanding the earned income tax credit, college affordability, child care, and education. Belgian police shot an Afghan migrant at Ghent station Tuesday night after the man approached officers, threatening to attack them, and ignored orders to drop a knife.  Two shots were fired by the officers after the man failed to respond to orders to drop the knife, one of which struck the would-be assailant, causing him to drop his weapon. The 28-year-old man, who was described as being of Afghan origin and as wearing “a white turban” was handcuffed and taken to hospital, and was in critical condition, although that has now improved.  The station and surrounding streets were evacuated by police as a precaution after the attack.  Belgian Flemish newspaper Het Laatste Nieuws reports nothing is known about the man, his identity, or motivation — however, Belgian police have said they do not believe the “undocumented” Afghan was moved by a terror motive to take a knife to attack police at the station. Police have not yet had the opportunity to interrogate the suspect.  A police spokesman told the paper: “The motive is not yet clear… that will be fully investigated”. The paper also reported although there was no certainty over who the man was, he did not appear to be known for previous crimes and “there are no signs of radicalisation.”  Het Belang van Limburg reports the police’s remarks on available CCTV footage, which shows: “The man with a knife rushes into the station and immediately picks two members from the patrolling railway police, who fired at him.”  The newspaper also reports that rumours the man quoted verses of the Quran during the attack had not been confirmed by police. Georgetown University is scrambling to do public penance for having sold 272 black slaves—men, women and children—in order to save the financially-strapped institution in 1838, while simultaneously preparing its next appeal for forgiveness by showcasing pro-abortion activists on campus.  On Tuesday, Georgetown president John J. DeGioia wrote a letter to all members of the Georgetown University Community, inviting them to an open conversation “on the historical role that Georgetown played in the institution of slavery and the persistence of racial injustice in our nation,” to be held on campus Thursday.  The event continues an uneasy conversation sparked by revelations that the Jesuit priests who ran the nation’s top Catholic university sold hundreds of slaves to be shipped to plantations in the Deep South in order to keep the fledgling university alive.  Jesuit Father Thomas Mulledy, who served as Georgetown’s President from 1829-38 and 1845-48, personally “authorized the sale of 272 enslaved people owned by the Society of Jesus in Maryland,” DeGioia’s letter acknowledges.  Last fall, Georgetown expunged the name of Fr. Thomas Mulledy from a building that was named after him, as well as the name of another Georgetown President of the same period, Fr. William McSherry, S.J., from a second building.  At Thursday’s gathering, Fr. David Collins, S.J., who chairs the Georgetown University Working Group on Slavery, Memory, and Reconciliation, will share a report of the Working Group with all present.  DeGioia said that the Working Group was created to determine “how best to acknowledge and recognize this history; examine and interpret the history of certain sites on our campus, including Mulledy Hall; and convene events and opportunities for dialogue.”  Earlier this year, Georgetown sought to make amends to the black community by creating a department of African American Studies, developing a new center focused on racial justice, and hiring new faculty.  Ironically, however, as Georgetown struggles to purge the memory of its complicity with black slavery in America, it is simultaneously preparing the need to do so all over again regarding an issue that has been repeatedly compared to slavery: the abortion industry.  Last fall, presidential hopeful Dr. Ben Carson drew a parallel between modern abortion and the U.S. slave trade, noting that abolitionists had no right to be personally opposed to slavery, while supporting it publicly. Others have highlighted a number of similarities between the woeful 1857 U.S. Supreme Court decision in Dred Scott with the equally wrongheaded jurisprudence involved in Roe v. Wade.  As far back as 1977, the Rev. Jesse Jackson made a similar argument, noting that if one accepts the position that life is private, and therefore you have the right to do with it as you please, the same logic had to be applied to slavery. “You could not protest the existence or treatment of slaves on the plantation because that was private and therefore outside your right to be concerned,” he said.  Pastor Clenard Childress, Jr., who runs Black Genocide.org, has said that in the United States today “The most dangerous place for an African-American is in the womb,” while also underscoring the similarities between arguments for abortion and those used to justify slavery.  Childress notes that in America black babies are aborted at 3.6 times the rate of whites. Among white women, there are 138 abortions for every 1000 live births; among blacks, there are 501 abortions for every 1000 births. This means that the black community suffers wildly disproportionate losses to abortion compared to whites.  This past spring, Georgetown University invited Planned Parenthood president Cecile Richards to speak to its student body on “reproductive rights” and Richards was greeted with a standing ovation by the 400 students and teachers in attendance.  Georgetown Right to Life, along with The American Society for the Defense of Tradition, Family and Property and others, organized a demonstration on campus to protest the University’s decision to invite Richards.  At the time, protesters noted that Ms. Richards has “overseen the wholesale death of over 2.8 million babies since becoming Planned Parenthood’s president in 2006” and that her presence on campus “dishonors the memory of all the Jesuit saints and martyrs who gave up their lives for God and the Church.” A disproportionate number of those aborted children were African Americans.  It doesn’t stretch the imagination to compare this event with inviting a noted slave trader in the mid-1850s to speak on campus about the benefits of that institution to commerce, social progress and the academy.  So as Georgetown desperately tries to make amends for its complicity in slavery a century and a half ago, it hastens to assure that a future Georgetown president will be writing a similar letter and creating a similar work group to beg forgiveness for its complicity in abortion.  Some people never learn.  Follow Thomas D. Williams on Twitter Follow @tdwilliamsrome The National Abortion Rights Action League (NARAL) has reacted fiercely to recent parallels drawn between abortion and slavery, calling the comparison “shameful & vile.”  NARAL’s tweet Sunday that abortion “is a basic human right—not slavery” and slamming comparisons between the two institutions came in reaction to a Huffington Post article attacking Judge John K. Bush, newly appointed to the 6th U.S. Circuit Court of Appeals, who has compared abortion to slavery.  African pro-life activist Obianuju Ekeocha retweeted the NARAL message, saying she wanted to preserve the comment for posterity, “so that one day long after we are dead, there will be records of the inhumane sentiments that held up abortion.”  I’m leaving this comment here so that one day long after we are dead, there will be records of the inhumane sentiments that held up abortion https://t.co/34BpZ9c7i6 — Obianuju Ekeocha (@obianuju) August 5, 2017  Abortion, like slavery in America, targets the black community in a particular way, and has been decried by countless black leaders as racist in its very roots.  Earlier this year, former presidential candidate Alan Keyes called abortion “more evil than slavery,” arguing that many Americans today hold the same underlying prejudices that made slavery possible in the 19th century.  Many Americans contend that “without abortion rights, women cannot be free,” Keyes wrote. “Thus they hold that the individual’s freedom to murder nascent human offspring is an indispensable prerogative of American citizenship, which must be protected with the coercive force of law.”  In its internal logic, Keyes said, this is “identical to what the proponents of race-based slavery said about the freedom of their communities and states.”  Alveda King, the niece of civil rights champion Rev. Martin Luther King Jr., campaigns non-stop for the pro-life cause and does not shy away from calling it the great civil rights issue of our day. Having had two abortions (one against her will) before becoming committed to the pro-life movement, Ms. King speaks from a position of experience and authority.  Moreover, Ms. King found out later in her life that her own mother had wished to abort her, but was prevented from doing so by her grandfather.  “My mother wanted to abort me, and that was basically a family secret,” she said. “My grandfather stopped her and said that he had a dream and saw me perfectly. He was a prophetic dreamer, like Martin.”  In the lead-up to last November’s election, a group of leading black Christian clergy and intellectuals wrote an “open letter” to Democratic Presidential candidate Hillary Clinton, denouncing Clinton’s complicity in America’s abortion industry, which has had a “catastrophic impact” on the black community.  The 26 prominent black Christian leaders, including eight African American bishops, assailed Clinton for what they call her “unconscionable silence in the face of such destruction of innocent black life,” through her vigorous support of the U.S. abortion industry.  “Black babies are dying at terrifying rates,” states the letter, a copy of which was sent to Breitbart News. “Don’t black lives matter?”  In their letter, the leaders noted that the rate of abortion among blacks is far higher than among whites, with “365 black babies aborted for every 1,000 that are born.”  “Blacks account for roughly 38% of all abortions in the country though we represent only 13% of the population,” they said, citing statistics that underscore the heavily racist component of abortion in America.  The Reverend Clenard H. Childress has called this phenomenon “black genocide,” building a national ministry around its exposure. Childress cites an estimate that since 1973 black women in America have had some 16 million abortions, an enormous loss. Without it, America’s black community would now number 52 million persons, he suggests.  The abortion movement, spearheaded by the International Planned Parenthood Federation (IPPF), has been no friend to blacks, despite their rhetoric to the contrary.  Margaret Sanger, founder of Planned Parenthood, the nation’s largest abortion provider, was a notorious racist and eugenicist, and worked actively to reduce the black population. As part of the eugenics movement in the 1930s, Sanger thought that abortion could effectively cull “inferior races” from the human gene pool.  Sanger chose inner cities as the sites for her first abortion clinics, and still today, 79 percent of Planned Parenthood’s abortion facilities are located in black or minority neighborhoods.  Planned Parenthood’s research and propaganda arm, the Guttmacher Institute, was named after former Planned Parenthood president Alan Guttmacher, who was also Vice-President of the American Eugenics Society.  Guttmacher was an advocate of coercive population control, and sought the involvement of the United Nations to achieve a significant and targeted reduction of population. “My own feeling,” he said in an interview in 1970, “is that we’ve got to pull out all the stops and involve the United Nations.”  “If you’re going to curb population, it’s extremely important not to have it done by the damned Yankees, but by the UN. Because the thing is, then it’s not considered genocide. If the United States goes to the black man or the yellow man and says slow down your reproduction rate, we’re immediately suspected of having ulterior motives to keep the white man dominant in the world. If you can send in a colorful UN force, you’ve got much better leverage,” he said.  Follow Thomas D. Williams on Twitter Follow @tdwilliamsrome For the last three years I’ve taken a week in January to write. Friends from my “Preacher Camp” have been part of this retreat, and as I type these words I’m again looking out a beautiful bay window of a little farm house that rests peacefully on the bank of the Choptank River, an hour east of D.C.  The wood stove behind me creaks softly as the warming metal pings and rings from a new load of split oak. Jenny, a beautiful and energetic golden retriever occasionally stirs the air as she rises to stretch or to nudge me in playful invitation. John is sitting by the fire ruffling through some pages of his current creativity. Otherwise the air is quiet. The world is at peace.  Or not. But it’s an illusion I need to enjoy for a few short days.  It’s not a feeling I’ve been able to muster through the last fortnight. The chaos constantly emanating from the White House sent a new series of disturbing ripples through the air two weeks ago, and they instantly expanded around the globe in enveloping, concentric circles of disbelief and anger and fear.  Since that disturbing Oval Office pronouncement there have been thousands of opinions written across the political spectrum, endless hours dedicated to punditocracy in the marketplace, countless words of pulpiteering offered by the Church.  And I’ve been afraid to speak.  This national moment may be an all-time low during my adulthood. Yes, there have been egregious presidential sins in my lifetime (red and blue), lies and fraud and corruption and abuse, but none of those individual failures has so captivated the entire nation in a descending vortex of disrespect and division and dissonance. And given the dysfunction of the cultural moment, none has had any more potential to become part of our moral or spiritual undoing.  And I’ve been afraid to speak.  Some say the whole issue is overblown, but civility and decency are too important to the soul of a nation for people with any respect for their government or for their own honor to ever take this kind of behavior lightly.  Some say he didn’t say it, but we’ve all heard so much that sounds just like this, and his “base” loves it precisely because they know he said it. No, the denials are another unfortunate distraction, and yet another assault on truth, which threatens the integrity of a people.  Some agree he said it, but claim he wasn’t being racist. But to be not-racist requires a concern for the racism that still casts a shadow of doubt on our claim that all are “created equal” — by being consistent and careful with the power of words. To be unaware of the harm of stereotypes or careless with the hurt of generalizations of words is to make real the hypocrisy of hollow claims. In the absence of a history that can be confirmed with integrity, words may actually speak louder than actions.  Some say it’s just more locker room talk, but coarse vulgarity is a sad part of the American vernacular — no one even raises an eyebrow anymore — so it’s hardly the profanity that should concern us. But any policy that would judge people’s worth by their national origin or their religion or by the color of their skin, not the content of their character, is a threat to our faith as well as to our only, true American exceptionalism:  Give me your tired, your poor, your huddled masses yearning to breathe free,  the wretched refuse of your teeming shore,  send these, the helpless to me.  I lift my torch beside the golden door.  The “Harbor Lady” stands knee-deep in Atlantic waters to beckon the people of the world to “life, liberty, and the pursuit of happiness.” She stands to offers the least of these an opportunity to join the exceptional experiment called democracy, not because they are worthy but because democracy, like faith, has the power to make us free to share and unafraid of any tomorrow — if we actually practice it. Any policy in contradiction to her light of welcome is as unpatriotic as it is unchristian.  So, I said it. And now I’m afraid that I’ve spoken.  It’s just that at this moment in our fragile history, I’m even more afraid to remain silent. The interim leadership of the Consumer Financial Protection Bureau has decided to reconsider regulatory rules for short-term, high-interest borrowing commonly known as payday lending. The federal agency, which was created in response to the financial crisis of 2007-2008 and the… Please submit transitions — including staff changes, ordinations, anniversaries or deaths — to Barbara Francis. This page will be updated weekly. STAFF CHANGES Kent Berghuis, to First Baptist Church, Dayton, Ohio, as lead pastor. Previously he was senior pastor of First… Russ Dean  Russ Dean is co-pastor of Park Road Baptist Church in Charlotte, N.C. A native of Clinton, S.C., and a graduate of Furman University and Southern Baptist Theological Seminary, he earned a D.Min. degree from Beeson Divinity School. He and his wife, Amy, have been in church ministry for 30 years, and they have served as co-pastors of Park Road since 2000. He is active in social justice ministries and interfaith dialogue, and when he isn’t writing sermons or posts for Baptist News Global you’ll find Russ in his shed doing wood working, playing jazz music, slalom or barefoot water skiing, hiking and camping, or watching his two teenage boys on the baseball field. La versión en español está disponible aquí.  Last week my mother died unexpectedly. She was 86 years old, and had struggled with Alzheimer/dementia for the last 12 years. This sickness advances slowly and gradually, and little by little family and friends lose the person whom they loved.  My mom seemed to be stable within her sickness, so her passing came as a surprise. I was with a dear friend at a Starbucks, when I received the news about her critical condition. As updates continued to come in, surprisingly, I was quite calm and composed.  Knowing that an unexpected trip was imminent, I went home, checked plane tickets, and took care of some urgent work related matters. The news of my mom’s passing arrived, and I prayed, placing her in God’s hands, and thanking God for receiving her into her eternal home.  As I talked to my brother that evening, we discussed my calm mood, and it became clear to me that I had been grieving for 12 years. Every time that I would have contact with my mom and that I discovered that she could no longer do this or that, I silently grieved. In addition, I felt that I did not have many regrets in my relationship with her. Of course, we had regular struggles as a mom and daughter, but in general, I can say that we had a good relationship, especially during my adult years.  Thankfully, I had a chance to reflect on this before I flew to Mexico. Thus, her funeral and burial became for me a true celebration of her life. Under normal circumstances, funerals and burials in Mexico must be performed within the first 24 to 48 hours after a person’s death. Funerals usually span two days, with long hours of visitation.  In light of this, I had a chance to spend much time with my mom’s friends and church family. As I listened to many stories about her, the predominant themes were related to her generosity, hospitality, great sense of humor, wonderful cooking abilities, writing skills and love for God, church and ministry. A thread that knitted many of these stories together was how she was a woman well ahead of her time. For instance, she was really progressive in her defense of women’s rights. She never called herself a feminist, but certainly she lived like one. I use the word “feminist” to describe a person, woman or man, who is in favor of women’s rights.  Surely, and to my kids’ regret, I do not take after my mom’s cooking abilities, and I wish I had more of her sense of humor. However, I take after her writing skills, love for God, church and ministry, and her defense of women’s rights. For anyone who is interested in women’s issues, her story is fascinating.  My mom was born in 1931 and due to diverse circumstances she could not attend college. Since she was extremely smart, her parents and she decided that she would attend a technical school to become a bilingual secretary. After she graduated, she had a fruitful and ascending career that achieved its climax when she became the executive secretary to the general director of an important, multinational glass factory. One time she told me that at a certain point, she made more money than her dad (her dad had a good job, too). Another time she told me (I can still feel her pride and pain), that this important businessman had mentioned that the only thing that she was missing, professionally speaking, was that she was not a man.  This brilliant, ascending career, ended the day that she got married at age 26. At that time, women could not have both; they either worked professionally or got married. Thus, she was automatically terminated the day after she got married. She was content with her marriage, but she wanted more. So, in a visionary and astute way, she crafted her own space.  Since no one would hire her due to her marital status, she became a translator who worked from home. One of her clients was a man who was involved in the shoe business. By translating his business deals, she learned the craft, and eventually started her own shoe store. In a creative way, my mother constructed a space where she was able to have a fulfilling life, in spite of the oppressive patriarchal system around her. My parents built a house where the store and the home were connected. So I grew up with a mom who would take care of her family and cook deliciously, and as she crossed literally a door, I would observe her transform herself into this wise, savvy business woman. For years, she would cross boundaries in powerful and graceful ways.  Throughout the years, people have asked me: When did you become a feminist, an advocate of women’s rights? I have always answered: “I do not remember. I did not become one, I was born one.”  As I reflect more and more on my mom’s story, I can say “yes,” I was born a feminist because I was carried, nursed and raised by one. And the story continues ….  My call to become an advocate for women’s rights started in the womb and arms of this woman, who had to open spaces by and for herself. She was oppressed by a patriarchal system and worldview, but still she felt empowered by God, and believed that she could have the best of both worlds: family and professional work. As she opened spaces for herself, she also opened the eyes, dreams and imagination of many women who were observing her, including me.  Due to her dementia, my mom never knew clearly about my ministry with the Christian Latina Leadership Institute. Had she known, she would have been very proud of this work and me, and for sure she would have been a strong supporter and a generous donor.  As I celebrate her life, I continue to feel empowered by her story. One that connects her and me, and the other women in my life (my daughter, sisters, nieces, grandmothers, aunts, great aunts, friends, mentors, colleagues, students, and powerful biblical women characters) in a strong chain of love, support, transformation, and hope.  As I stand on a giant’s shoulders, here I am, called more than ever, to continue my work of empowering and opening spaces for women. My hope is that in God’s timing and with God’s blessings, one day no woman in the world will suffer oppression and limitations, as my mom did, just because of her gender.  In the meantime, I am thankful for God’s power that keeps moving women around the world to open their own spaces, to be fulfilled and content, and to become powerful and inspiring role models for the next generation of women.  So pressing forward, I move ahead with the trust and confidence that my mom’s life and my own, are but a link in a powerful chain that eventually, in God’s timing and horizon, will produce a just world for all human beings, women and men alike. May it be so! A federal judge in Mississippi has turned down a request to dismiss a lawsuit against the North American Mission Board of the Southern Baptist Convention on religious liberty grounds.  Senior U.S. District Judge Glen H. Davidson ruled Jan. 18 that claims by a former Baptist state convention executive that leaders of the mission agency orchestrated his firing and tried to interfere with other business arrangements can move forward.  NAMB lawyers had argued the dispute with former Baptist Convention of Maryland/Delaware Executive Director Will McRaney was over internal workings of a religious organization and not subject to judgment by a secular court.  The judge disagreed, however, finding that a First Amendment doctrine that limits civil courts from resolving disputes about ecclesiastical polity applies only to employer/employee relationships.  Because the 560-church two-state convention is a separate organization from NAMB, the judge said the “ministerial exception” does not come into play.  The judge said a similar doctrine known as the “ecclesiastical abstention” also does not bar the case, because it does not necessarily delve into matters of religious doctrine.  McRaney claims that NAMB officials pressured state convention leaders to fire him in 2015 by threatening to withhold $1 million a year in denominational funding. He further alleges that national leaders tried to get him disinvited from speaking engagements and defamed him by posting a photograph at NAMB headquarters portraying him as a troublemaker.  Judge Davidson did throw out an interference claim in Florida, saying that since his participation in that particular event was not canceled, McRaney suffered no damages.  Since his departure from Maryland, McRaney has written extensively arguing the “new NAMB” under President Kevin Ezell isn’t working for Southern Baptists.  Previous stories:  ‘Ministerial exception’ not a shield for wrongdoing, former Baptist leader says in lawsuit  Former state exec sues NAMB for libel, interference in business relationships  State Baptist exec blames firing on interference from SBC agency  State convention ‘saddened and disappointed’ by claims of former exec Please submit transitions — including staff changes, ordinations, anniversaries or deaths — to Barbara Francis. This page will be updated weekly.  STAFF CHANGES  Kent Berghuis, to First Baptist Church, Dayton, Ohio, as lead pastor. Previously he was senior pastor of First Baptist Church, Oklahoma City, Okla.  Trevor Brown, to First Baptist Church, Amarillo, Texas, as associate pastor. He comes from First Baptist Church, Round Rock, Texas, where he was minister to students.  Larry Fields, to Oakwood Baptist Church, Knoxville, Tenn., as pastor.  Kim Gunn, concluding her tenure as minister to children at New Highland Baptist Church, Mechanicsville, Va., effective Feb. 28.  David Hughes, to First Baptist Church, Augusta, Ga., as minister to students, effective March 5. He recently served as youth minister at First Baptist Church, Easley, S.C.  Gina C. Jacobs-Strain, to American Baptist Women’s Ministries, as executive director, effective Feb. 1. She is department chair of the Division of Student Success at Nyack College, Nyack, N.Y., and associate regional pastor for Women in Ministry for American Baptist Churches of New Jersey.  David Jordan, to First Baptist Church, Decatur, Ga., as senior pastor, effective March 11. He comes from Providence Baptist Church, Charlotte, N.C., where he was teaching pastor.  Suzette Mason, to First Baptist Church, Norman, Okla., as minister of discipleship and children. She comes from First Baptist Church, Tallahassee, Fla., where she was minister of children.  Enrique Newman, to North Scituate (R.I.) Baptist Church, as interim pastor.  Scotty Robertson, to First Baptist Church, Middletown, Ohio, as pastor. He comes from the pastorate of Mill Creek (Ind.) Baptist Church.  Todd Tenaglia, to Karl Road Baptist Church, Columbus, Ohio, as children’s director.  Hal West, to First Baptist Church, Walterboro, N.C., as transitional pastor.  Baptist News Global provides a free listing of ministry-related jobs for Baptist churches, theological institutions and organizations across the United States. Click here to learn more.  RETIREMENTS  James E. Bennett Jr., retiring as minister of music and worship at First Baptist Church, Aiken, S.C., where he has served since 1997.  Randolph Miller, retiring as pastor of Eastwood Baptist Church, Medford, Oregon.  ORDINATIONS  Linsey Addington, ordained to ministry on Jan. 21 by Church at Ponce and Highland, Atlanta, Ga.  David Klock, ordained to ministry by Colonial Park Community Church, Harrisburg, Pa., on Jan. 7.  DEATHS  William S. Fairhurst III, 86, died on Jan. 5 in Oneonta, N.Y. He served American Baptist churches in Millers Mills, Weedsport, Meridian and Rose in New York, as well as churches in Maine and New Hampshire. He is survived by his wife, Mary Louise; children, Deborah Cooper, Cynthia Ploutz, Jennifer Bowdoin, Patricia Fairhurst and William Fairhurst; 14 grandchildren and four great-grandchildren.  Dale Lee Gore, 83, died Jan. 18 in Grand Prairie, Texas. He had served as pastor of these Texas Baptist churches: Baggett Creek in Gustine; First Baptist in Whitewright; First Baptist in Goldthwaite; Trinity in Sherman; First Baptist in Denison; and Elm Grove in Belton. He was director of missions for Grayson and Austin Baptist associations. He served on the executive board of the Baptist General Convention of Texas, the Texas Baptist Children’s Home board and the Howard Payne University board of trustees. He is survived by his wife, LaVerne; two sons, Dennis and Jeffrey; daughter, Jane Cluck; nine grandchildren and four great-grandchildren.  KUDOS  STAFF ANNIVERSARY  Brian Lockamy, 20 years as associate minister for students at New Hope Baptist Church, Raleigh, N.C.  In case you missed them:  Transitions for the week of 01.19.18  Transitions for the week of 01.12.18  Transitions for the week of 01.05.18  Transitions for the week of 12.29.17 It seems religion in America just can’t catch a break. Religious affiliation and participation continue to dive, driven in part by young generations uninterested in belonging to anything, least of all churches. And now newly published research suggests that the Internet — which faith communities had hoped would bridge the gap with Millennials and the “nones” — may have a hand in keeping those and other demographics away. Please submit transitions — including staff changes, ordinations, anniversaries or deaths — to Barbara Francis. This page will be updated weekly.  STAFF CHANGES  Kent Berghuis, to First Baptist Church, Dayton, Ohio, as lead pastor. Previously he was senior pastor of First Baptist Church, Oklahoma City, Okla.  Trevor Brown, to First Baptist Church, Amarillo, Texas, as associate pastor. He comes from First Baptist Church, Round Rock, Texas, where he was minister to students.  Larry Fields, to Oakwood Baptist Church, Knoxville, Tenn., as pastor.  Kim Gunn, concluding her tenure as minister to children at New Highland Baptist Church, Mechanicsville, Va., effective Feb. 28.  David Hughes, to First Baptist Church, Augusta, Ga., as minister to students, effective March 5. He recently served as youth minister at First Baptist Church, Easley, S.C.  Gina C. Jacobs-Strain, to American Baptist Women’s Ministries, as executive director, effective Feb. 1. She is department chair of the Division of Student Success at Nyack College, Nyack, N.Y., and associate regional pastor for Women in Ministry for American Baptist Churches of New Jersey.  David Jordan, to First Baptist Church, Decatur, Ga., as senior pastor, effective March 11. He comes from Providence Baptist Church, Charlotte, N.C., where he was teaching pastor.  Suzette Mason, to First Baptist Church, Norman, Okla., as minister of discipleship and children. She comes from First Baptist Church, Tallahassee, Fla., where she was minister of children.  Enrique Newman, to North Scituate (R.I.) Baptist Church, as interim pastor.  Scotty Robertson, to First Baptist Church, Middletown, Ohio, as pastor. He comes from the pastorate of Mill Creek (Ind.) Baptist Church.  Todd Tenaglia, to Karl Road Baptist Church, Columbus, Ohio, as children’s director.  Hal West, to First Baptist Church, Walterboro, N.C., as transitional pastor.  Baptist News Global provides a free listing of ministry-related jobs for Baptist churches, theological institutions and organizations across the United States. Click here to learn more.  RETIREMENTS  James E. Bennett Jr., retiring as minister of music and worship at First Baptist Church, Aiken, S.C., where he has served since 1997.  Randolph Miller, retiring as pastor of Eastwood Baptist Church, Medford, Oregon.  ORDINATIONS  Linsey Addington, ordained to ministry on Jan. 21 by Church at Ponce and Highland, Atlanta, Ga.  David Klock, ordained to ministry by Colonial Park Community Church, Harrisburg, Pa., on Jan. 7.  DEATHS  William S. Fairhurst III, 86, died on Jan. 5 in Oneonta, N.Y. He served American Baptist churches in Millers Mills, Weedsport, Meridian and Rose in New York, as well as churches in Maine and New Hampshire. He is survived by his wife, Mary Louise; children, Deborah Cooper, Cynthia Ploutz, Jennifer Bowdoin, Patricia Fairhurst and William Fairhurst; 14 grandchildren and four great-grandchildren.  Dale Lee Gore, 83, died Jan. 18 in Grand Prairie, Texas. He had served as pastor of these Texas Baptist churches: Baggett Creek in Gustine; First Baptist in Whitewright; First Baptist in Goldthwaite; Trinity in Sherman; First Baptist in Denison; and Elm Grove in Belton. He was director of missions for Grayson and Austin Baptist associations. He served on the executive board of the Baptist General Convention of Texas, the Texas Baptist Children’s Home board and the Howard Payne University board of trustees. He is survived by his wife, LaVerne; two sons, Dennis and Jeffrey; daughter, Jane Cluck; nine grandchildren and four great-grandchildren.  KUDOS  STAFF ANNIVERSARY  Brian Lockamy, 20 years as associate minister for students at New Hope Baptist Church, Raleigh, N.C.  In case you missed them:  Transitions for the week of 01.19.18  Transitions for the week of 01.12.18  Transitions for the week of 01.05.18  Transitions for the week of 12.29.17 Baptist News Global provides a free listing of ministry-related jobs for Baptist churches, theological institutions and organizations across the United States. Each posting is for 30 days and is limited to 150 words. Businesses may purchase a post in the “And More” section for $1.20 per word (minimum of $50 for 30 days). To submit a ministry-related job or inquire about other advertising options on this page, contact Barbara Francis at 336-717-1135, ext. 8 or [email protected].  MINISTRY JOBS  California  COMMUNITY PASTOR. First Baptist Church, Pasadena, CA, outside of Los Angeles, is seeking a Community Pastor to join our awesome team. This is a full-time position with benefits. Our church is affiliated with the American Baptist Churches family. Full details about the position are at https://fbcpasadena.com/community-pastor or by emailing [email protected] for more info. Candidates accepted until the position is filled. We will begin reviewing applications in January of 2018. (Posted 12.13.17)  Back to top of page    Georgia  MINISTER OF SPIRITUAL DEVELOPMENT. First Baptist Church of Roswell, GA, is seeking a full-time minister of spiritual development who will lead the church in planning, conducting and evaluating a comprehensive, well-rounded, multifaceted Christian education and spiritual formation program for all adult age groups and stages of faith. The minister will also develop and administer a plan of outreach, assimilating new members into the spiritual development program of the church. A seminary degree is expected; a minimum of three to five years of congregational experience is preferred. A full job description is available at www.fbroswell.org. Review of applications will begin March 1. A cover letter and resume may be sent to [email protected] or to the attention of the Search Committee, First Baptist Church, 710 Mimosa Blvd., Roswell, GA 30075. (Posted 01.16.18)  ASSOCIATE PASTOR. First Baptist Church of Griffin, GA, a moderate congregation, is seeking an Associate Pastor to join our ministry staff. He or she needs to have strong people, preaching, and management skills. This person must work well as a team with other ministers and focus upon the entire congregation. However, he/she will lead and preach weekly in the Connexion worship ministry, our contemporary service that meets at the same time as our traditional service. This candidate will at times throughout the church year also preach in the traditional service. Preference will be given to candidates with an M.Div. degree from an accredited seminary and a minimum of four years of church ministry/preaching experience. Resumes and cover letters will be received at [email protected]riffin.org until Feb. 15, 2018. (01.16.18)  MINSTER TO STUDENTS. First Baptist Church of Griffin, GA, a moderate congregation, is seeking a skilled and passionate full-time minister to students to join a staff of four other ministers. This staff minister, male or female, is responsible for planning, coordinating and evaluating a comprehensive ministry to students, grade 6 through college. A seminary graduate is preferred. Salary is commensurate with experience. Candidates should send a resume and cover letter to [email protected] no later than Feb. 15, 2018. (01.16.18)  Back to top of page    Indiana  PASTOR. Westport Baptist Church, an American Baptist Church in Westport, Indiana, seeks an energetic pastor to guide them in the next chapter of their ministry. The applicant will show evidence of a servant leader’s heart; have strong and challenging Biblically-based teaching and preaching ability; and be passionate about evangelism and discipleship, while assisting the church in reaching its community with the Good News of Jesus Christ. This is a lively congregation of 85-100 in worship, with a history of long-term pastoral leadership, strong youth programming and community ministry/involvement. The church owns a building across the street from the church structure, used for Sunday School and many community activities. A graduate theological degree is expected and recognition of one’s ordination by American Baptist Churches will be required. Please send resume’ or ABPS profile to Rev. Soozi Whitten Ford ([email protected]). Church website: www.westportbaptistchurch.org. (Posted 01.05.18)  SENIOR PASTOR. American Baptist Church in city in east central Indiana (population 35,664) seeking a Senior Pastor. This is a full-time position in a church which runs approximately 205 in worship with an annual budget of $435k. The church seeks a pastor who is a solid biblical teacher who would work well with strong leaders in a healthy congregation. American Baptist experience and ordination preferred, as well as a minimum of five years of experience in pastoral ministry. Immediate start date. Please send resumes or contact Phyllis Goodyear, Region Minister for American Baptist Churches of Indiana and Kentucky at [email protected]g. (Posted 11.20.17)  Back to top of page    Mississippi  SENIOR PASTOR. University Baptist Church, Hattiesburg, MS, is seeking a Senior Pastor. We desire a dynamic and energetic minister with strong preaching skills, and at least five years of ministry experience. Candidates should hold a masters or doctoral degree from an accredited seminary or divinity school. We are affiliated with CBF and Alliance of Baptists and adhere to historical Baptist principles of priesthood of the believer and the autonomy of the local church. UBC is an open and affirming congregation and supports both women and men in ministry positions. The Senior Pastor will serve as the leader of the church’s pastoral ministries and will work with deacons, standing committees and the co\\xad\\xad\\xad\\xad\\xad\\xad\\xad\\xadngregation to minister to members and others in our community. Salary and benefits will depend on qualifi\\xad\\xadcations and experience and will range from $65,000 to $75,000. Hattiesburg is a city of 50,000 and home to The University of Southern Mississippi and William Carey University, two community colleges and a large regional public health facility, Forrest General Hospital.For more information on University Baptist Church, please visit www.ubchm.org. Submit cover letter and resume to [email protected]. Deadline for applications is February 1, 2018. (Posted 12.12.17)  Back to top of page    Missouri  ASSOCIATE PASTOR FOR YOUTH AND COMMUNITY ENGAGEMENT. Kirkwood Baptist Church, Kirkwood, MO, is seeking a creative, energetic, and motivated person to work with our youth and be the staff liaison for our community engagement opportunities. This position will require a person to take initiative and work both independently and as part of a four-person ministerial team. If you are interested, please send your resume to [email protected]. Website: http://kirkwoodbaptist.org. (Posted 01.16.18)  Back to top of page    North Carolina  PASTOR. Gravel Hill Baptist Church, Denton, NC, is seeking a full-time or would consider bi-vocational Pastor. We are a 110 year old KJV Southern Baptist Church with a congregation of 50-100 with a vibrant youth ministry. We are seeking a Spirit filled Man of God to serve as under Sheppard of our flock. If interested please send resume with salary expectations to [email protected] which will be forwarded to our search committee. (Posted 01.22.18)  ASSOCIATE PASTOR. Pullen Memorial Baptist Church, near downtown and NCSU in Raleigh, NC, seeks an Associate Pastor to serve full time with full benefits. Pullen is a 130 plus year old social-justice oriented, progressive, ecumenical, inclusive, welcoming and affirming ABC/Alliance of Baptists church. We believe in a radical vision that all people are created in the image of God, an expansive love beyond humanity’s limits. We affirm that there are many paths to God and that no one person or religion holds all the truth. Read more about us on our website, especially http://www.pullen.org/our-touchstone/ and http://www.pullen.org/partnerships-and-affiliations/. Qualifications: Ordained minister in the Christian tradition. Graduation from an accredited college or university with a bachelor’s degree; graduation from an accredited seminary or graduate school with a Master of Divinity or comparable degree, and at least five years of experience in Christian education, pastoral care, work with seniors, young adults, and/or congregational life activities, supervisory responsibilities, or comparable work experience. Send resume and cover letter to [email protected] or mail to Pullen Memorial Baptist Church, Attn: Associate Pastor Search Committee, 1801 Hillsborough Street, Raleigh, NC 27605. Resumes will be accepted until the position is filled, but apply by February 15th for assured consideration. Position open after May 1. For more detailed information, please review the Associate Pastor job description at www.pullen.org/jobs. (Posted 01.16.18)  Back to top of page    Pennsylvania  FULL-TIME PASTOR. The First Baptist Church of Lewisburg, founded in 1844 and located in Lewisburg, PA, the home of Bucknell University, is seeking a full-time pastor. We are an American Baptist Church in the tradition of Roger Williams thus we accept no humanly devised confession or creed as binding. As a welcoming and affirming congregation, we come from a variety of backgrounds and embrace a pluralism of race, ethnicity and gender. We acknowledge that there are individual differences of conviction and theology among our members, hence, our credo “Free to think, Bound to serve.” Responsibilities for the position include: Provide a teaching ministry based on theological, educational and historical foundations; Communicate a comprehensive understanding of the Bible in terms relevant to person’s lives; Create an atmosphere in which people feel accepted and included; Provide opportunities for individual, couples and/or family counseling; Support the Church’s community outreach through existing programs; and make formal and informal connections with church members and others in their homes or other settings. Preferred skills include: An accredited, ordained minister; strong preaching and pastoral skills; participation in the life and activities of the church; church growth planning experience; an interest in continuing education; and the ability to communicate with people of all ages and backgrounds. Recent graduates of an accredited Christian seminary are also encouraged to apply. For additional information and to receive a copy of our Church profile, please contact Mike Wilson, Pulpit Committee Chair at [email protected]. (Posted 01.16.18)  Back to top of page    South Carolina  FULL-TIME MINISTER OF MUSIC. Oakland Baptist Church, Rock Hill, SC, a theologically moderate church located in Rock Hill, SC, is seeking a Minister of Music. The Minister of Music will be a devoted follower of Christ, hold a music degree, and have a background that includes theological training, experience as a church musician conducting choirs and handbells, and the ability to participate in and/or provide guidance for traditional and non-traditional services. Please visit https://oaklandchurch.com/music-search/ for more information on this position. Send resumes to [email protected]. (Posted 01.18.18)  PASTORAL RESIDENT. First Baptist Church, Greenville, SC, is seeking a recent seminary graduate to serve as pastoral resident. The pastoral residency program offers an exciting opportunity for the resident to gain invaluable pastoral experience in collaboration with an innovative pastoral staff and a “teaching congregation.” Over the course of two years the pastoral resident will engage in various responsibilities and educational opportunities that will expand the resident’s knowledge and skills, as well as deepen their sense of pastoral identity and authority. We anticipate the resident beginning in June 2018. The position is salaried and with benefits. Potential candidates should go to http://firstbaptistgreenville.com/pastoral-residency-program/ for more information and an application. (Posted 01.17.18)  ASSOCIATE YOUTH MINISTER. First Baptist Church, Greenville, SC, is seeking an Associate Youth Minister. This is a full-time staff position with benefits. This position is an entry-level position, assisting our full time Minister of Youth and Recreation and is designed for recent seminary or divinity school graduates. Using their gifts and talents for ministry, they should have a passion for ministering to youth and their families ages 6th grade through 12th grade in a large youth ministry. This position offers foundational experience and an expansion of knowledge and skill in collaboration with an innovative pastoral staff and youth minister with 10 years of experience. The Associate Youth Minister will report directly to the Minister of Youth and Recreation. Full Job Description and Application: http://firstbaptistgreenville.com/associate-youth-minister/. (Posted 01.16.18)  Back to top of page    Texas  FULL-TIME MINISTER OF MUSIC AND OUTREACH. University Baptist Church, Austin, TX, is seeking a Minister of Music and Outreach who is responsible for leading music for worship and directing the choir at UBC. We are seeking a musical, creative individual who can work collaboratively with pastoral staff and lay leaders. Learn more at www.ubcaustin.org. Send resumes to [email protected]. (Posted 01.18.18)  SENIOR PASTOR. Second Baptist Church of Lubbock, Texas, a moderate CBF church, is seeking a senior pastor for only the fifth time in 59 years. If interested, provide a resume to [email protected]. (Posted 12.12.17)  Back to top of page    VIRGINIA  SENIOR PASTOR. Glen Allen Baptist Church, Glen Allen, VA, is seeking a senior pastor to provide Christian based leadership for the life and work of Glen Allen Baptist Church; develop meaningful services of worship; provide pastoral care to the people within the church and community; cultivate the development and direction of the spiritual leadership for all staff personnel. The candidate should be a graduate of an accredited college or university, a Master of Divinity is preferred. Five plus years of experience as a pastor in a Baptist church with experience in managing others is desired. Applications can be sent to [email protected]. (Posted 01.23.18)  MINISTER TO FAMILIES WITH YOUTH. First Baptist Waynesboro, VA is seeking to hire a full-time Minister to Families with Youth. Applicants should have experience and a passion for ministering alongside youth and their families. Candidates with a degree from an accredited seminary and/or experience in a congregational setting are preferred. First Baptist Waynesboro is affiliated with the Baptist General Association of Virginia and the Cooperative Baptist Fellowship. Candidates must be willing to serve in a context that is theologically diverse, to embrace women and men in all levels of leadership, to understand child development, and to value a holistic (spiritual, mental, physical, and emotional) approach to meeting the needs of the community. Resumes with a cover letter may be submitted to [email protected]. (Posted 01.16.18)  SENIOR PASTOR. Kilmarnock Baptist Church, Kilmarnock, Va., is searching for a senior pastor that feels led to our church. The church is located in a rural waterfront community in Kilmarnock on the Northern Neck of Virginia. We are seeking a leader who is excited about his/her calling; exhibits interpersonal skills described in I Corinthians 13:4-7; is dynamic and energetic; possesses strong preaching skills; and is vigilant in maintaining contact with all church members. Our church practices community involvement with other area churches and is rich with seniors who are active in missions. The pastor, staff and church members, working together, will strive to be effective witnesses to spread the gospel of Jesus Christ in our community. We are aligned with BGAV. Submit your resume to [email protected] before January 31, 2018. Website: http://www.kilmarnockbaptist.org. (Posted 12.14.17)  Back to top of page    AND MORE  Back to top of page Advertise  Reach your audience and customers with Baptist News Global  Baptist News Global is a reader-supported, nonprofit news organization providing original and curated news, opinion and analysis about matters of faith to more than 3.2 million readers annually. BNG covers people, events and ideas that are shaping American culture and Baptist life.  Numerous advertising, sponsorships and underwriting options are available on  • Baptistnews.com, our signature website  • BNG Headlines, our daily email service  • Herald, our magazine published five times a year  • Two of the most popular pages on our website: Ministry Jobs, postings by churches searching for pastors and church staff, and Transitions, our compilation of ministry changes across Baptist life  By marketing through Baptist News Global, you will reach a highly-engaged audience that includes  • pastors, staff and other key leaders in more than 4,500 churches nationwide  • students and faculty at faith-based universities, theology schools and seminaries  • leaders of organizations affiliated with national and regional Baptist networks and other Christian groups  • Christian women and men who are influencers in their communities  Your advertisement or content sponsorship also demonstrates your support for an independent, trustworthy source of news and opinion for Baptists and other Christians.  BNG’s “Classifieds” for Churches and Other Ministries. Reach your audience as a Sponsor of this popular page.  BNG’s Media Guide provides an overview of our online and print readership, BNG’s advertising policy and discounts available with advertising packages. To request a guide, contact:  Barbara Francis  Advertising Manager  [email protected]  336.717.1135, ext. 8  804-698-0749 In the last couple of months, the Black Lives Matter sign that hangs from a post on the front lawn of our church has been vandalized several times. Sometimes the metal sign has been severely bent. Other times, someone has… An ancient tension in Christian discipleship often trips us up. Are we capable of doing what God asks us to do? If we are, why do we not follow the divine guidance more? This debate is as old as Augustine… We are living in a time in the United States when helping seems to be a zero sum game. If we tend to the well being of others, we are somehow diminished and our economic security is compromised. The erosion of compassion leads to a spiritual death, and the ability to inure ourselves to the needs of others threatens the common good. Many churches around the globe are celebrating the 500th anniversary of Martin Luther’s radical upheaval of medieval Catholicism. What he set in motion on Oct. 31, 1517, as he posted his 95 Theses on the door of the Castle Church in Wittenberg, challenging the religious authorities to a debate, affects us still. We are still trying to catch up with Luther in many areas; and in other areas, we rightly distance ourselves from his writings, especially his perspectives on Jews.  What was Luther so upset about that he nailed the placard of grievances to the door, which served as the public bulletin board of that little village? Let’s look back at the Catholicism that nurtured him to see what was going on.  Catholicism at the time of Luther had become a complicated system of trying to earn one’s salvation. One could travel to Rome and view the relics: a splinter from the cross; the chains of St. Paul; a lock from John’s head etc., ad nauseum. By viewing these so-called holy treasures or climbing Pilate’s steps on hands and knees, kissing the stairs for good measure, one might hope to release one’s parents from purgatory. This approach to working out one’s salvation suggested one could not really count on Christ’s forgiveness totally. This made Luther miserable, fearful that his salvation was not secure.  In the Augustinian cloister he had fasted, slept in the cold, and confessed the smallest of sins in a never-ending cycle of penance. Later in life he believed that these austerities had harmed his digestive system, about which he wrote entirely too much.  The thing that really set it off for Luther was the idea of indulgences. The pope had a cash flow problem, and he was in a capital campaign project to enhance buildings in Rome. So, the idea was born to issue indulgences; for a sum of money, one could buy forgiveness for oneself or parents. The brilliant marketing slogan of the Dominican Tetzel, who supervised the sale, was:  As soon as the coin in the coffer clings,  Another soul from purgatory springs.  One’s eternal destiny seemed to depend on factors other than the death of Christ. Later Luther was to observe that if the Pope had the power over purgatory, why did he not let them all out? It was more revenue positive to do it this way, evidently.  Luther’s breakthrough came when he realized through his study of Scripture that God’s justification is a gift; it is not something we must earn. By learning that God’s wrath had been submerged in mercy and that grace transcended works, he was no longer captive to a troubled conscience, but to the liberating Word of God.  There are parts of the Reformation legacy we have not fully realized. We are still reforming in the area of the priesthood of all believers. We know that women have made wonderful progress, yet in Cooperative Baptist Fellowship churches, only 6.5 percent of senior pastors are women. Next week the Tennessee Baptist Convention will be deciding whether or not a church with a woman as its senior minister will be allowed to remain in the fold. It is hard to believe that this time-worn conversation continues.  We are still reforming in our thought about vocation. Luther believed that any profession could be an expression of Christian service. This teaching can revitalize our understanding of the dignity of work and how Christians bear witness in the public square. My husband, of blessed memory, pursued a Christian vocation as a medical doctor; my calling as a minister was not superior to his, although I could argue that a seminary was better than a med school — primarily because you get to sing rather than cut things up!  Finally, Baptists are still reforming in understanding a theology of the cross. Luther was very critical of those who expect all “to gleam in glory” rather than seeking God where God chooses to hide — in the cradle and the cross. Rather than speculating about the actions of God, we should return again to the place of self-revealing in suffering, desolation and humility. That is where we should look for God.  Often we see human suffering as a judgment or a sign of abandonment by God. Yet, that is where God is to be found, truly. The cross demonstrates God’s vulnerability to all that would assail humanity, and that by being “pushed out of the world onto the cross” (Bonhoeffer) God can redeem the groaning exigencies of life.  So, as we celebrate this great inflection point in Christian history, we do so with awareness that the reforming impulse interrogates our practice as Baptists. We ever need to be reformed, along with the whole church.  Related news:  Protestant decline leaves fewer to celebrate Reformation Day  Unlike some Christians, Lutherans still all-in on Protestant identity  Related opinion:  The Reformation: Going back to the basics | Nora Lozano  ‘Out of love for the truth:’ The Reformation at 500 | Bill Leonard  ‘Exsurge Domine’: Pursuing Re-formation | Bill Leonard  Related curation:  Three surprising ways the Protestant Reformation shaped our world  After 500 years, Reformation-era divisions have lost much of their potency This past week the board of trustees met at our school, and we sorted through the challenges of theological schools as churches face decline. The state of the church and the state of the seminary are closely tied, and we need one another more than ever.  Many debate whether a theological education is necessary for effective ministry. We all know stories of the self-taught minister or of those who apprenticed with revered and experienced leaders and never went to seminary. Some have served effectively, but more have not. The challenge with this kind of limited preparation is that the learner does not hear the challenge of many voices; the learner is not confronted with the sweep of theological interpretation of “the faith once delivered.” Irenaeus spoke of the “heresy of truncation,” by which he meant narrowing the richness of the Christian tradition to one’s own bias.  We know that many seminaries are in trouble; there are few that are not fragile in some way. I recently attended a gathering of Cooperative Baptist Fellowship seminary presidents and deans, and I again heard the challenge of recruitment, the burgeoning debt of seminarians, issues of placement for women graduates, the white privilege of which many seminaries are oblivious. It is a “troubled industry.”  The mission of a seminary matters more than ever! Ministers are “stewards of the mysteries” of the faith, as Paul put it. It is their responsibility to serve as reliable guides in the things of the Spirit. Their study and contextualized learning forms a pastoral imagination that prompts them to move toward people with appropriate demonstrations of care, with appropriate boundaries.  Craig Dykstra of the Lilly Endowment named “pastoral imagination” as short-hand for the adaptive, wise leadership capacity which excellent pastors exhibit, in the words of Eileen Campbell-Reed and Chris Scharen. Pastoral imagination refers to an individual’s capacity for seeing a situation of ministry in all its holy and relational depths, and responding with wise and fitting judgment and action.  The church is changing. As we commemorated the 500th anniversary of the Protestant Reformation, we wonder if it was the first symptom of Christendom’s ultimate demise. Alan Bean describes Christendom as “the close identification of the Christian Church with the powers that be.” And it has been on a downward trajectory for 500 years. Constantinian Christianity is in its dotage, as he puts it.  We all know the trends in church attendance and the challenge of engaging younger adults. The close affinity of white evangelicals with current political agents may prove to be the death knell for segments of Christianity as we have known them in America. Many younger adults attend non-denominational churches or, more commonly, they view religion with a combination of incomprehension and derision, as Bean observes.  Public Religion Research Institute recently released “America’s Changing Religious Identity, a view of the American religious landscape:  White Christians now account for fewer than half of the public. White evangelical Protestants are in decline — along with white mainline Protestants and white Catholics. Non-Christian religious groups are growing, but they still represent less than one in 10. America’s youngest religious groups are all non-Christians. Muslims, Hindus and Buddhists are all far younger than white Christian groups. Atheists and agnostics account for a minority of all religious unaffiliated. Most are secular. Nearly half of LGBT Americans are religious unaffiliated. This is roughly twice the number of Americans overall (24 percent) who are religiously unaffiliated.  Clearly there is a generational shift in religious identity, yet there is some good news in all of this.  Visionary leaders are asking about whether present connotations for Christianity and evangelicalism have any future. We know that reinforcing the ways of Christendom, where Christianity had cultural privilege, will not do.  A seminary is still relevant if:  It prepares students to cut through the thicket of competing claims with wisdom from above.  It cultivates a love for the church that is in order to shape it for the future.  It welcomes and promotes a multi-cultural ethos.  It cultivates respect for the lived religion of others in a religiously plural world.  It takes note of the wounds students bring with them, especially where they have experienced injustice.  It equips learners to engage America’s changing religious identity.  Seminary graduates need equipping to form communities dedicated to transformative practices of prayer and worship, a place where belonging trumps believing — at least at the beginning. A recent graduate of our school served as a seminary intern in a progressive church, and she found a way to permeate the boundary between church and community as the congregation began to welcome those with no ecclesial home. Through their identification with the oppressed minorities of the world, a new vitality emerged, and the church created a position for her ministry. This is where authentic growth will occur.  A seminary will remain relevant as it forms people with the capacity to discern and join the movement of God in this world. It is a challenging and joyful mission. This past Sunday we finished the Christian year by celebrating the Reign of Christ. Next Sunday, we begin the season of Advent, eagerly embracing the rituals that prepare us to receive the infant of promise once again. Year A in the lectionary has flown by, and we begin the liturgical cycle anew, seeking guidance from Scripture to help us navigate our trouble-laden world.  In this liminal time, Pope Francis has chosen to visit Myanmar, an unprecedented journey. Christians are only about 4 percent to 5 percent of the approximately 55 million people who live there; of these there are less than half a million Romans Catholics. Clearly, his mission is more expansive than a pastoral visit to his flock in this remote country. Rather, he hopes to draw attention to the plight of hundreds of thousands of Rohingya who are being driven out of the Rakhine State into Bangladesh, where they live the liminal reality of refugees. He is meeting with military leaders and state counselor Daw Aung San Suu Kyi, and then will visit representatives of the displaced in Bangladesh.  One has to respect his intention to use the power of his position to alleviate human suffering. Of course, he will bring his great moral and global sensitivity, and he is savvy enough to understand where some of the triggers lie. We pray that his pilgrimage will go beyond the symbolic and help open a way to peaceful resolution. Will he use the name “Rohingya?” It is an explosive appellation, filled with unresolved claim to indigenous identity. Kofi Annan, who oversaw a United Nations report on the atrocities being wreaked and endured, declined to use the term out of deference to the Myanmar government.  The Western press has kept this humanitarian crisis in view, yet tends to oversimplify the complexity. External interpreters of the Myanmar situation expected radical change after the National League for Democracy won the 2015 election, yet the power of “the Lady” as state counselor, or prime minister, is constricted, more honorific than controlling. The military retains inordinate power with a voting bloc in Parliament, and exploits a nation at war with itself. Roger Cohen, writing for The New York Times on Nov. 25 describes the impact of the election: “This was not a handover of power. It was a highly controlled, and easily reversible, cession of partial authority.” She is hemmed in by military red lines, yet she has influence and embodies the aspirations of her land.  Formerly portraying Aung San Suu Kyi as something of a saint, commentators have criticized her roundly for her handling of the crisis. Many call it a spectacular fall from grace, and Oxford University went so far as to strip her of an honor. Other Nobel Peace laureates have criticized her stance, suggesting her present inaction is unworthy of a recipient. Yet, the constitutional structure limits her from even recognizing the Rohingya as one of the ethnic minorities of the land. Her responsibility is to her country, not to our assumptions of how she should act, and the Union of Myanmar is so fragile.  Cohen perceives her decisions this way: “She is playing a long game for real democratic change.” She stands between the military and the people, and “there is nothing in her history to suggest she’s anything but resolute.”  It is easier to be a protest figure than to govern, and she is still learning the intractable political game. While the international community has little sway in the internal dynamics of governing Myanmar, strategic sanctions for a developing nation hold power. The United States was too quick, it appears, in lifting them. Perhaps with sufficient financial leverage behind her, she could speak forthrightly and galvanize justice for these displaced persons.  Religious difference plays a critical role. Buddhism hold a favored place in Myanmar, as the Preservation of Race and Religion legislation makes clear, and Buddhist monks have stoked fearmongering about the Muslim minority becoming a radicalized expression of Islam. Some Christians have contributed to this perspective, also.  The reading for the First Sunday of Advent begs God to put the world to rights: “O that you would tear open the heavens and come down …” (Isaiah 64:1). The prophet knows that humans cannot heal this world without divine assistance, yet God will not do it without us. When the people of God cease to call on God’s name, they are in peril, yet God will “meet those who gladly do right” (64:5). Advent is all about sensing the threshold where God is bringing redemption.  I will be in Myanmar in the coming week, and I am eager to learn from those on the ground about the impact of the Pope’s visit. I am also eager to enter into the liminal longings for peace, shared by all.  Related story:  Can Pope Francis help Myanmar’s Muslims without hurting its Christians? | Christianity Today Helms Jarrell, co-director of the QC Family Tree intentional Christian community, had given crystal-clear instructions for the youth group’s annual trip to Boone, N.C. They had just hauled a van-full of Enderly Park teenagers up from Charlotte and the group… Helms Jarrell, co-director of the QC Family Tree intentional Christian community, had given crystal-clear instructions for the youth group’s annual trip to Boone, N.C. They had just hauled a van-full of Enderly Park teenagers up from Charlotte and the group… Helms Jarrell, co-director of the QC Family Tree intentional Christian community, had given crystal-clear instructions for the youth group’s annual trip to Boone, N.C. They had just hauled a van-full of Enderly Park teenagers up from Charlotte and the group… Christ’s disciples are called to serve the least of these. But that may not include the least of those in prison and jails, a new study finds. LifeWay Research surveyed 1,000 Protestant senior pastors and found that most of them… Helms Jarrell, co-director of the QC Family Tree intentional Christian community, had given crystal-clear instructions for the youth group’s annual trip to Boone, N.C. They had just hauled a van-full of Enderly Park teenagers up from Charlotte and the group… An English version is available here.  Como teóloga, me encanta el período de la Reforma. No conocía mucho sobre esta época, hasta que comencé mi educación teológica. Ese año en particular, mi seminario invitó a un erudito de la Reforma como profesor visitante. Desafortunadamente, pocos estudiantes se inscribieron para su clase. Por lo tanto, el decano persuadió a un grupo de estudiantes a tomar este curso. Yo fui parte de ese grupo. No estaba segura de qué esperar, pero los resultados, hasta el día de hoy, han sido abundantes.  El conocimiento sobre la Reforma me ha ayudado a entender muchas de las razones por las cuales la gente cristiana y protestante vive y opera de la manera en que lo hace hoy. Además, mis estudios sobre este periodo han seguido dando muchos frutos en mi carrera profesional y ministerial. Por ejemplo, hice uno de mis exámenes comprensivos doctorales sobre este tema, he predicado y dado muchas conferencias y clases al respecto, he publicado diferentes escritos basados en el tema, y recientemente hice una entrevista de televisión sobre la Reforma, junto con mi apreciada colega Raquel Contreras.  Dado este trasfondo, esperé el aniversario de los 500 años de la Reforma con mucha expectación. ¡El aniversario fue anoche! Hace cinco siglos, el 31 de octubre de 1517, Lutero publicó sus 95 tesis en la puerta de la iglesia del castillo de Wittenberg. Al hacer esto, Lutero intentaba traer a la luz algunos de los abusos de la Iglesia Católica Romana de ese tiempo.  Si bien la iglesia oficial afirmaba que por la gratitud de haber recibido su salvación, la gente cristiana debía contribuir a la iglesia comprando indulgencias, la creencia popular era que al comprar indulgencias, las personas podían obtener el perdón y la salvación. Después de batallar durante mucho tiempo con la idea de la salvación por obras, y la noción de un Dios como juez el cual era muy difícil de satisfacer, un buen día Lutero descubrió el versículo que afirma: Los justos vivirán por la fe (Romanos 1:17). En ese momento, Lutero encontró a un Dios de amor y misericordia que ofrece salvación por la gracia a través de la fe, y no por obras.  Al publicar sus 95 tesis, Lutero esperaba desafiar las creencias tradicionales de la época sobre la salvación y la gracia, y los abusos que éstas producían entre la gente. Además, Lutero deseaba que la gente viviera la nueva relación que él estaba experimentando con un Dios de amor y misericordia. Estaba seguro de que una vez que la iglesia oficial se diera cuenta de los abusos que la gente común estaba viviendo, ésta cambiaría y se reformaría de estos entendimientos y prácticas inapropiadas. La meta de Lutero no era comenzar una nueva iglesia, sino reformar a la ya existente.  ¿Qué pasó después? Los libros de historia de la iglesia narran que eventualmente Lutero fue expulsado de la Iglesia Católica Romana, por lo que él y sus seguidores fueron forzados a comenzar a comportarse como una iglesia nueva y alternativa. Estos y otros eventos de esos tiempos, eventualmente dieron nacimiento a la rama protestante del cristianismo.  Este año hubo muchas oportunidades para conmemorar el aniversario de los 500 años de la Reforma: servicios de adoración, conferencias, ponencias, libros, blogs y columnas. Pero ahora que este aniversario ha pasado, ¿qué sigue? ¿Ahora qué?  Si bien es cierto que hay muchos conceptos teológicos ricos que emanan de este período, en este momento me gustaría concentrarme en una meta que los reformadores promovieron para reformar a la iglesia. Al seguir las ideas del movimiento humanista, los reformadores querían reformar la teología y la práctica de la iglesia volviéndose a las fuentes originales. A lo que ellos se referían, era a regresar a los escritos de la Biblia y de los tiempos patrísticos.  Durante el tiempo de los reformadores, el hecho de tener acceso a las Escrituras (como un libro real que podían tener en sus manos y leer) era una verdadera novedad. Recordemos que Gutenberg acababa de inventar la imprenta y, como consecuencia de esto, la gente común pudo tener acceso a libros. Pero más que tener las Escrituras en sus manos, la meta real era reformar a la iglesia al guiar tanto al clero como a la gente laica a pensar y vivir de acuerdo con los principios básicos del Evangelio..  Vale la pena explorar más este concepto. ¿Cómo se podría vivir hoy en día?  Este fin de semana pasado tuve la oportunidad de volver a los principios básicos del Evangelio. Cuando se dieron varios desastres naturales (huracanes y terremotos) tanto en agosto como en septiembre pasados, me sentí tan impotente y deseosa de hacer algo más que escribir y dar conferencias/clases sobre estos eventos. Por lo tanto, envié un mensaje a la comunidad del Instituto de Liderazgo para Latinas para preguntar si alguien estaba interesada en ir a un viaje misionero práctico. La respuesta fue positiva. Así que este fin de semana pasado, estas hermanas del LLI y yo fuimos a servir a personas afectadas por el huracán Harvey en Houston.  Guiadas por Butch Green, misionero del Compañerismo Bautista Cooperativo (CBF), movimos y reorganizamos casi 200 cajas de ropa y suministros que se distribuirán entre las personas más necesitadas de la zona. Luego, fuimos a empacar las pertenencias de la Sra. C., o lo que quedaba de ellas. Debido al huracán Harvey, ella perdió su casa y la mayoría de sus pertenencias, pero lo más trágico fue que perdió a su hijo. El necesitaba diálisis y como no pudo recibir ningún tratamiento durante dos semanas, su salud empeoró y no pudo sobrevivir.  Cuando terminamos nuestra labor, una de las hermanas le preguntó a la Sra. C. si podíamos orar por ella, a lo cual ella estuvo de acuerdo. Inmediatamente después, la Sra. C. ofreció una hermosa oración por todas las miembros del equipo. Concluyó diciendo: “Sé que hoy Dios está muy agradado con las acciones de estas hermanas”.  Esta experiencia sacra me impulsó a seguir preguntándome: ¿qué más implica volver a los principios básicos del Evangelio, de tal manera que podamos agradar a Dios?  Creo, junto con los reformadores, que esto implica tomar en serio la necesidad de seguir reformándonos/transformándonos al igual que a nuestras iglesias, con el fin de continuar creciendo en nuestra fidelidad y obediencia al llamado de Dios. ¿Cuál es este llamado? Es uno que implica el abrazar la noción de un Dios compasivo que desea una vida abundante para todas sus criaturas. Es involucrarnos con todo nuestro ser en promover el proyecto de Dios: acercar su Reino a la tierra. Es un atrevido intento de vivir de acuerdo con los valores y acciones de Jesús, quien invirtió su energía en el desarrollo integral de las personas más vulnerables, y desafió las estructuras opresivas de su tiempo.  En resumen, es hacer lo que le da la gloria a Dios, o en las palabras de mi querida hermana Alicia Zorzoli: “Es hacer lo que hace feliz a Dios”. (Washington, DC) – Judicial Watch today announced that Justice Department refuses to release the proposed budget of Robert Mueller’s Special Counsel Office. Judicial Watch is seeking the information through a Freedom of Information Act (FOIA) lawsuit.  Judicial Watch sought “the copy of the budget prepared or submitted” by Special Counsel Robert Mueller. But, on Friday, January 19, the Justice Department notified Judicial Watch that it refuses to turn over documents, stating: “seven pages were located that contain records responsive to your … request. We have determined that this material should be withheld in full because it is protected from disclosure under the FOIA.” The Justice Department asserts the Mueller budget information cannot be released because its release could interfere with “law enforcement proceedings” and the material is protected from disclosure by the “deliberative process privilege.”  Judicial Watch filed a FOIA lawsuit against the DOJ on October 5, 2017, after it failed to respond to a July 10, 2017, request (Judicial Watch v. U.S. Department of Justice (No. 1:17-cv-02079)). Judicial Watch is seeking:  A copy of the budget prepared and submitted by Robert S. Mueller III or his staff….  A copy of all guidance memoranda and communications by which the Justice Management Division will review the Special Counsel’s Office’s “Statement of Expenditures”…  A copy of each document scoping, regulating, or governing the Special Counsel’s Office appointed under the leadership of Mueller III…  The Justice Department has thus far ignored Judicial Watch’s requests for documents about its management of the Mueller operation.  The Justice Department also sent Judicial Watch a copy of a previously published document showing expenditures by the Special Counsel’s Office from May 17, 2017, to September 30, 2017. The total was $3,213,695, nearly a million dollars per month.  On July 7, 2017, The Washington Post reported that Special counsel Mueller submitted a proposed budget to the Justice Department, “but officials declined to make the document public and committed only to releasing reports of the team’s expenditures every six months.”  Judicial Watch is pursuing numerous additional FOIA lawsuits related to the surveillance, unmasking, and illegal leaking targeting President Trump and his associates during the FBI’s investigation of potential Russian involvement in the 2016 presidential election.  “Special Counsel Mueller’s operation is not above the law. The American people have a right to know how much taxpayer money is planned for his massive investigation,” said Judicial Watch President Tom Fitton. “No one else in DC seems to be providing oversight of the Mueller operation, so once again it is up to the citizen’s group Judicial Watch to fight for accountability.”  ### Total Expenses are now $13,533,937.28  (Washington, DC) – Judicial Watch announced today that it obtained travel records from the U.S. Department of the Air Force in response to a Freedom of Information Act (FOIA) request for President Donald Trump and his family. The total for President Trump’s travels in this production is $3,199,188.30. Added to the previously released costs, the known travel costs for President Trump’s political and leisure travel is now $13,533,937.28.  President Trump and First Lady Melania Trump flew to his Bedminster golf club on June 30 and returned July 3. Flight time was 2.8 hours at $15,994 per hour for a total of $44,783.30.  President Trump and Melania Trump flew various trips between Bedminster and New York for a vacation on August 4 through August 21. Flight time was 5.9 hours at $15,994 per hour for a total of $94,364.60.  President Trump flew to Yuma, AZ, to meet with Marines and then attended a campaign rally in Phoenix on August 22. He flew 10.6 hours at $142,380 per hour for a total of $1,509,228.  President Trump flew to Springfield, MO, on Aug. 30 to appear at a rally as a kickoff for tax reform at the Loren Cook Company. He flew 3.8 hours at $142,380 for a total of $541,044.  President Trump flew to Huntsville, AL, on September 22 to campaign for Sen. Luther Strange. He then spent the weekend at Bedminster, returning to the White House on September 24. He flew 6.8 hours at $142,380 per hour for a total of $968,184.  President Trump flew to Bedminster on September 29 through October 1. Flight time was 2.6 hours at $15,994 per hour for a total of $41,584.40.  “The president is accountable to the taxpayers – they spend our hard-earned dollars and that’s why Judicial Watch keeps track of certain travel costs,” said Judicial Watch President Tom Fitton. “Trump’s regular trips to his homes are adding up to a hefty sum.”  The Secret Service has not yet provided to Judicial Watch documents requested through FOIA in conjunction with these trips.  Judicial Watch has also monitored travel for President Obama’s family and found a total of $114,691,322.17 in expenses to date, which includes annual Christmas family vacations in Hawaii; Michelle’s annual ski trips to Aspen; President Obama’s annual golf trip to Palm Springs, and various fundraising trips around the country, including California and New York.  ### Category: Number of Pages:5 Date Created:January 20, 2018 Date Uploaded to the Library:January 23, 2018    Donate now to keep these documents public! Brent Walker can see the U.S. Capitol and the U.S. Supreme Court building when he arrives at work. And that’s appropriate since he often finds himself heading over to those two buildings, as well as the White House and other… The leader of the Baptist World Alliance has lauded the Evangelical Church in Germany for its courageous stance in sustaining and nurturing the Christian faith in the European nation that gave birth to the Reformation — and for denouncing the anti-Semitism of… A prominent Southern Baptist layman is seeking a gag order in a lawsuit alleging sexual abuse, saying pretrial publicity jeopardizes his ability to receive a fair trial.  Paul Pressler, architect of a strategy credited with a rightward shift in leadership of the Southern Baptist Convention in the 1980s and 1990s, asked the court to order a Texas man accusing him of sexual assault spanning decades and his lawyer not to discuss details of the lawsuit on social media, the Internet or with news media.  A motion filed Jan. 19 accuses Gareld Duane Rollins Jr. and Houston attorney Daniel Shea of “attempting to try this case in the media.” It cites a report published in the Houston Chronicle and elsewhere quoting from documents unavailable to the public.  Pressler’s attorney, Ted Tredennick, said “high media interest and the sensitive nature” of the case justify a gag order.  “Texas courts have addressed the issue of judicial openness and found that a judicial order prohibiting attorneys from communication with the media does not violate the First Amendment,” the court document says. “Confidentiality of any pretrial depositions and other discovery is imperative to maintaining Defendants’ right to a fair and impartial jury.”  Tredennick said confidentiality “is of the highest necessity” when dealing with accusations of sexual misconduct. “Comments or sensitive materials released to the public cannot be retracted,” he said, and a confidentiality demand “is the only means” to protect his clients’ rights.  Pressler’s attorney also asked Harris County Judge R.K. Sandhill to postpone deposing Pressler and his wife until after a Feb. 23 hearing on whether the case is barred by statute of limitations. The letter says Pressler, 87, is in “poor health” and “the stress a deposition will pose” for both Pressler and his 84-year-old wife, Nancy, jeopardize their well-being.  In another development, Rollins amended his original complaint Jan. 12 to add the Southern Baptist Convention to other parties accused of aiding and abetting his alleged abuse coinciding with the denomination’s “conservative resurgence.”  Citing an SBC resolution in 2003 reminding “all” Southern Baptists of their responsibility to report any accusation of child abuse to legal authorities, Shea argues the entire convention had a duty “to exercise reasonable” care to control actions of the resurgence leadership.  The revised complaint claims the various parties either knew or should have known that Pressler posed a danger to children but continued to hold him up as a model of moral virtue.  “Rather than reporting, they collectively concealed,” the document contends. “Rather than implementing discipline, they collectively encouraged self-adulation. Rather than cooperating in the workings of justice, they collectively obstructed it.”  Pressler has previously denied accusations in the lawsuit, and the other defendants say they are not liable for any alleged harm.  Previous stories:  SBC leaders mostly mum on Pressler lawsuit, but bloggers fill the void  Pressler claims statute of limitations defense; his accuser adds slander claim  Lawyer who is suing prominent SBC leaders describes ‘Vatican light’ system for enabling abuse  Roy Moore, Paul Pressler and morals Sociologist Nancy Ammerman’s first book was about Bible-believing fundamentalists living in a modern world. That was in 1987.  Her next book, published in 1992, offered a comprehensive look at the turmoil consuming the Southern Baptist Convention.  Those works, as well as her research and writing interests since then, were inspired and informed by an indelible Baptist upbringing.  “My career as a sociologist studying religious communities started from being a preacher’s kid,” said Ammerman, who is a professor of sociology of religion at Boston University.  And not just any preacher’s kid, but a Southern Baptist preacher’s kid. Eventually, that would imbue her with credibility and insights other scholars may not enjoy in researching churches.  “My family’s theology was conservative enough that I could pass in a fundamentalist church,” Ammerman said. “I could get along. I knew what they were talking about.”  The experience also imparted a natural interest in the controversies then embroiling the SBC.  “I would say things like, ‘what’s going on in the SBC is so interesting. Somebody should study this,” said Ammerman, then in her first professorial position at Emory University in Atlanta.  Eventually she made that statement to Southern Baptist journalist Walker Knight.  “Walker said, ‘Well, why not you?’”  She dove in, spending three years in intense study that included interviewing more than 100 people on all sides of the issue, including agency heads and key decision makers. Ammerman studied publications and tapes of floor debates. With the help of the Baptist Sunday School Board (now LifeWay Christian Resources), she conducted extensive surveys of more than 1,000 Southern Baptists, including pastors, Women’s Missionary Union presidents, chairs of deacon boards and others.  The resulting book, Baptist Battles: Social Change and Religious Conflict in the Southern Baptist Convention, traced the controversy’s roots not to traditional, rural Baptist churches but to Southern Baptists in urban contexts who were disturbed by a changing world, Ammerman said.  It was an experience that hit close to home.  “The more I wrote and the more I read, I thought ‘I’m living this,’” she said.  Being raised in church helped Ammerman with other aspects of sociology.  Socially, preachers’ kids are both insiders and outsiders in their congregations. Frequent moves forces them continually to establish and re-establish their roles and places in churches.  Likewise, a sociologist’s role “is to observe and learn the culture of a place” while remaining “a little bit on the outside,” she said.  Congregations impact personal faith  Ammerman said her Baptist heritage also imparted an appreciation for the important roles congregations play in personal faith. The ways religion is lived out, and how that is nurtured by religious groups, is one of Ammerman’s academic interests.  “I grew up thinking congregations were pretty important. I think my focus on congregational life is really rooted in local church autonomy.”  Access to those communities has generated a body of work that illuminates the practical roles faith plays in individual lives.  Her first project, Bible Believers: Fundamentalists in the Modern World, focused on the religious experiences of the members of an independent Baptist church in New England.  Ammerman published Pillars of Faith: American Congregations and their Partners in 2005 to describe how diverse congregations function.  She also has published articles on the spiritual-but-not-religious phenomenon and on the local, social and religious dimension of congregations, among others.  Tying much of it together is an interest in “lived religion,” or how faith is experienced and interpreted by ordinary people.  And that often overlaps with the spiritual-but-not-religious language that permeates American culture.  That phrase “gets used as a way to say, ‘I am somebody who recognizes there is a spiritual dimension to life, that there is something beyond myself — but please don’t associate me with those bad religious people,’” Ammerman said. “They are making the distinction between what they think spirituality means and what they think religion means.”  That’s one of the ways people describe their faith. Teasing out such bottom-up definitions of religious experience is important in understanding faith in American culture.  Asking people if they believe in God is often met with uncertain or negative responses.  “But if we ask, ‘Do you ever light candles, do you celebrate Christmas, do you pray on certain occasions, do you still have your rosary beads?’” she said. “Then we find out there is a layer of religious practice and a sense of religiosity that is under the surface.”  But research has demonstrated that while there is a distrust of organized religion, “there are very few people who are engaged in deep spiritual life who are not participants in religious communities,” Ammerman added.  Related story:  Walker Knight: more challenges ahead for Baptist journalism QC Family Tree, founded by Greg and Helms Jarrell, is an intentional Christian community forming relationships and seeking justice alongside residents of the Enderly Park neighborhood of Charlotte, N.C. This series in the “Faith & Justice” project is part of the BNG Storytelling Projects initiative. In “Faith & Justice,” we tell the stories of the people and organizations that are helping to bend the “arc of moral justice” toward justice and transforming communities.  This series is written by Blake Tommey. Photos are by Lesley-Ann Tommey. Videos are recorded and edited by Blake and Lesley-Ann Tommey.  In this series, we learn how the Jarrells are organizing to combat gentrification, which increasingly threatens longtime Enderly Park residents with rising property values and the reality of displacement.  Additionally, we will explore how seeking justice starts with young people. That’s why the Jarrells and others continue to rally around children and teenagers in Enderly Park.  Ultimately, for QC Family Tree, seeking justice in Enderly Park means standing with their neighbors, even and especially when social and economic justice feels elusive and long deferred. In the meantime, QCFT will continue to be a place where the reality of relationship is its own form of justice.  Watch videos from local residents and leaders as well as the photo gallery to see for yourself.  _____________  Seed money to launch our Storytelling Projects initiative and our initial series of projects has been provided through generous grants from the Christ Is Our Salvation Foundation and the Eula Mae and John Baugh Foundation. For information about underwriting opportunities for Storytelling Projects, contact David Wilkinson, BNG’s executive director and publisher, at [email protected] or 336.865.2688.  About our Storytelling Projects initiative It was not by accident that we chose “Faith & Justice” as the first topic to explore in BNG’s Storytelling Projects initiative. Issues related to justice lie at the forefront of contemporary faith and constitute ministry priorities for many faith communities.  In a 1967 address to the Southern Christian Leadership Conference, Martin Luther King Jr., exhorted people of faith to “realize the arc of the moral universe is long, but it bends toward justice.”  In Faith & Justice, we tell the compelling stories of the people and organizations that are helping to bend the “arc of the moral universe” toward justice and, in so doing, are transforming the communities where they live.   The concept of being an “illegal” immigrant pretty much dates back to 1924 — less than a century ago. For most of American history, coming here “legally” meant next to nothing. And almost none of those who came here “legally” a century ago would make it across the border under today’s far more stringent standards. Letters to the Editor  We welcome letters from our readers as part of our mission-driven goal to facilitate “Conversations That Matter” among people of faith. For instructions on how to submit a letter to the editor and a few guidelines for what we’re looking for, click here.  Gary Dalton, December 1, 2017  Gavril Andreicut, December 1, 2017  Christa Brown, November 27, 2017  Susan Glass, November 22, 2017  Philip Brown, November 22, 2017  Kirby D. Smith, November 22, 2017  David Hicks, November 21, 2017  Stan Hastey, November 20, 2017  Christa Brown, November 16, 2017  Karen Dahl, Sandy Utah, November 16, 2017  Haley Cawthon, St. Petersburg, Fla., July 7, 2017  Luke Smith, Staunton, Va., July 7, 2017  Ken Sehested, Nov. 15, 2016  Erik Kluzek, March 26, 2016  Frank Kendall, March 21, 2016  Robert Carson, Feb. 29, 2016 When done from an unhealthy place, these new alliances have the ring of old retailers like Sears or Kmart who can no longer compete in the marketplace of ideas. But when done out of conviction about the needs for the unity of the church in responding to an increasingly indifferent post-Christendom society, they strike me as creative, inventive and hopeful. All photos taken in this photo gallery of QC Family Tree are by Lesley-Ann Hix Tommey.  The West Side Community Land Trust, QC Family Tree\\'s latest expression of justice-seeking, is educating local residents and homeowners to resist lucrative purchase offers on their homes and keep an affordable footing in Enderly Park. Nevertheless, profit-hungry developers continue to reproduce new, upscale craftsman homes alongside weathered homes that have stood for decades in West Charlotte.  In this series on QC Family Tree, we learn how the Jarrells are organizing to combat gentrification, which increasingly threatens long-time Enderly Park residents with rising property value and the reality of displacement.  Additionally, we will explore how seeking justice starts with young people. That’s why the Jarrells and others continue to rally around children and teenagers in Enderly Park.  Ultimately, for QC Family Tree, seeking justice in Enderly Park means standing with their neighbors, even and especially when social and economic justice feels elusive and long deferred. In the meantime, QCFT will continue to be a place where the reality of relationship is its own form of justice. Read more about QC Family Tree, watch videos and view the photo gallery.  Read more in the QC Family Tree Series  What is QC Family Tree?  For this intentional Christian community, seeking the world’s healing means battling gentrification close at home  With little opportunity for youth and children — or almost anyone else — Charlotte neighborhood finds hope in QC Family Tree  Video: What does justice look like in Enderly Park?  Video: How is QC Family Tree seeking justice in Enderly Park?  Video: What do you love about Enderly Park?  Video: How is QC Family Tree on the path toward justice?  Video: Why are you fighting for stable housing in West Charlotte?  Related commentary at baptistnews.com:  Requiem for the ‘cut’: Finding connections in a gentrifying neighborhood | Greg Jarrell  Where to go from here: Re-imagining Charlotte | Greg Jarrell  Related news at baptistnews.com:  Marginalized are harder to see ― and help ― in tourist towns, ministers say  Related curated at baptistnews.com:  Church makes scripture-centered fight against neighborhood displacement the core of its mission  Church planting and the gospel of gentrification  QC Family Tree, founded by Greg and Helms Jarrell, is an intentional Christian community forming relationships and seeking justice alongside residents of the Enderly Park neighborhood of Charlotte, N.C. This series in the “Faith & Justice” project is part of the BNG Storytelling Projects initiative. In “Faith & Justice,” we tell the stories of the people and organizations that are helping to bend the “arc of moral justice” towards justice and who are transforming communities.  _____________  Seed money to launch our Storytelling Projects initiative and our initial series of projects has been provided through generous grants from the Christ Is Our Salvation Foundation and the Eula Mae and John Baugh Foundation. For information about underwriting opportunities for Storytelling Projects, contact David Wilkinson, BNG’s executive director and publisher, at [email protected] or 336.865.2688. Cornelia Hagens, volunteer with the QC Family Tree youth group, speaks about what justicelooks like for children and teenagers in Enderly Park, Charlotte, NC.  In this series on QC Family Tree, we learn how the Jarrells are organizing to combat gentrification, which increasingly threatens long-time Enderly Park residents with rising property value and the reality of displacement.  Additionally, we will explore how seeking justice starts with young people. That’s why the Jarrells and others continue to rally around children and teenagers in Enderly Park.  Ultimately, for QC Family Tree, seeking justice in Enderly Park means standing with their neighbors, even and especially when social and economic justice feels elusive and long deferred. In the meantime, QCFT will continue to be a place where the reality of relationship is its own form of justice. Read more about QC Family Tree, watch videos and view the photo gallery.  Read more in the QC Family Tree Series  What is QC Family Tree?  For this intentional Christian community, seeking the world’s healing means battling gentrification close at home  With little opportunity for youth and children — or almost anyone else — Charlotte neighborhood finds hope in QC Family Tree  Video: What does justice look like in Enderly Park?  Video: How is QC Family Tree seeking justice in Enderly Park?  Video: What do you love about Enderly Park?  Video: Why are you fighting for stable housing in West Charlotte?  Photo Gallery: QC Family Tree in photos  Related commentary at baptistnews.com:  Requiem for the ‘cut’: Finding connections in a gentrifying neighborhood | Greg Jarrell  Where to go from here: Re-imagining Charlotte | Greg Jarrell  Related news at baptistnews.com:  Marginalized are harder to see ― and help ― in tourist towns, ministers say  Related curated at baptistnews.com:  Church makes scripture-centered fight against neighborhood displacement the core of its mission  Church planting and the gospel of gentrification  QC Family Tree, founded by Greg and Helms Jarrell, is an intentional Christian community forming relationships and seeking justice alongside residents of the Enderly Park neighborhood of Charlotte, N.C. This series in the “Faith & Justice” project is part of the BNG Storytelling Projects initiative. In “Faith & Justice,” we tell the stories of the people and organizations that are helping to bend the “arc of moral justice” towards justice and who are transforming communities.  _____________  Seed money to launch our Storytelling Projects initiative and our initial series of projects has been provided through generous grants from the Christ Is Our Salvation Foundation and the Eula Mae and John Baugh Foundation. For information about underwriting opportunities for Storytelling Projects, contact David Wilkinson, BNG’s executive director and publisher, at [email protected] or 336.865.2688. Enderly Park is blistering under an unseasonable September heat, and Frank Byers saunters across Tuckaseegee Road to the rec center where he likes to play cards with his neighbors.  He doesn’t use the crosswalk, but in many ways he’s earned it, that and another cigarette. Then he’s ready to talk football.  The Carolina Panthers’ veteran tight end Greg Olsen broke his foot during Sunday’s game, but Cam and the boys beat the Bills, and that’s fine by Frank.  “I’m my own team lover and I’m not riding the bandwagon no more; I’m a hometown diehard,” Byers says.  Despite some years rooting for the Steelers — one of the many bad habits Byers has sworn off — his devotion to the Queen City runs deep. Decades before he settled on the West Side of Charlotte, Byers grew up in Dilworth, the city’s first streetcar suburb, just two miles south of the city center. Yet, even as Dilworth dodged the 1970s urban renewal project, which disrupted many historically black neighborhoods, young Baby Boomers later crept in seeking in-town location and older houses. Rents skyrocketed and Byers had to leave. But hip, young families soon sniffed him out at his new home in Wilmore. Rents went up, again, and Byers had to leave — again.  From his new apartment in Enderly Park, owned and leased affordably by the QC Family Tree intentional Christian community, Byers is once more on a collision course with gentrification. But this time, he’s not alone. For the past decade, Frank has been engaged in the life and work of QC Family Tree and its directors, Greg and Helms Jarrell, together being authentic neighbors in Enderly Park and seeking justice with a community marked by stunning strength yet crippling disinvestment. And as the smell of hops wafts from the Lucky Dog Bark & Brew down the street, Byers and QC Family Tree are enlisting their neighborhood and other willing allies in a fight against the forces that put profit above people and exercise economic supremacy over Charlotte’s vulnerable.  Yet, as Byers works alongside QC Family Tree to defend his community from displacement, even serving on the board of the emerging West Side Community Land Trust, Byers says he isn’t angry. He doesn’t speak of justice, but only of his family and an enduring love for his city.  “I’m living in a city that’s hungry for some kind of championship,” Byers says.  While the neon-blue panther on Byers’ ball cap sufficiently attests to his meaning, you can’t help but imagine that he isn’t just talking about Charlotte’s Super Bowl hopefuls. Byers says he simply wants to stay in the neighborhood he loves and keep his son, Frank Jr., in the same school until he graduates. The hometown diehard, on the run from gentrification his whole life, is standing with his Enderly Park family, and they are hungry for a win.  “Racism is a system that removes people from land and land from people,” says Greg Jarrell, co-director of the QC Family Tree intentional community, which has been cultivating solidarity with Enderly Park residents like Byers for more than 12 years.  “The story of race is a story in which people who look like me — white people — control and profit from land, and remove the ability of people of color to be grounded in the earth. That’s what colonialism is all about, who’s in charge of the land. Gentrification is the new way of settler colonialism. It removes the people who have lived on that land, sometimes for generations, and it’s the natural progression of late capitalism and the roots of it go way back. What does racism look like in this community? Right now, the instability of people’s housing.”  When the Jarrells first moved to Enderly Park in 2005, however, seeking justice did not yet mean practicing community development in an effort to heal broken places, Greg says. In fact, he believes the prerequisite to seeking justice with their neighbors means constantly indicting their own privilege and tendency to exercise supremacy through compassion. Effectively, justice meant siding with their neighbors, even when siding did not mean helping, and the Jarrells went about designing ways to do so.  “Racism is a system that removes people from land and land from people.”  In addition to gathering the neighborhood’s youth for basketball tournaments, movie nights and meals at their house on Parkway Avenue, the Jarrells established monthly community meals to which anybody in the neighborhood was invited and for which ordinary residents became servers. As natural skepticism fell and relationships deepened, community meals began to play a central role in how neighbors listened to each other and identified ways of organizing around places of injustice. If a local family faced impending eviction, the community responded. If a teenager became a flight risk, they responded.  Gradually, Greg says, family after family fell victim to rising property value and increasing development in the neighborhood. “Just like in most cities across the country, housing has become very expensive and this neighborhood has seen a doubling of housing prices in the last 18 months.”  “Extreme disinvestment in this neighborhood over the past 40 years has created an environment where poor people lived here for a long time, often in bad conditions that most folks would not want to live in,” continued Greg. “But the people who have lived here made the absolute best of it that they could and did a tremendous job. Now, the people still on the deeds, whose names are on the papers downtown but who haven’t invested any money for decades, are positioned to make large profits off of that disinvestment. That harms our neighbors, the large majority of whom don’t own their houses. So when the rent goes up, they just have to leave.”  As QC Family Tree and neighbors like Byers began to organize against displacement, they quickly learned that North Carolina tenant-landlord law strongly favors landlords. Without ownership of land, Greg says, a given resident has very little recourse.  Byers discovered that precedent firsthand in 2016 when the landlord who rented his first apartment in Enderly Park suddenly sold the building for profit without notifying him. Overnight and without any improvements to the apartment, Byers’ rent rose from $600 a month to $900, leaving him no option but to turn to his neighbors at QC Family Tree, which owns five apartments that were gifted by a local landlord. Through a Section 8 voucher — a program, Greg explains, that has a highly stabilizing effect on housing but that has stalled by more than two years in the Charlotte area — Byers began renting from QC Family Tree at a price commensurate with his income.  For now, Byers helps his 12-year-old son get ready for school and prepare for his first season playing football in a rent-stable apartment, where Frank Jr. can attend the school he loves. Since getting clean from drug addiction and joining the work of QC Family Tree, life has never been so good, Byers says, and he wants to engage in the kind of advocacy that helped him find wholeness along the way.  “I’m seeing them help people, and that’s what I’ve been prone to do — help somebody else because somebody helped me over the years.”  “I’m seeing them help people, and that’s what I’ve been prone to do — help somebody else because somebody helped me over the years.”  “So we’re forming an organization to help people with affordable housing, people that live here and want to stay here in the 28208 zip code. We want to have them somewhere to live that’s affordable. This organization will have some property for you to live. We started last year in October and I’ve seen a lot of people come together.”  That organization, called the West Side Community Land Trust, is the latest expression of QC Family Tree’s journey toward justice. With Byers as chair of the board, WSCLT is comprised of the Jarrells, local West Side residents and other investors, Greg says, who want to see housing stabilize in West Charlotte and don’t feel the need to profit from investing in that mission. Through grassroots organizing and the building of a stakeholder pool, community land trusts effectively seek the legal recourse of land ownership, which, once attained, gives the group the ability to hold rents at sustainable and affordable levels for low-income residents.  With a median household income of $20,000 — more than $30,000 less than the median white household income — black residents of Enderly Park must have affordable housing to remain in their neighborhood and resist being displaced beyond the interstate where transportation and infrastructure provide even fewer opportunities, Helms Jarrell says. That’s why WSCLT is focused on educating local residents to resist lucrative offers on their property and lobbying for the funding necessary to purchase land in the 28208, 28214, 28216 and 28217 zip codes. As property value rises by the minute, Helms says, QC Family Tree often wonders if justice is possible, even in light of a God who seeks justice alongside them.  “I’ve been thinking about whether justice is possible and I don’t think it is right now.”  “When we think about the narrative of Israel’s departure from Egypt, they get out of Egypt but they’re still not in the Promised Land for another 40 years, wandering around in the wilderness. So it seems that there is a way toward justice. There is a practice that can be worked out to move us in the direction of justice, but I don’t know if we’re ever going to get there. The best I feel like we can do right now is to try and practice and work at it, and stretch into ways of abundant living, neighborliness, co-creation, resiliency, mirroring self-determination, being able to listen to the power of the voices of people who have been oppressed.”  After an entire day’s work is done and the babysitters have arrived, Helms Jarrell makes her way through Enderly Park to the WSCLT monthly meeting. En route, pristine craftsman bungalows dot the landscape, intermingled with the weathered homes of generations of resilient families. Once at the board table, she convenes the hopeful few in devising ways to be better neighbors and to defend their community’s right to remain in their homes. This quarter, they will focus on partnering with local investment initiatives and participating in a “Take Back the Block” demonstration, all the while building their membership $5, $20 and even $1,000 at a time through their fundraising website (squareup.com/store/westsideclt).  Yet, Helms explains, the end means nothing if the means reflect the same supremacy that now threatens their loved ones. For QC Family Tree, the work itself must be marked by a relinquishing of privilege and a sharing of mutual leadership and care. That’s why QC Family Tree and WSCLT safeguards their work by placing long-time Enderly Park residents like Frank Byers on governing boards and executive posts as well as continuing to host their neighbors for warm meals and careful listening. Ultimately, Helms says, housing justice is not an abstract issue — it lives and breathes, loves and fears, and desires a real peace.  “I don’t think of us doing the work of justice. I’m not a justice hero,” Helms says.  “It’s not abstract. Housing doesn’t just come down and hit us. It’s completely fluid, organic and relational. It means trying to listen and discern; what are the things going on and how can I respond with my gifts? We’re always wondering, ‘Did I do it right? Am I still doing it right? What is right? Should I even be worried about being right?’ I don’t know where justice lies, but I feel like I’m best practicing it when I have immersed myself in a community where I know God exists, because God resides with those who are marginalized and oppressed.”  Read more in the QC Family Tree Series  What is QC Family Tree?  With little opportunity for youth and children — or almost anyone else — Christian community builds chances from the ground up  Video: What does justice look like in Enderly Park?  Video: How is QC Family Tree seeking justice in Enderly Park?  Video: What do you love about Enderly Park?  Video: How is QC Family Tree on the path toward justice?  Video: Why are you fighting for stable housing in West Charlotte?  Photo Gallery: QC Family Tree in photos  Related commentary at baptistnews.com:  Requiem for the ‘cut’: Finding connections in a gentrifying neighborhood | Greg Jarrell  Where to go from here: Re-imagining Charlotte | Greg Jarrell  Greg Jarrell is a regular contributor to BNG opinion. Read more in his column.  Related news at baptistnews.com:  Marginalized are harder to see ― and help ― in tourist towns, ministers say  Related curated at baptistnews.com:  Church makes scripture-centered fight against neighborhood displacement the core of its mission  Church planting and the gospel of gentrification  QC Family Tree, founded by Greg and Helms Jarrell, is an intentional Christian community forming relationships and seeking justice alongside residents of the Enderly Park neighborhood of Charlotte, N.C. This series in the “Faith & Justice” project is part of the BNG Storytelling Projects initiative. In “Faith & Justice,” we tell the stories of the people and organizations that are helping to bend the “arc of moral justice” towards justice and who are transforming communities.  _____________  Seed money to launch our Storytelling Projects initiative and our initial series of projects has been provided through generous grants from the Christ Is Our Salvation Foundation and the Eula Mae and John Baugh Foundation. For information about underwriting opportunities for Storytelling Projects, contact David Wilkinson, BNG’s executive director and publisher, at [email protected] or 336.865.2688. With little opportunity for youth and children — or almost anyone else — Christian community builds chances from the ground up Helms Jarrell, co-director of the QC Family Tree intentional Christian community, had given crystal-clear instructions for the youth group’s annual trip to Boone, N.C.  They had just hauled a van-full of Enderly Park teenagers up from Charlotte and the group was allowed one hour of free time on King Street, after which they were to report promptly for the dinner reservation.  An hour passed, but no youth group. What on earth had they gotten into? The Jarrells had no doubt. The notoriously mischievous gang was clearly loafing around the giant barrels of candy at Mast General Store. Except, they weren’t. With no kids in sight, Greg and Helms left Mast General and methodically perused the next stretch of King Street, shop by shop.  Suddenly, they spotted the entire group gathered in a storefront across the street, utterly enthralled in what was happening before their eyes — a potter was turning clay in his shop, all the while answering questions and talking with the kids about his craft.  “That was an ‘aha!’ moment,” says Helms, who, with her husband Greg, has brought together Enderly Park youth for 12 years, seeking ways to inspire their imagination and empower their sense of identity in the world. Through weekly character-building gatherings, serving with their community and forming relationships rooted in mutual trust, the QC Family Tree youth group is seeking justice in their own lives and the life of their community, and challenging a long pattern of disinvestment in the lives of black youth.  A week after returning from Boone, Helms says, Samia Dillard, a 14-year-old member of the youth group, helped draft a grant proposal to create a pottery studio in the clubhouse behind the Jarrells’ home on Parkway Avenue. They got it. So they purchased a kiln, wheels and a mass of supplies for creating ceramic pottery and other art, which now cascades over the walls and shelves of the QC Family Tree pottery studio. Pottery, however, is only one way in which the youth of Enderly Park are imagining their worth and power in the world, Greg says.  “One of the most important parts of justice is using our imaginations, which doesn’t sound very tangible, but you can’t create a world that you can’t imagine.”  “There are multiple layers of imagination built into our pottery project — you have to imagine what a brick of clay can become, then you have to utilize your hands and develop the skill needed to construct what your imagination is telling you is possible. You have to be able to see, but then you have to practice a craft in order to cultivate your desire. In the process of doing that, you begin to imagine yourself differently. These kids have been told by popular culture, American culture and history books that their lives don’t matter. They’re not as important as my children are, just by the virtue of their racial heritage. That imagination has to change.”  Enderly Park is marked by stark disinvestment and lack of opportunity for children and teenagers, 86 percent of whom are black and therefore begin with $50,000 less in household income than the average white child in Charlotte, Greg says. The result, as well as the cause? Only half of all Enderly Park residents have attained a high school diploma, with young men being 12 percent more likely than young women to drop out of high school. That’s why, when the Jarrells first moved to Enderly Park in 2005, Helms says, they immediately began gravitating toward their young neighbors, who constantly gathered across the street at the rec center.  Early days convening a youth group involved fierce basketball games with the kids, during which the Jarrells earned the monicker “Old School” and gradually diffused the natural skepticism. Basketball games morphed into youth group dinners, movie nights and holiday outings, and as relationships deepened, larger issues in the students’ lives began to rise to the surface. Gradually, the structure of the QCFT youth group began to reflect a response to those issues as well as an expanding relationship with their families, Helms says.  Earned trips to pick out a Christmas tree or play at the beach became incentives for students to pursue good grades and care about their homework. Regular Wednesday evening “Devos” became a weekly occasion to instill in the students respect for themselves and others. Sleepovers at the Jarrells’ home even became a way to protect children at risk of lingering in the street late at night or fleeing the prospect of their family’s impending eviction. Christmas 2016 finally featured a massive pottery market to display and sell the ceramic art that students had been crafting during the fall, and the youth group took home hundreds of dollars in earned income.  Twenty-year-old Marquell Pettiford grew up in the QC Family Tree youth group and says his experience will not only stick in his memory forever but has formed him as a valuable and compassionate leader in his community.  “I acquired a lot of traits, a lot of abilities, and it expanded my horizon and made me a more valuable person to the people around me. It sent me on another direction in life,” Pettiford says.  “Who knows who I would have been if I’d never met them, because you have to remember, in the area we’re in there’s a lot of crazy stuff going on around you and it’s easy to get consumed by it. I don’t like to talk about myself, but I don’t really follow too much. I’m my own self and I respect everybody for being their self. And now that I’m getting older I’m learning to respect the truth more. I’m starting to recognize these things and I’m starting to see God every day of my life. Since QC Family Tree, since I picked up those traits, I want to be an impact on people’s lives like they have.”  Pettiford, now a local coffee roaster, rap artist and business student, says the rest of the world doesn’t understand neighborhoods like Enderly Park, which, contrary to popular belief in more privileged communities, is not full of crackheads, prostitutes and gang bangers, he adds. Instead, Pettiford sees a community that he loves deeply, that sticks together and that grows with each other like family. It’s that community, he says, that he and QC Family Tree have both chosen to invest in and be oriented toward in love and solidarity.  Justice for young people like Pettiford usually means entering the kind of slow, steady fight that most tend to ignore. If racism is like an iceberg, Helms explains, the visible portion is overtly racist acts against people of color, with which the larger American conversation is highly preoccupied at the moment. But when you seek true justice with your black neighbors, she adds, you begin to deal with the portion of the iceberg that lies invisible below the surface, namely the kind of white supremacy that permeates all sectors of life and negatively affects everyone in a spiritually, physically, emotionally traumatic way, especially young people.  It’s that kind of injustice — marked by lack of educational opportunity, unsustainable income, over-policing and mass incarceration — that has contributed to the degradation of the family, which is where children truly learn to respect themselves and others, says Cornelia Hagens, a dedicated grandmother and volunteer with the QCFT youth group. Hagens’ constant presence among the youth of Enderly Park is not simply about disciplinary oversight, she explains, but about empowering an entire generation of black youth with reverence for their own character and thoughtfulness toward their path in the world.  “Somebody needs to put it in their heads to have a little respect about yourself, a little motivation about yourself, and say ‘Listen, you’re smart.’”  “They need to hear it from somebody that’s going to show some kind of care for them. In this neighborhood, our kids are doomed. I don’t want to just say black kids, but it seems that way. It’s like a platform laid out for them to go nowhere. It is our responsibility to teach our future, because they are our future. We have nothing without these kids. We want our kids to be leaders. We want our kids to drive our country forward, drive our people forward, because we’re all people. That’s what matters to me, these kids.”  While you can’t empower a generation of young people overnight, Hagens says, you can root yourself in their lives and persist through the systems that keep them unmotivated, uninspired and disrespectful toward their future. It’s a process, she explains, of continually demonstrating the goodness and power that comes when you live with respect toward yourself and others.  From her home just next door to the Jarrells’ on Tuckaseegee Road, this devoted grandmother and 30-year resident of Enderly Park says she finds hope in two places — the work of QC Family Tree, which brings young people off the streets and into an environment that cultivates their self-worth, and finally, her grandson Kylan. Kylan, she says, represents a whole new generation of kind, intelligent people, who, as far as she can see, have the best chance yet of transforming the black community and our country’s unwritten future.  Read more in the QC Family Tree Series  What is QC Family Tree?  For this intentional Christian community, seeking the world’s healing means battling gentrification close at home  Video: What does justice look like in Enderly Park?  Video: How is QC Family Tree seeking justice in Enderly Park?  Video: What do you love about Enderly Park?  Video: How is QC Family Tree on the path toward justice?  Video: Why are you fighting for stable housing in West Charlotte?  Photo Gallery: QC Family Tree in photos  Related commentary at baptistnews.com:  Requiem for the ‘cut’: Finding connections in a gentrifying neighborhood | Greg Jarrell  Where to go from here: Re-imagining Charlotte | Greg Jarrell  Greg Jarrell is a regular contributor to BNG opinion. Read more from his column.  Related news at baptistnews.com:  Marginalized are harder to see ― and help ― in tourist towns, ministers say  Related curated at baptistnews.com:  Church makes scripture-centered fight against neighborhood displacement the core of its mission  Church planting and the gospel of gentrification  QC Family Tree, founded by Greg and Helms Jarrell, is an intentional Christian community forming relationships and seeking justice alongside residents of the Enderly Park neighborhood of Charlotte, N.C. This series in the “Faith & Justice” project is part of the BNG Storytelling Projects initiative. In “Faith & Justice,” we tell the stories of the people and organizations that are helping to bend the “arc of moral justice” towards justice and who are transforming communities.  _____________  Seed money to launch our Storytelling Projects initiative and our initial series of projects has been provided through generous grants from the Christ Is Our Salvation Foundation and the Eula Mae and John Baugh Foundation. For information about underwriting opportunities for Storytelling Projects, contact David Wilkinson, BNG’s executive director and publisher, at [email protected] or 336.865.2688. Graham, son of the famous evangelist Billy Graham and president of the international Christian relief organization Samaritan’s Purse, appeared on Fox & Friends on Sunday to tout what he believes Trump accomplished during his first year.  Related opinion: On immigration, Franklin Graham is dead wrong | Mark Wingfield  Related news: Baptists withdraw support for Franklin Graham rally in Puerto Rico  Canadian Christians divide over Franklin Graham  Fox pundit credits God, Franklin Graham, for electing Trump With little opportunity for youth and children — or almost anyone else — Christian community builds chances from the ground up On an intense weeklong trip through the Holy Land, six rabbis and five ministers, all women, try to figure out a way to make the discourse about the Israeli-Palestinian conflict less toxic. An English version is available here.  La semana pasada mi mamá murió inesperadamente. Tenía 86 años y había batallado con Alzheimer/demencia durante los últimos doce años. Esta enfermedad avanza lenta y gradualmente, y poco a poco la familia y amistades pierden a la persona amada.  Mi mamá parecía estar estable dentro de su enfermedad, así que su muerte fue sorpresiva. Cuando recibí las noticias sobre su estado crítico de salud, estaba con una querida amiga en un Starbucks. A medida que los informes seguían llegando, sorprendentemente, me mantuve bastante calmada y tranquila.  Sabiendo que un viaje inesperado era inminente, volví a casa, busqué boletos de avión, y me encargué de algunos asuntos urgentes relacionados con mi trabajo. Llegó la noticia del fallecimiento de mi mamá y oré … poniéndola en las manos de Dios y agradeciéndole por recibirla en su hogar eterno.  Mientras hablaba con mi hermano esa noche, platicamos sobre mi estado de ánimo calmado, y se volvió claro para mí que había estado viviendo un duelo por doce años. Cada vez que tenía contacto con mi mamá y descubría que ella ya no podía hacer esto o aquello …. Vivía un duelo silencioso. Además, sentía que no tenía muchos remordimientos en mi relación con ella. Por supuesto, tuvimos las batallas regulares que se dan entre madre e hija, pero en general puedo decir que tuvimos una buena relación, especialmente durante mi etapa de mujer adulta.  Gracias a Dios tuve la oportunidad de reflexionar en todo esto antes de viajar a México. Por lo tanto, el funeral y entierro de mi mamá se convirtieron para mí en una verdadera celebración de su vida. En circunstancias normales, los funerales y entierros en México se deben realizar dentro de las primeras 24 a 48 horas, después de la muerte de una persona. Los funerales generalmente duran dos días, en los cuales la capilla está abierta por muchas horas.  Debido a esto, tuve la oportunidad de pasar mucho tiempo con las amistades de mi mamá, así como con los hermanos y hermanas de su iglesia. Mientras escuchaba muchas historias acerca de ella, me percaté de ciertos temas predominantes relacionados con su generosidad, hospitalidad, gran sentido del humor, maravillosas habilidades culinarias, destrezas como escritora, y su amor por Dios, la iglesia y el ministerio. Un hilo que entretejió muchas de estas historias fue que ella fue una mujer muy adelantada para su tiempo. Por ejemplo, fue realmente progresista en su defensa de los derechos de las mujeres. Nunca se llamó a sí misma feminista, pero ciertamente vivía como tal. Estoy usando la palabra “feminista” para describir a una persona, mujer u hombre, que está a favor de los derechos de las mujeres.  Con seguridad, y para pesar de mi hija e hijo, no heredé las habilidades culinarias de mi madre, y me hubiera encantado tener más de su sentido del humor. Sin embargo, heredé sus destrezas como escritora, su amor por Dios, la iglesia y el ministerio, y su defensa de los derechos de las mujeres. Para cualquier persona que esté interesada en las cuestiones de la mujer, la historia de mi mamá resulta fascinante.  Mi mamá nació en 1931 y debido a diversas circunstancias no pudo asistir a la universidad. Como era extremadamente inteligente, su papá, mamá, y ella decidieron que asistiría a una escuela técnica para llegar a ser una secretaria bilingüe. Después de graduarse tuvo una carrera fructífera y ascendente que alcanzó su clímax, cuando se convirtió en la secretaria ejecutiva del director general de una importante fábrica internacional de vidrio. Una vez me contó que, en un momento dado, ganaba más dinero que su padre (su papá también tenía un buen trabajo). Otra vez me comentó (todavía puedo sentir su orgullo y pesar), que este importante hombre de negocios había mencionado que lo único que le faltaba a ella, profesionalmente hablando, era el ser hombre.  Esta brillante y ascendente carrera terminó el día en que se casó, a la edad de 26 años. En ese tiempo, las mujeres no podían tener ambas cosas, o trabajaban profesionalmente o se casaban. Por lo tanto, fue despedida automáticamente el día después de casarse. Estaba contenta con su matrimonio, pero quería más… Así que, de una manera visionaria y astuta, creó su propio espacio.  Dado a que nadie la contrataría debido a su estado civil, se volvió una traductora que trabajaba desde su casa. Uno de sus clientes era un hombre que estaba involucrado en el negocio del calzado. Al traducir los negocios y asuntos de este cliente, mi mamá aprendió el oficio y eventualmente puso su propia zapatería. De una manera creativa, mi mamá construyó un espacio donde pudo tener una vida plena, a pesar del sistema patriarcal opresivo que la rodeaba. Mi papá y mi mamá construyeron una casa donde la tienda y la casa estaban conectadas. Así que crecí con una mamá que cuidaba de su familia y cocinaba deliciosamente, y que, al cruzar literalmente una puerta, se transformaba a sí misma, frente a mis ojos, en una mujer de negocios sabia e inteligente. Durante años cruzó esta frontera con poder y gracia.  A través del tiempo, la gente me ha preguntado: ¿En qué momento te volviste feminista, defensora de los derechos de las mujeres? A lo cual siempre he respondido: “No recuerdo. No me convertí en una feminista, nací siendo una”.  A medida que reflexiono más y más en la historia de mi mamá, puedo decir “sí”, nací feminista porque fui llevada en el vientre, amamantada, y criada por una. Y la historia continúa …  Mi llamado a ser defensora de los derechos de las mujeres comenzó en el vientre y los brazos de esta mujer, que tuvo que abrir espacios por sí misma. Estaba oprimida por una cosmovisión y sistema patriarcales, pero aún así, se sentía fortalecida por Dios, y creía que podía tener lo mejor de ambos mundos: familia y trabajo profesional. Al abrirse espacios para sí misma, también le abrió los ojos, los sueños y la imaginación a muchas mujeres que la observaban, incluyéndome a mí.  Debido a su demencia, mi mamá nunca supo claramente acerca de mi ministerio con el Instituto Cristiano de Liderazgo para Latinas (Christian Latina Leadership Institute). Si lo hubiera sabido, se hubiera sentido muy orgullosa de este trabajo y de mí, y seguramente, habría sido una animosa colaboradora y generosa donante.  A medida que sigo celebrando su vida, continúo sintiéndome empoderada con su historia. Una que nos conecta a ella y a mí, y a otras mujeres en mi vida (mi hija, hermanas, sobrinas, abuelas, tías, tías-abuelas, amigas, mentoras, colegas, estudiantes y poderosas mujeres de la biblia) en una fuerte cadena de amor, apoyo, transformación y esperanza.  Al seguir mi trayectoria, parada y plantada en los hombros de esta mujer gigante y sus logros, heme aquí, llamada más que nunca a continuar mi trabajo de abrir espacios para las mujeres y empoderarlas. Mi esperanza es que, en el tiempo de Dios y con su bendición, llegue un día en que ninguna mujer en el mundo sufra la opresión y limitaciones que mi mamá sufrió, sólo por su género.  Mientras tanto, le agradezco a Dios por su poder que mueve a mujeres en todo el mundo a abrir sus propios espacios, a sentirse satisfechas y contentas, y a convertirse en modelos poderosos e inspiradores para la próxima generación de mujeres.  Así que, prosiguiendo hacia adelante, sigo con la confianza de que la vida de mi mamá y la mía son sólo un eslabón en una poderosa cadena que eventualmente, en el tiempo y el horizonte de Dios, producirá un mundo justo para todos los seres humanos, mujeres y hombres por igual. ¡Que así sea! The bustle of the past few weeks slows today. It is a time of reflection and quieting the spirit. Even the relentless urge to consume begins to re-set as the year comes to a close. We realize that we are more than what we possess or give. Like Mary, we ponder what is yet to come. The U.S. Supreme Court has agreed to accept a case to decide whether President Donald Trump’s latest travel ban unconstitutionally discriminates against Muslims.  On Jan. 19 the high court asked lawyers on both sides to address whether the president’s third executive order banning immigration from six Muslim-majority countries violates the First Amendment ban on establishing religion.  The state of Hawaii claims that adding North Korea and Venezuela to the list of banned countries does not alter the fact that the policy’s intent is to prevent Muslims from entering the country.  “Indeed, one might be forgiven for assuming that these countries were added primarily to improve the government’s ‘litigating position,’ rather than to achieve any legitimate substantive goal,” observes Hawaii’s opposition brief.  The White House claims the president has constitutional and statutory power to restrict the entry of aliens into the United States in the interest of national security. A Sept. 24 presidential order places restrictions on certain foreign nationals from eight countries — Chad, Iran, Libya, North Korea, Syria, Venezuela, Somalia and Yemen — judged by the administration to pose a “heightened risk.”  Two earlier versions of the ban were struck down by courts. The Ninth Circuit Court of Appeals said in December the current ban “exceeds the scope” of the president’s “delegated authority.”  Court observers expect the case to be argued in March or April, with a decision likely to follow in June.  The Council on American-Islamic Relations, the nation’s largest Muslim civil rights and advocacy organization, said the Supreme Court should reinstate lower-court injunctions in two states preventing the ban from taking effect.  “The illegal bigotry that animates Muslim Ban 3.0 should be as clear to the Supreme Court as it is to the Muslims it stigmatizes,” said CAIR National Litigation Director Lena Masri.  “If the First Amendment means anything at all, it means that the president of the United States cannot wield the federal government’s powers in a way calculated to disfavor Muslims and demonize Islam,” said CAIR Senior Litigation Attorney Gadeir Abbas. “The Supreme Court — along with the rest of us — must do everything possible to oppose Muslim Ban 3.0.” The head of the Baptist World Alliance has criticized a new law in Bolivia that various faith organizations say could curtail religious freedom.  BWA General Secretary Elijah Brown wrote Bolivia’s legislature Jan. 17 voicing concern that ambiguity in the country’s new penal code “could lead to unintended restrictions on religious freedom and to the direct persecution of churches and individuals of faith.”  New legislation in Bolivia states: “Whoever recruits, transports, deprives of freedom or hosts people with the aim of recruiting them to take part in armed conflicts or religious or worship organizations will be penalized 7 to 12 years of imprisonment.”  Various Christian news outlets described the law as a ban on proselytizing. The National Association of Evangelicals in Bolivia said it is “deeply worried” about the law, saying it could be used to ban evangelism or inviting someone to a Christian event.  Brown, who took over Jan. 1 as head of the 238 member-body Baptist World Alliance, asked lawmakers in Bolivia to either modify or repeal the law. The 250-church Bolivian Baptist Union and smaller Baptist Convention of Bolivia are among of BWA members.  Brown, previously a professor at East Texas Baptist University and executive vice president of the 21st Century Wilberforce Initiative, expressed hope “that freedom of religion and expression will be strengthened” in Bolivia and said Baptists “are praying “for the ongoing well-being of the country.” Enderly Park is blistering under an unseasonable September heat, and Frank Byers saunters across Tuckaseegee Road to the rec center where he likes to play cards with his neighbors. He doesn’t use the crosswalk, but in many ways he’s earned… Every day Baptist News Global staff works hard to produce content which fulfills our mission statement: “To interpret Christianity and culture through compelling journalism that informs and motivates people to make a difference in the world.” We’d love to know how well you think we did that this year. Your insights will help us live into our mission in the midst of very challenging times. Tony disappears behind the abandoned house at the curve, and he doesn’t reappear for a while. Another fellow follows him a couple minutes later, and also stays gone. In fact, he never reappears. Must be trouble, one assumes, given all the negative assumptions about this neck of the woods. Some illicit activity is going on — drug deals, probably, or perhaps prostitution. Maybe some guys just hanging out, playing dominoes and wasting their days away.  A few weeks later, Tony and I are walking together. As we round that same curve, he says, “Come on, let’s take the cut.” He is teaching me a bit of secret knowledge today. Behind the abandoned house we go, towards a small thicket. As we near it, I begin to see a well-worn path cutting through the privette, underneath the old oak trees. The leaves have been swept away by footsteps, so that the path is made clear by the emergence of a thin line of Carolina red clay.  A few steps later we are in the backyard of the matriarch of the neighborhood. We traverse her property line with great care, trying to match the care she has shown to multiple generations of children and parents here. She lives on the next street over, and now we have moved quickly and efficiently onto her street to visit with the family next door to her. “The cut” is simply a shortcut. It saves us a quarter mile of traversing our neighborhood’s uneven street network.  Not too long after that walk, a couple of youth introduce me to another cut. We’re out on a warm spring day, but the weather has not yet been warm long enough for the kudzu to spread. We head to the dead end of a particular street, where they show me yet another thin line of Carolina clay. To the right, a long hill descends to a stream. The opportunistic vine occupies every available space between us and the water. The power company keeps it this way to provide clearance for high-tension lines above. To the left, an older neighbor waves to us from his porch, his watchful eye noting who comes and goes. He is quickly out of sight, separated from us by a thicket that will become forest if left alone for much longer. As we round the bend between kudzu on one side and the tangle of bushes and vines and the other, our destination comes into view: Cook-Out, the local burgers and shakes chain, stands flanked by our mechanic and a small grocer.  The youth show me the way, but offer an important tip — no one uses this cut during the summer. It gets too overgrown with kudzu. The fear of ticks and snakes and creeping, crawling things makes the long way more attractive. But with the first frost, the kudzu gives way again to the rusty orange path. The critters go underground. The dead end street begins to make connections once more.  In Charlotte, like many other Southern cities, neighborhoods do not always have urban form. They are not densely built or easy to walk. The patterns of automobile dominance that characterized the time of rapid growth here led to neighborhoods that do not connect, either to other neighborhoods, or even within themselves. Covering what is a short distance “as the crow flies,” as we say in the South, to a neighbor’s house or to the corner store, can require a lengthy walk.  With time, as neighbors develop intimacy with the places that become home turf, they can imagine how to make these connections for themselves where planners and builders have failed them. And so they do, mingling private and public space, reclaiming dead places and making them human again.  Cities and towns like Charlotte built neighborhoods that made human connection difficult, particularly without the assistance of an automobile. Long blocks, lack of a simple street grid, separation of uses, ever-larger lots, lack of sidewalks, and emphasis on single family housing rather than a mix of housing and commercial types: all of these factors created spaces that put us at greater distance from one another. In the 1960s and ’70s, as middle-class people moved from these neighborhoods, out into even less-connected neighborhoods, the poor were left to occupy these spaces where infrastructure stifled connection.  Connection is the currency that keeps the economy of urban life flowing. So, the so-called “inner-cities” of Charlotte got double bad news with “white flight” — all of the negative policy and disinvestment decisions that made inner-city life tough around the country, and none of the diverse, walkable, human-powered infrastructure that facilitates the creative vibrancy and opportunity of urban spaces.  Facing these significant barriers, poor people did what they always do in the face of oppressive circumstances. They acted with imagination, creativity, and resiliency to create what they needed for human life. The cut is do-it-yourself urbanism. It is the resilient human spirit finding ways to build connection where planners and councils and developers failed to deliver. The cut is what happens when people who have always had to improvise look at what is given and use their imaginations to improve it.  Enderly Park, the Charlotte neighborhood where those cuts have helped to nurture my sense of neighborliness, is now being gentrified. Which is to say, speculators are now circling, and those imaginative, improvisational folks that have lived here for decades are being banished. They are deemed hazardous to profit margins, and must be displaced to other parts of town. Among the collateral damage of this kind of neighborhood change is the cut. Our cuts are being blocked by a new feature: fences.  Gentrifiers love their privacy fences, and they build them really well, and thoroughly. Soon we will all be cut off from one another. The lessons of improvised urbanism are being ignored, replaced by the assertion of private property rights and the false assumption that a few pickets around our yards can protect us from each other.  The poor people and children who build cuts are prophetic voices, challenging the middle-class affinity towards individualism, toward the belief that one’s house is one’s castle. Poor folks build the infrastructure for solidarity instead. They know that yards are terrific, but meaningless without neighbors with whom to share them. That hard-won knowledge is being erased now. The work of their hands and feet is being ignored because of what some piece of paper at the register of deeds says.  It is winter again. The rust colored path reveals itself once more, and shortens my walk to the grocery. A neighbor greets me as we pass one another, surrounded by dormant kudzu vines. I take comfort in this chance meeting. Fences are growing rampant now, covering the paths that lead toward more human connections. But on this quiet winter day, I can imagine that we are more creative than fences. That we still need to bump into each other. That we will still find new ways of getting to our neighbor’s door. In response to growing revelations of sexual abuse, many women have written #metoo on social media. But there is a perplexing silence among Latinas. There is too much evidence to suggest that this lack of voices means that sexual abuse is absent in the Hispanic community. Perhaps in a communal culture such as the Hispanic one, it makes sense that we move forward right now with a #wetoo, until we are ready to say individually #metoo. LGBTQ individuals and movements have become increasingly transparent about their identity and causes. Now, it may be time for the nation’s biggest Christian congregations to follow suit. New data shows that none of America’s 100 largest megachurches report having LGBTQ-affirming policies. The bustle of the past few weeks slows today. It is a time of reflection and quieting the spirit. Even the relentless urge to consume begins to re-set as the year comes to a close. We realize that we are more than what we possess or give. Like Mary, we ponder what is yet to come. Where are the churches willing to model vulnerability? I’ll tell you what they look like. They have given up on Christendom. They have given up on the notion that they hold some place of privilege in our culture. Divorce hurts children. My grandparents’ divorce still hurts their children and their grandchildren to this day — even though both have been dead for years. Certainly, the pain has lessened in time, but the wound is still there, for many… If people are really committed to biblical laws, then they should be committed to all of them. Instead of asking if it should be legal to run a heterosexuals-only bakery, we should ask who else a biblical legalist should turn away. Refusing to make devil’s food cakes for gay couples may not be enough. With little opportunity for youth and children — or almost anyone else — Christian community builds chances from the ground up Besides growing a church with new converts to Christianity, New City Church’s central mission is to slow, stop, and even turn back the negative effects of gentrification in urban Minneapolis communities. A few weeks ago, I saw the renderings of a new development headed to the edge of a gentrifying neighborhood in Charlotte, N.C. The space looked nice, like the kind of spot many Charlotteans would be excited to walk or bike to. It was the sort of space that might build connections, that will create small businesses and local economy. It was exactly, in that respect, what Charlotte needs more of.  But one thing about those renderings was not quite right. That particular neighborhood is not far down the trajectory of gentrification, though it clearly is headed that way. Much of the displacement that is likely to come has yet to happen. It remains more than 90 percent African American today. But the developer’s renderings, filled with mock people, included zero persons of color. Not a single one — Black, Hispanic, Asian, Native American, or otherwise.  Someone took the time to put those people in that drawing. They made choices. There was likely no harm meant, but they were still choices. Those pretend people did not draw themselves. As drawings envisioning a future in a space that does not yet exist, they were choices of imagination.  The imagination that occupied this developer’s office could not foresee a future where the people who live near this new building would actually visit it. This is nonsensical when your goal is to build neighborhood-scale businesses. Meeting actual neighborhood needs is in the interest of neighborhood businesses — it is their lifeblood. The thinking that animates this new development is coherent only if the goal is to remove the neighbors you have and to replace them with the neighbors you want. In this way, the ripping of social fabric, the unsettling of children, the splitting of families, and the destruction of communal public spaces that constitute the process of gentrification are taken to be desirable or, if not desirable, then definitely inevitable, not worth fighting, and, conveniently, highly profitable.  This is, in miniature, the crisis that we have reached in Charlotte, which has finally been made glaringly obvious on our streets following the killing of Keith Scott, though it was there to see long ago: that those who now have the most power to shape our future cannot imagine a future worth having.  James Baldwin, concerned about the dehumanization of his neighbors in Harlem in the 1950s and ’60s, asked, “What will happen to all that beauty?” The same question stands before our city now. One place to begin understanding the uprising in our streets is with the recognition that too many of us have missed innumerable opportunities to see places and faces of beauty. We have lost countless chances to learn to attach the description “beautiful” to people, culture and neighborhoods that our policy and investment decisions indicated were expendable. The problem might be stated that those who hold power and privilege in our city have been making the choice not to see either the beauty or the trauma that lives in our most vulnerable communities. But the problem runs more deeply than simply not seeing. It is built into the architecture and design, the physical and economic geography of Charlotte. We have built too many places where people do not know that not seeing is a choice. These are real places, where bricks walls and iron gates begin to silently form the terrain of souls. This hurts us all. Not all in the same way, but it hurts us all.  There is poverty that destroys bodies, that starves minds, that brutalizes flesh. This is poverty in the literal sense. We rightly work against this, through all sorts of initiatives, though with few satisfying results. But this is not the only poverty. There is also poverty of the soul. It manifests itself in the inability to see one’s neighbor as fully human. It hides from suffering, including its own. Poverty of the soul starves the imagination and neuters the spirit. But its immediate effects are mostly felt by other people. A poverty of imagination takes its feebleness for creativity and foists its will upon those without the power to resist.  The opportunity stands before our city, now as never before, to name the wounds inflicted by the coupling of power with poverty of imagination. These wounds now become the subjects of our work, as do the unnamed spirits that have allowed us to go on doing harm without thought for repair. Segregation in our housing. Re-segregation of our schools. The destruction of neighborhoods for the building of highways. Outcomes in the justice system determined by race. Gentrification. Food deserts. Over-policing of minority communities. Creating task forces rather than taking real and corrective action.  Naming those wounds and telling the whole truth of our history, if we can be courageous enough to do it, gives way to another, a hopeful, opportunity: re-imagining Charlotte. The spaces where we live and work and play today are not a given. They do not have to be. They were dreamed into being, with unquestionable success, but also with much harm along the way. Those spaces are maintained in inequitable fashion by systems that fail to prioritize the flourishing of all our denizens as neighbors to one another. Our city will not flourish unless the re-imagination of our common good is done by different, more fecund imaginations — the young, the excluded, the displaced, the poor, the unheard, the disenfranchised. To those folks, neither the conference table nor the boardroom has ever been open. They do not draw up the plans, or get the financing, or create the renderings for new projects. Our city — all of us — suffers because of it.  A few months back, I got to listen in on a conversation among neighbors in that soon-to-be-gentrified neighborhood. They were imagining how they would create a more flourishing place in their little corner of the world. Rich palettes of color enlivened their descriptions. The worlds they dreamed were far more colorful than just white — which, truth be told, is just thought to be the quickest way to green, the real goal — but were richly hued in every imaginable color. They described a world marked not only by its striking diversity, but by the strength of each color that dressed buildings, gardens and sidewalks. They could imagine that the strength of each benefitted everyone. And given the chance to dream dreams, they created a space where they, in their own neighborhood, could belong.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch testfile.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"testfile.txt\",\"w\") \n",
    " \n",
    "file.write(s) \n",
    " \n",
    "file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3\n",
    "\n",
    "input_data = sagemaker_session.upload_data(path=\"testfile.txt\", bucket=bucket, key_prefix=\"L1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now doing languge modeling / pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!python pretraining.py \n",
    "\n",
    "--output_dir \"./pretrain_outputs/BERTtext\"  \n",
    "--train_data_file \"testfile.txt\"  \n",
    "--seed 42  \n",
    "--model_type \"bert\"  \n",
    "--mlm  \n",
    "--model_name_or_path \"bert-base-uncased\"  \n",
    "--block_size=512  \n",
    "--learning_rate 1e-4  \n",
    "--num_train_epochs 5  \n",
    "--do_train  \n",
    "--per_gpu_train_batch_size 4   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/07/2020 21:58:31 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "model_class: <class 'transformers.modeling_bert.BertForMaskedLM'>\n",
      "03/07/2020 21:58:31 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/ec2-user/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "03/07/2020 21:58:31 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/07/2020 21:58:32 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ec2-user/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "03/07/2020 21:58:32 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/ec2-user/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "03/07/2020 21:58:42 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "03/07/2020 21:58:42 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "Model: BertForMaskedLM(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (cls): BertOnlyMLMHead(\n",
      "    (predictions): BertLMPredictionHead(\n",
      "      (transform): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "03/07/2020 21:58:42 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir=None, config_name=None, device=device(type='cpu'), do_eval=False, do_train=True, eval_all_checkpoints=False, eval_data_file=None, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=0.0001, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=True, mlm_probability=0.15, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=0, no_cuda=False, num_train_epochs=5.0, output_dir='./pretrain_outputs/BERTtext', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=2000, save_total_limit=2, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train_data_file='testfile.txt', warmup_steps=0, weight_decay=0.0)\n",
      "03/07/2020 21:58:42 - INFO - __main__ -   Loading features from cached file bert_cached_lm_510_testfile.txt\n",
      "Loaded and Cached examples\n",
      "03/07/2020 21:58:42 - INFO - __main__ -   ***** Running training *****\n",
      "03/07/2020 21:58:42 - INFO - __main__ -     Num examples = 110\n",
      "03/07/2020 21:58:42 - INFO - __main__ -     Num Epochs = 5\n",
      "03/07/2020 21:58:42 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
      "03/07/2020 21:58:42 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "03/07/2020 21:58:42 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "03/07/2020 21:58:42 - INFO - __main__ -     Total optimization steps = 140\n",
      "Epoch:   0%|                                              | 0/5 [00:00<?, ?it/s]Iterating through training data\n",
      "\n",
      "Iteration:   0%|                                         | 0/28 [00:00<?, ?it/s]\u001b[ABatch number: 0\n",
      "steps_trained_in_current_epoch: 0\n",
      "Checkpoint 2 NK\n",
      "args.device: cpu\n",
      "Training mode\n",
      "args.mlm True\n",
      "inputs tensor([[  101,  8036,   103,  ..., 12365,  2607,   102],\n",
      "        [  101, 27954,  1012,  ...,  7632,   103,   102],\n",
      "        [  101,  1035,  1035,  ...,  2155,  3392,   102],\n",
      "        [  101,  1996,   103,  ...,  8370, 18704,   102]])\n",
      "labels tensor([[-100, -100, 1997,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, 2595, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, 4125,  ..., -100, -100, -100]])\n"
     ]
    }
   ],
   "source": [
    "!python pretraining.py --output_dir \"./pretrain_outputs/BERTtext\"  --train_data_file \"testfile.txt\" --seed 42 --model_type \"bert\" --mlm True --model_name_or_path \"bert-base-uncased\" --block_size=512 --learning_rate 1e-4 --num_train_epochs 5 --do_train --evaluate_during_training --per_gpu_train_batch_size 4 --save_total_limit 2  --save_steps 2000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-41-63c492649e94>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-41-63c492649e94>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    --output_dir \"./pretrain_outputs/BERTtext\"\u001b[0m\n\u001b[0m                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "!python run_pretraining.py  --output_dir \"./pretrain_outputs/BERTtext\" \n",
    "    --model_type bert     \n",
    "    --mlm     \n",
    "    --tokenizer_name ./models/BertTokenizer    \n",
    "    --do_train     \n",
    "    --learning_rate 1e-4     \n",
    "    --num_train_epochs 5     \n",
    "    --save_total_limit 2     \n",
    "    --save_steps 2000     \n",
    "    --per_gpu_train_batch_size 4     \n",
    "    --evaluate_during_training     \n",
    "    --seed 42 \n",
    "    --train_data_file testfile.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data path for the SageMaker PyTorch container. We don't need to create an own container. \n",
    "# container_data_dir = '/opt/ml/input/data/training'\n",
    "# container_model_dir = '/opt/ml/model'\n",
    "\n",
    "parameters = {\n",
    "#     'train': container_data_dir,\n",
    "#     'model_dir' container_model_dir: ,\n",
    "    'model_type' : \"bert\",\n",
    "    \"model_name_or_path\" : \"bert-base-uncased\",\n",
    "    'mlm': True,\n",
    "    'do_train': True,\n",
    "    'seed' : 42,\n",
    "    'num_train_epochs' : 2,\n",
    "    'learning_rate' : 1e-4,\n",
    "    'per_gpu_train_batch_size': 4,\n",
    "    'evaluate_during_training' : False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazon SageMaker PyTorch framework\n",
    "\n",
    "train_instance_type = 'ml.p3.2xlarge'\n",
    "\n",
    "estimator = PyTorch(entry_point='pretraining.py',\n",
    "                    source_dir = './train_scripts/', # the local directory stores all relevant scripts for modeling\n",
    "                    hyperparameters=parameters,\n",
    "                    role=role,\n",
    "                    framework_version='1.3.1',\n",
    "                    train_instance_count=2,\n",
    "                    train_instance_type=train_instance_type\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-08 06:10:21 Starting - Starting the training job...\n",
      "2020-03-08 06:10:23 Starting - Launching requested ML instances......\n",
      "2020-03-08 06:11:22 Starting - Preparing the instances for training.........\n",
      "2020-03-08 06:13:17 Downloading - Downloading input data\n",
      "2020-03-08 06:13:17 Training - Downloading the training image.......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-03-08 06:14:22,608 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-03-08 06:14:22,634 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2020-03-08 06:14:35,501 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2020-03-08 06:14:35,526 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2020-03-08 06:14:36,974 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2020-03-08 06:14:37,258 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[35mGenerating setup.py\u001b[0m\n",
      "\u001b[35m2020-03-08 06:14:37,258 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[35m2020-03-08 06:14:37,258 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[35m2020-03-08 06:14:37,258 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[35mProcessing /tmp/tmpw1s8vy46/module_dir\u001b[0m\n",
      "\u001b[35mCollecting tensorboardX\n",
      "  Downloading tensorboardX-2.0-py2.py3-none-any.whl (195 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.21.2)\u001b[0m\n",
      "\u001b[35mCollecting transformers\n",
      "  Downloading transformers-2.5.1-py3-none-any.whl (499 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (0.25.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.6/site-packages (from tensorboardX->-r requirements.txt (line 1)) (3.11.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from tensorboardX->-r requirements.txt (line 1)) (1.16.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from tensorboardX->-r requirements.txt (line 1)) (1.12.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->-r requirements.txt (line 2)) (0.14.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: scipy>=0.17.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->-r requirements.txt (line 2)) (1.2.2)\u001b[0m\n",
      "\u001b[35mCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 3)) (1.11.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 3)) (4.36.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 3)) (2.22.0)\u001b[0m\n",
      "\u001b[35mCollecting tokenizers==0.5.2\n",
      "  Downloading tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7 MB)\u001b[0m\n",
      "\n",
      "2020-03-08 06:14:34 Training - Training image download completed. Training in progress.\u001b[35mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2020.2.20-cp36-cp36m-manylinux2010_x86_64.whl (690 kB)\u001b[0m\n",
      "\u001b[35mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.38.tar.gz (860 kB)\u001b[0m\n",
      "\u001b[35mCollecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 4)) (2019.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 4)) (2.8.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorboardX->-r requirements.txt (line 1)) (44.0.0.post20200106)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: botocore<1.15.0,>=1.14.7 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers->-r requirements.txt (line 3)) (1.14.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers->-r requirements.txt (line 3)) (0.3.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers->-r requirements.txt (line 3)) (0.9.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 3)) (2019.11.28)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 3)) (1.25.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 3)) (3.0.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 3)) (2.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers->-r requirements.txt (line 3)) (7.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.15.0,>=1.14.7->boto3->transformers->-r requirements.txt (line 3)) (0.15.2)\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: default-user-module-name, sacremoses\n",
      "  Building wheel for default-user-module-name (setup.py): started\u001b[0m\n",
      "\u001b[35m  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=78057 sha256=1c74067cad1fef1ce7e31ae568a0790db2335a5e6ddab0fc2e7710a761b0ada5\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-3hsl8eoh/wheels/ca/9c/de/ad94b60fea1ce8d488ca522f788a6e51772aed8f7f486a5152\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.38-py3-none-any.whl size=884629 sha256=f06df49dd8b9cd0ec94f189153a3f9c057b6ef139595a3406c032d9fc8d652fc\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/e9/be/8b52f6e7e8c333b56f9440575b4c5eb4d96d27b5d22df5a71e\u001b[0m\n",
      "\u001b[35mSuccessfully built default-user-module-name sacremoses\u001b[0m\n",
      "\u001b[35mInstalling collected packages: tensorboardX, sentencepiece, tokenizers, regex, sacremoses, filelock, transformers, default-user-module-name\u001b[0m\n",
      "\u001b[35mSuccessfully installed default-user-module-name-1.0.0 filelock-3.0.12 regex-2020.2.20 sacremoses-0.0.38 sentencepiece-0.1.85 tensorboardX-2.0 tokenizers-0.5.2 transformers-2.5.1\u001b[0m\n",
      "\u001b[35mWARNING: You are using pip version 20.0.1; however, version 20.0.2 is available.\u001b[0m\n",
      "\u001b[35mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[35m2020-03-08 06:14:43,082 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[35mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"seed\": 42,\n",
      "        \"model_type\": \"bert\",\n",
      "        \"do_train\": true,\n",
      "        \"per_gpu_train_batch_size\": 4,\n",
      "        \"mlm\": true,\n",
      "        \"evaluate_during_training\": false,\n",
      "        \"num_train_epochs\": 2,\n",
      "        \"learning_rate\": 0.0001,\n",
      "        \"model_name_or_path\": \"bert-base-uncased\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"pytorch-training-2020-03-08-06-10-19-858\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-496641494145/pytorch-training-2020-03-08-06-10-19-858/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"pretraining\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"pretraining.py\"\u001b[0m\n",
      "\u001b[35m}\n",
      "\u001b[0m\n",
      "\u001b[35mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"do_train\":true,\"evaluate_during_training\":false,\"learning_rate\":0.0001,\"mlm\":true,\"model_name_or_path\":\"bert-base-uncased\",\"model_type\":\"bert\",\"num_train_epochs\":2,\"per_gpu_train_batch_size\":4,\"seed\":42}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=pretraining.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=pretraining\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-west-2-496641494145/pytorch-training-2020-03-08-06-10-19-858/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"do_train\":true,\"evaluate_during_training\":false,\"learning_rate\":0.0001,\"mlm\":true,\"model_name_or_path\":\"bert-base-uncased\",\"model_type\":\"bert\",\"num_train_epochs\":2,\"per_gpu_train_batch_size\":4,\"seed\":42},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"pytorch-training-2020-03-08-06-10-19-858\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-496641494145/pytorch-training-2020-03-08-06-10-19-858/source/sourcedir.tar.gz\",\"module_name\":\"pretraining\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"pretraining.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--do_train\",\"True\",\"--evaluate_during_training\",\"False\",\"--learning_rate\",\"0.0001\",\"--mlm\",\"True\",\"--model_name_or_path\",\"bert-base-uncased\",\"--model_type\",\"bert\",\"--num_train_epochs\",\"2\",\"--per_gpu_train_batch_size\",\"4\",\"--seed\",\"42\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[35mSM_HP_SEED=42\u001b[0m\n",
      "\u001b[35mSM_HP_MODEL_TYPE=bert\u001b[0m\n",
      "\u001b[35mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[35mSM_HP_PER_GPU_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[35mSM_HP_MLM=true\u001b[0m\n",
      "\u001b[35mSM_HP_EVALUATE_DURING_TRAINING=false\u001b[0m\n",
      "\u001b[35mSM_HP_NUM_TRAIN_EPOCHS=2\u001b[0m\n",
      "\u001b[35mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[35mSM_HP_MODEL_NAME_OR_PATH=bert-base-uncased\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python pretraining.py --do_train True --evaluate_during_training False --learning_rate 0.0001 --mlm True --model_name_or_path bert-base-uncased --model_type bert --num_train_epochs 2 --per_gpu_train_batch_size 4 --seed 42\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35mmodel_class: <class 'transformers.modeling_bert.BertForMaskedLM'>\u001b[0m\n",
      "\u001b[35m03/08/2020 06:14:45 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m2020-03-08 06:14:48,061 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-03-08 06:14:48,337 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-03-08 06:14:48,337 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-03-08 06:14:48,338 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-03-08 06:14:48,338 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmp3xaap8_2/module_dir\u001b[0m\n",
      "\u001b[34mCollecting tensorboardX\n",
      "  Downloading tensorboardX-2.0-py2.py3-none-any.whl (195 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.21.2)\u001b[0m\n",
      "\u001b[34mCollecting transformers\n",
      "  Downloading transformers-2.5.1-py3-none-any.whl (499 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (0.25.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from tensorboardX->-r requirements.txt (line 1)) (1.16.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from tensorboardX->-r requirements.txt (line 1)) (1.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.6/site-packages (from tensorboardX->-r requirements.txt (line 1)) (3.11.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=0.17.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->-r requirements.txt (line 2)) (1.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->-r requirements.txt (line 2)) (0.14.1)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers==0.5.2\n",
      "  Downloading tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 3)) (1.11.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 3)) (4.36.1)\u001b[0m\n",
      "\u001b[34mCollecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2020.2.20-cp36-cp36m-manylinux2010_x86_64.whl (690 kB)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.38.tar.gz (860 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 3)) (2.22.0)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 4)) (2019.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 4)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorboardX->-r requirements.txt (line 1)) (44.0.0.post20200106)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers->-r requirements.txt (line 3)) (0.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers->-r requirements.txt (line 3)) (0.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.15.0,>=1.14.7 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers->-r requirements.txt (line 3)) (1.14.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers->-r requirements.txt (line 3)) (7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 3)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 3)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 3)) (1.25.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 3)) (2019.11.28)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.15.0,>=1.14.7->boto3->transformers->-r requirements.txt (line 3)) (0.15.2)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: default-user-module-name, sacremoses\n",
      "  Building wheel for default-user-module-name (setup.py): started\n",
      "  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=78057 sha256=44863181d926e7416124fae5adbdfe390ebfe6d5e26c7f4a34f30b01748d9184\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-hxa_6ot8/wheels/06/18/8d/22372211d0b496d42b3d3a479638944499268552597dfd8eae\n",
      "  Building wheel for sacremoses (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.38-py3-none-any.whl size=884629 sha256=58e10c30c273b11489a10d539db77ae1fd99375776a7f72c840701a7a3ac5f2b\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/e9/be/8b52f6e7e8c333b56f9440575b4c5eb4d96d27b5d22df5a71e\u001b[0m\n",
      "\u001b[34mSuccessfully built default-user-module-name sacremoses\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tensorboardX, tokenizers, filelock, regex, sacremoses, sentencepiece, transformers, default-user-module-name\u001b[0m\n",
      "\u001b[34mSuccessfully installed default-user-module-name-1.0.0 filelock-3.0.12 regex-2020.2.20 sacremoses-0.0.38 sentencepiece-0.1.85 tensorboardX-2.0 tokenizers-0.5.2 transformers-2.5.1\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 20.0.1; however, version 20.0.2 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-03-08 06:14:54,167 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"seed\": 42,\n",
      "        \"model_type\": \"bert\",\n",
      "        \"do_train\": true,\n",
      "        \"per_gpu_train_batch_size\": 4,\n",
      "        \"mlm\": true,\n",
      "        \"evaluate_during_training\": false,\n",
      "        \"num_train_epochs\": 2,\n",
      "        \"learning_rate\": 0.0001,\n",
      "        \"model_name_or_path\": \"bert-base-uncased\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2020-03-08-06-10-19-858\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-496641494145/pytorch-training-2020-03-08-06-10-19-858/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"pretraining\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"pretraining.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"do_train\":true,\"evaluate_during_training\":false,\"learning_rate\":0.0001,\"mlm\":true,\"model_name_or_path\":\"bert-base-uncased\",\"model_type\":\"bert\",\"num_train_epochs\":2,\"per_gpu_train_batch_size\":4,\"seed\":42}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=pretraining.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=pretraining\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-496641494145/pytorch-training-2020-03-08-06-10-19-858/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"do_train\":true,\"evaluate_during_training\":false,\"learning_rate\":0.0001,\"mlm\":true,\"model_name_or_path\":\"bert-base-uncased\",\"model_type\":\"bert\",\"num_train_epochs\":2,\"per_gpu_train_batch_size\":4,\"seed\":42},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2020-03-08-06-10-19-858\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-496641494145/pytorch-training-2020-03-08-06-10-19-858/source/sourcedir.tar.gz\",\"module_name\":\"pretraining\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"pretraining.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--do_train\",\"True\",\"--evaluate_during_training\",\"False\",\"--learning_rate\",\"0.0001\",\"--mlm\",\"True\",\"--model_name_or_path\",\"bert-base-uncased\",\"--model_type\",\"bert\",\"--num_train_epochs\",\"2\",\"--per_gpu_train_batch_size\",\"4\",\"--seed\",\"42\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=42\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_TYPE=bert\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[34mSM_HP_PER_GPU_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_MLM=true\u001b[0m\n",
      "\u001b[34mSM_HP_EVALUATE_DURING_TRAINING=false\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=bert-base-uncased\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python pretraining.py --do_train True --evaluate_during_training False --learning_rate 0.0001 --mlm True --model_name_or_path bert-base-uncased --model_type bert --num_train_epochs 2 --per_gpu_train_batch_size 4 --seed 42\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mmodel_class: <class 'transformers.modeling_bert.BertForMaskedLM'>\u001b[0m\n",
      "\u001b[34m03/08/2020 06:14:56 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[35mModel: BertForMaskedLM(\u001b[0m\n",
      "\u001b[35m03/08/2020 06:14:46 - INFO - filelock -   Lock 139832942036192 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685.lock\n",
      "  (bert): BertModel(\u001b[0m\n",
      "\u001b[35m03/08/2020 06:14:46 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp0hmvw4dm\n",
      "    (embeddings): BertEmbeddings(\u001b[0m\n",
      "\u001b[35m#015Downloading:   0%|          | 0.00/361 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 361/361 [00:00<00:00, 312kB/s]\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\u001b[0m\n",
      "\u001b[35m03/08/2020 06:14:47 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json in cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "      (position_embeddings): Embedding(512, 768)\u001b[0m\n",
      "\u001b[35m03/08/2020 06:14:47 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "      (token_type_embeddings): Embedding(2, 768)\u001b[0m\n",
      "\u001b[35m03/08/2020 06:14:47 - INFO - filelock -   Lock 139832942036192 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685.lock\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\u001b[0m\n",
      "\u001b[35m03/08/2020 06:14:47 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\u001b[0m\n",
      "\u001b[35m03/08/2020 06:14:47 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "    )\n",
      "  \"architectures\": [\n",
      "    (encoder): BertEncoder(\n",
      "    \"BertForMaskedLM\"\n",
      "      (layer): ModuleList(\n",
      "  ],\n",
      "        (0): BertLayer(\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "          (attention): BertAttention(\n",
      "  \"bos_token_id\": null,\n",
      "            (self): BertSelfAttention(\n",
      "  \"do_sample\": false,\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  \"eos_token_ids\": null,\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  \"finetuning_task\": null,\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  \"hidden_act\": \"gelu\",\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "            )\n",
      "  \"hidden_size\": 768,\n",
      "            (output): BertSelfOutput(\n",
      "  \"id2label\": {\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    \"0\": \"LABEL_0\",\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    \"1\": \"LABEL_1\"\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "  },\n",
      "            )\n",
      "  \"initializer_range\": 0.02,\n",
      "          )\n",
      "  \"intermediate_size\": 3072,\n",
      "          (intermediate): BertIntermediate(\n",
      "  \"is_decoder\": false,\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  \"label2id\": {\n",
      "          )\n",
      "    \"LABEL_0\": 0,\n",
      "          (output): BertOutput(\n",
      "    \"LABEL_1\": 1\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  },\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "  \"length_penalty\": 1.0,\n",
      "          )\n",
      "  \"max_length\": 20,\n",
      "        )\n",
      "  \"max_position_embeddings\": 512,\n",
      "        (1): BertLayer(\n",
      "  \"model_type\": \"bert\",\n",
      "          (attention): BertAttention(\n",
      "  \"num_attention_heads\": 12,\n",
      "            (self): BertSelfAttention(\n",
      "  \"num_beams\": 1,\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  \"num_hidden_layers\": 12,\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  \"num_labels\": 2,\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  \"num_return_sequences\": 1,\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "  \"output_attentions\": false,\n",
      "            )\n",
      "  \"output_hidden_states\": false,\n",
      "            (output): BertSelfOutput(\n",
      "  \"output_past\": true,\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  \"pad_token_id\": null,\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  \"pruned_heads\": {},\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "  \"repetition_penalty\": 1.0,\n",
      "            )\n",
      "  \"temperature\": 1.0,\n",
      "          )\n",
      "  \"top_k\": 50,\n",
      "          (intermediate): BertIntermediate(\n",
      "  \"top_p\": 1.0,\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  \"torchscript\": false,\n",
      "          )\n",
      "  \"type_vocab_size\": 2,\n",
      "          (output): BertOutput(\n",
      "  \"use_bfloat16\": false,\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  \"vocab_size\": 30522\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\u001b[0m\n",
      "\u001b[35m}\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "\n",
      "          )\u001b[0m\n",
      "\u001b[35m03/08/2020 06:14:47 - INFO - filelock -   Lock 139832950489440 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "        )\u001b[0m\n",
      "\u001b[35m03/08/2020 06:14:47 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp3mutc9fp\n",
      "        (2): BertLayer(\u001b[0m\n",
      "\u001b[35m#015Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]#015Downloading:  23%|██▎       | 52.2k/232k [00:00<00:00, 381kB/s]#015Downloading: 100%|██████████| 232k/232k [00:00<00:00, 1.11MB/s]\n",
      "          (attention): BertAttention(\u001b[0m\n",
      "\u001b[35m03/08/2020 06:14:47 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "            (self): BertSelfAttention(\u001b[0m\n",
      "\u001b[35m03/08/2020 06:14:47 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[35m03/08/2020 06:14:47 - INFO - filelock -   Lock 139832950489440 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[35m03/08/2020 06:14:47 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[35m03/08/2020 06:14:48 - INFO - filelock -   Lock 139833140879312 acquired on /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\u001b[0m\n",
      "\u001b[35m03/08/2020 06:14:48 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp1k092eq8\n",
      "            )\u001b[0m\n",
      "\u001b[35m#015Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]#015Downloading:   0%|          | 34.8k/440M [00:00<23:35, 311kB/s]#015Downloading:   0%|          | 261k/440M [00:00<18:05, 405kB/s] #015Downloading:   0%|          | 1.11M/440M [00:00<12:56, 566kB/s]#015Downloading:   1%|          | 2.56M/440M [00:00<09:10, 795kB/s]#015Downloading:   1%|          | 4.92M/440M [00:00<06:29, 1.12MB/s]#015Downloading:   2%|▏         | 7.39M/440M [00:00<04:36, 1.57MB/s]#015Downloading:   2%|▏         | 9.75M/440M [00:00<03:18, 2.17MB/s]#015Downloading:   3%|▎         | 12.2M/440M [00:00<02:23, 2.99MB/s]#015Downloading:   3%|▎         | 14.6M/440M [00:01<01:45, 4.04MB/s]#015Downloading:   4%|▍         | 17.0M/440M [00:01<01:18, 5.39MB/s]#015Downloading:   4%|▍         | 19.4M/440M [00:01<01:00, 6.97MB/s]#015Downloading:   5%|▍         | 21.9M/440M [00:01<00:47, 8.88MB/s]#015Downloading:   6%|▌         | 24.3M/440M [00:01<00:38, 10.8MB/s]#015Downloading:   6%|▌         | 26.7M/440M [00:01<00:31, 13.0MB/s]#015Downloading:   7%|▋         | 29.1M/440M [00:01<00:27, 14.8MB/s]#015Downloading:   7%|▋         | 31.6M/440M [00:01<00:24, 16.9MB/s]#015Downloading:   8%|▊         | 33.9M/440M [00:01<00:22, 18.1MB/s]#015Downloading:   8%|▊         | 36.4M/440M [00:01<00:20, 19.7MB/s]#015Downloading:   9%|▉         | 38.8M/440M [00:02<00:19, 20.3MB/s]#015Downloading:   9%|▉         | 41.2M/440M [00:02<00:18, 21.5MB/s]#015Downloading:  10%|▉         | 43.6M/440M [00:02<00:18, 21.6MB/s]#015Downloading:  10%|█         | 46.1M/440M [00:02<00:17, 22.4MB/s]#015Downloading:  11%|█         | 48.4M/440M [00:02<00:17, 22.3MB/s]#015Downloading:  12%|█▏        | 50.9M/440M [00:02<00:16, 22.9MB/s]#015Downloading:  12%|█▏        | 53.3M/440M [00:02<00:17, 22.7MB/s]#015Downloading:  13%|█▎        | 55.7M/440M [00:02<00:16, 23.3MB/s]#015Downloading:  13%|█▎        | 58.1M/440M [00:02<00:16, 22.9MB/s]#015Downloading:  14%|█▍        | 60.6M/440M [00:02<00:16, 23.3MB/s]#015Downloading:  14%|█▍        | 63.0M/440M [00:03<00:16, 23.0MB/s]#015Downloading:  15%|█▍        | 65.4M/440M [00:03<00:16, 23.4MB/s]#015Downloading:  15%|█▌        | 67.8M/440M [00:03<00:16, 23.0MB/s]#015Downloading:  16%|█▌        | 70.2M/440M [00:03<00:15, 23.5MB/s]#015Downloading:  16%|█▋        | 72.6M/440M [00:03<00:15, 23.0MB/s]#015Downloading:  17%|█▋        | 75.1M/440M [00:03<00:15, 23.5MB/s]#015Downloading:  18%|█▊        | 77.5M/440M [00:03<00:15, 23.0MB/s]#015Downloading:  18%|█▊        | 79.9M/440M [00:03<00:15, 23.5MB/s]#015Downloading:  19%|█▊        | 82.3M/440M [00:03<00:15, 23.0MB/s]#015Downloading:  19%|█▉        | 84.7M/440M [00:04<00:15, 23.5MB/s]#015Downloading:  20%|█▉        | 87.1M/440M [00:04<00:15, 23.1MB/s]#015Downloading:  20%|██        | 89.6M/440M [00:04<00:14, 23.5MB/s]#015Downloading:  21%|██        | 92.0M/440M [00:04<00:15, 23.1MB/s]#015Downloading:  21%|██▏       | 94.4M/440M [00:04<00:14, 23.4MB/s]#015Downloading:  22%|██▏       | 96.8M/440M [00:04<00:14, 23.1MB/s]#015Downloading:  23%|██▎       | 99.2M/440M [00:04<00:14, 23.5MB/s]#015Downloading:  23%|██▎       | 102M/440M [00:04<00:14, 23.1MB/s] #015Downloading:  24%|██▎       | 104M/440M [00:04<00:14, 23.4MB/s]#015Downloading:  24%|██▍       | 106M/440M [00:04<00:14, 23.1MB/s]#015Downloading:  25%|██▍       | 109M/440M [00:05<00:14, 23.5MB/s]#015Downloading:  25%|██▌       | 111M/440M [00:05<00:14, 23.1MB/s]#015Downloading:  26%|██▌       | 114M/440M [00:05<00:13, 23.5MB/s]#015Downloading:  26%|██▋       | 116M/440M [00:05<00:14, 23.1MB/s]#015Downloading:  27%|██▋       | 119M/440M [00:05<00:13, 23.4MB/s]#015Downloading:  27%|██▋       | 121M/440M [00:05<00:13, 23.1MB/s]#015Downloading:  28%|██▊       | 123M/440M [00:05<00:13, 23.5MB/s]#015Downloading:  29%|██▊       | 126M/440M [00:05<00:13, 23.1MB/s]#015Downloading:  29%|██▉       | 128M/440M [00:05<00:13, 23.4MB/s]#015Downloading:  30%|██▉       | 131M/440M [00:05<00:13, 23.1MB/s]#015Downloading:  30%|███       | 133M/440M [00:06<00:13, 23.4MB/s]#015Downloading:  31%|███       | 135M/440M [00:06<00:13, 23.1MB/s]#015Downloading:  31%|███▏      | 138M/440M [00:06<00:12, 23.4MB/s]#015Downloading:  32%|███▏      | 140M/440M [00:06<00:12, 23.1MB/s]#015Downloading:  32%|███▏      | 143M/440M [00:06<00:12, 23.4MB/s]#015Downloading:  33%|███▎      | 145M/440M [00:06<00:12, 23.2MB/s]#015Downloading:  34%|███▎      | 148M/440M [00:06<00:12, 23.4MB/s]#015Downloading:  34%|███▍      | 150M/440M [00:06<00:12, 23.1MB/s]#015Downloading:  35%|███▍      | 152M/440M [00:06<00:12, 23.5MB/s]#015Downloading:  35%|███▌      | 155M/440M [00:07<00:12, 23.1MB/s]#015Downloading:  36%|███▌      | 157M/440M [00:07<00:12, 23.5MB/s]#015Downloading:  36%|███▌      | 160M/440M [00:07<00:12, 23.1MB/s]#015Downloading:  37%|███▋      | 162M/440M [00:07<00:11, 23.5MB/s]#015Downloading:  37%|███▋      | 165M/440M [00:07<00:11, 23.1MB/s]#015Downloading:  38%|███▊      | 167M/440M [00:07<00:11, 23.5MB/s]#015Downloading:  38%|███▊      | 169M/440M [00:07<00:11, 23.1MB/s]#015Downloading:  39%|███▉      | 172M/440M [00:07<00:11, 23.5MB/s]#015Downloading:  40%|███▉      | 174M/440M [00:07<00:11, 23.1MB/s]#015Downloading:  40%|████      | 177M/440M [00:07<00:11, 23.5MB/s]#015Downloading:  41%|████      | 179M/440M [00:08<00:11, 23.1MB/s]#015Downloading:  41%|████      | 181M/440M [00:08<00:11, 23.5MB/s]#015Downloading:  42%|████▏     | 184M/440M [00:08<00:11, 23.1MB/s]#015Downloading:  42%|████▏     | 186M/440M [00:08<00:10, 23.5MB/s]#015Downloading:  43%|████▎     | 189M/440M [00:08<00:10, 23.0MB/s]#015Downloading:  43%|████▎     | 191M/440M [00:08<00:10, 23.5MB/s]#015Downloading:  44%|████▍     | 194M/440M [00:08<00:10, 23.1MB/s]#015Downloading:  44%|████▍     | 196M/440M [00:08<00:10, 23.5MB/s]#015Downloading:  45%|████▌     | 198M/440M [00:08<00:10, 23.1MB/s]#015Downloading:  46%|████▌     | 201M/440M [00:09<00:10, 23.5MB/s]#015Downloading:  46%|████▌     | 203M/440M [00:09<00:10, 23.1MB/s]#015Downloading:  47%|████▋     | 206M/440M [00:09<00:10, 23.5MB/s]#015Downloading:  47%|████▋     | 208M/440M [00:09<00:10, 23.1MB/s]#015Downloading:  48%|████▊     | 210M/440M [00:09<00:09, 23.5MB/s]#015Downloading:  48%|████▊     | 213M/440M [00:09<00:09, 23.1MB/s]#015Downloading:  49%|████▉     | 215M/440M [00:09<00:09, 23.5MB/s]#015Downloading:  49%|████▉     | 218M/440M [00:09<00:09, 23.1MB/s]#015Downloading:  50%|████▉     | 220M/440M [00:09<00:10, 21.7MB/s]#015Downloading:  51%|█████     | 223M/440M [00:09<00:09, 23.6MB/s]#015Downloading:  51%|█████     | 226M/440M [00:10<00:08, 24.4MB/s]#015Downloading:  52%|█████▏    | 228M/440M [00:10<00:09, 23.0MB/s]#015Downloading:  52%|█████▏    | 231M/440M [00:10<00:09, 22.8MB/s]#015Downloading:  53%|█████▎    | 233M/440M [00:10<00:08, 23.3MB/s]#015Downloading:  53%|█████▎    | 235M/440M [00:10<00:08, 23.0MB/s]#015Downloading:  54%|█████▍    | 238M/440M [00:10<00:08, 23.4MB/s]#015Downloading:  55%|█████▍    | 240M/440M [00:10<00:08, 23.0MB/s]#015Downloading:  55%|█████▌    | 243M/440M [00:10<00:08, 23.4MB/s]#015Downloading:  56%|█████▌    | 245M/440M [00:10<00:08, 23.1MB/s]#015Downloading:  56%|█████▌    | 248M/440M [00:11<00:08, 23.5MB/s]#015Downloading:  57%|█████▋    | 250M/440M [00:11<00:08, 23.1MB/s]#015Downloading:  57%|█████▋    | 252M/440M [00:11<00:08, 23.5MB/s]#015Downloading:  58%|█████▊    | 255M/440M [00:11<00:08, 23.0MB/s]#015Downloading:  58%|█████▊    | 257M/440M [00:11<00:07, 23.7MB/s]#015Downloading:  59%|█████▉    | 260M/440M [00:11<00:07, 23.1MB/s]#015Downloading:  60%|█████▉    | 262M/440M [00:11<00:07, 23.4MB/s]#015Downloading:  60%|██████    | 264M/440M [00:11<00:07, 23.0MB/s]#015Downloading:  61%|██████    | 267M/440M [00:11<00:07, 23.4MB/s]#015Downloading:  61%|██████    | 269M/440M [00:11<00:07, 23.0MB/s]#015Downloading:  62%|██████▏   | 272M/440M [00:12<00:07, 23.4MB/s]#015Downloading:  62%|██████▏   | 274M/440M [00:12<00:07, 23.0MB/s]#015Downloading:  63%|██████▎   | 277M/440M [00:12<00:06, 23.4MB/s]#015Downloading:  63%|██████▎   | 279M/440M [00:12<00:06, 23.1MB/s]#015Downloading:  64%|██████▍   | 281M/440M [00:12<00:06, 23.5MB/s]#015Downloading:  64%|██████▍   | 284M/440M [00:12<00:06, 23.1MB/s]#015Downloading:  65%|██████▍   | 286M/440M [00:12<00:06, 23.5MB/s]#015Downloading:  66%|██████▌   | 289M/440M [00:12<00:06, 23.1MB/s]#015Downloading:  66%|██████▌   | 291M/440M [00:12<00:06, 23.4MB/s]#015Downloading:  67%|██████▋   | 293M/440M [00:12<00:06, 23.1MB/s]#015Downloading:  67%|██████▋   | 296M/440M [00:13<00:06, 23.5MB/s]#015Downloading:  68%|██████▊   | 298M/440M [00:13<00:06, 23.1MB/s]#015Downloading:  68%|██████▊   | 301M/440M [00:13<00:05, 23.5MB/s]#015Downloading:  69%|██████▉   | 303M/440M [00:13<00:05, 23.1MB/s]#015Downloading:  69%|██████▉   | 306M/440M [00:13<00:05, 23.5MB/s]#015Downloading:  70%|██████▉   | 308M/440M [00:13<00:05, 23.1MB/s]#015Downloading:  70%|███████   | 310M/440M [00:13<00:05, 23.5MB/s]#015Downloading:  71%|███████   | 313M/440M [00:13<00:05, 23.0MB/s]#015Downloading:  72%|███████▏  | 315M/440M [00:13<00:05, 23.5MB/s]#015Downloading:  72%|███████▏  | 318M/440M [00:14<00:05, 23.0MB/s]#015Downloading:  73%|███████▎  | 320M/440M [00:14<00:05, 23.5MB/s]#015Downloading:  73%|███████▎  | 322M/440M [00:14<00:05, 23.0MB/s]#015Downloading:  74%|███████▍  | 325M/440M [00:14<00:04, 23.5MB/s]#015Downloading:  74%|███████▍  | 327M/440M [00:14<00:04, 23.0MB/s]#015Downloading:  75%|███████▍  | 330M/440M [00:14<00:04, 23.1MB/s]#015Downloading:  75%|███████▌  | 332M/440M [00:14<00:04, 23.1MB/s]#015Downloading:  76%|███████▌  | 335M/440M [00:14<00:04, 23.6MB/s]#015Downloading:  77%|███████▋  | 337M/440M [00:14<00:04, 23.1MB/s]#015Downloading:  77%|███████▋  | 340M/440M [00:14<00:04, 23.6MB/s]#015Downloading:  78%|███████▊  | 342M/440M [00:15<00:04, 23.1MB/s]#015Downloading:  78%|███████▊  | 344M/440M [00:15<00:04, 23.5MB/s]#015Downloading:  79%|███████▊  | 347M/440M [00:15<00:04, 23.0MB/s]#015Downloading:  79%|███████▉  | 349M/440M [00:15<00:03, 23.5MB/s]#015Downloading:  80%|███████▉  | 352M/440M [00:15<00:03, 23.0MB/s]#015Downloading:  80%|████████  | 354M/440M [00:15<00:03, 23.5MB/s]#015Downloading:  81%|████████  | 356M/440M [00:15<00:03, 23.0MB/s]#015Downloading:  81%|████████▏ | 359M/440M [00:15<00:03, 23.5MB/s]#015Downloading:  82%|████████▏ | 361M/440M [00:15<00:03, 23.0MB/s]#015Downloading:  83%|████████▎ | 364M/440M [00:15<00:03, 23.6MB/s]#015Downloading:  83%|████████▎ | 366M/440M [00:16<00:03, 23.0MB/s]#015Downloading:  84%|████████▎ | 369M/440M [00:16<00:03, 23.6MB/s]#015Downloading:  84%|████████▍ | 371M/440M [00:16<00:03, 23.0MB/s]#015Downloading:  85%|████████▍ | 373M/440M [00:16<00:02, 23.5MB/s]#015Downloading:  85%|████████▌ | 376M/440M [00:16<00:02, 23.0MB/s]#015Downloading:  86%|████████▌ | 378M/440M [00:16<00:02, 23.6MB/s]#015Downloading:  86%|████████▋ | 381M/440M [00:16<00:02, 23.0MB/s]#015Downloading:  87%|████████▋ | 383M/440M [00:16<00:02, 23.5MB/s]#015Downloading:  88%|████████▊ | 385M/440M [00:16<00:02, 23.0MB/s]#015Downloading:  88%|████████▊ | 388M/440M [00:17<00:02, 23.5MB/s]#015Downloading:  89%|████████▊ | 390M/440M [00:17<00:02, 23.0MB/s]#015Downloading:  89%|████████▉ | 393M/440M [00:17<00:02, 23.5MB/s]#015Downloading:  90%|████████▉ | 395M/440M [00:17<00:01, 23.0MB/s]#015Downloading:  90%|█████████ | 398M/440M [00:17<00:01, 23.6MB/s]#015Downloading:  91%|█████████ | 400M/440M [00:17<00:01, 23.0MB/s]#015Downloading:  91%|█████████▏| 403M/440M [00:17<00:01, 23.6MB/s]#015Downloading:  92%|█████████▏| 405M/440M [00:17<00:01, 23.0MB/s]#015Downloading:  93%|█████████▎| 407M/440M [00:17<00:01, 23.6MB/s]#015Downloading:  93%|█████████▎| 410M/440M [00:17<00:01, 23.1MB/s]#015Downloading:  94%|█████████▎| 412M/440M [00:18<00:01, 23.7MB/s]#015Downloading:  94%|█████████▍| 415M/440M [00:18<00:01, 23.1MB/s]#015Downloading:  95%|█████████▍| 417M/440M [00:18<00:00, 23.7MB/s]#015Downloading:  95%|█████████▌| 420M/440M [00:18<00:00, 23.0MB/s]#015Downloading:  96%|█████████▌| 422M/440M [00:18<00:00, 23.7MB/s]#015Downloading:  96%|█████████▋| 425M/440M [00:18<00:00, 23.1MB/s]#015Downloading:  97%|█████████▋| 427M/440M [00:18<00:00, 23.8MB/s]#015Downloading:  98%|█████████▊| 430M/440M [00:18<00:00, 23.2MB/s]#015Downloading:  98%|█████████▊| 432M/440M [00:18<00:00, 23.9MB/s]#015Downloading:  99%|█████████▊| 435M/440M [00:19<00:00, 22.5MB/s]#015Downloading:  99%|█████████▉| 437M/440M [00:19<00:00, 22.4MB/s]#015Downloading: 100%|█████████▉| 439M/440M [00:19<00:00, 23.1MB/s]#015Downloading: 100%|██████████| 440M/440M [00:19<00:00, 22.8MB/s]\n",
      "            (output): BertSelfOutput(\u001b[0m\n",
      "\u001b[35m03/08/2020 06:15:07 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin in cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[35m03/08/2020 06:15:07 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\u001b[0m\n",
      "\u001b[35m03/08/2020 06:15:07 - INFO - filelock -   Lock 139833140879312 released on /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\u001b[0m\n",
      "\u001b[35m03/08/2020 06:15:07 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "            )\u001b[0m\n",
      "\u001b[35m03/08/2020 06:15:11 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "          )\u001b[0m\n",
      "\u001b[35m03/08/2020 06:15:11 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "          (intermediate): BertIntermediate(\u001b[0m\n",
      "\u001b[35m03/08/2020 06:15:14 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=False, do_train=True, eval_all_checkpoints=False, eval_data_file=None, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=0.0001, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=True, mlm_probability=0.15, model_dir='/opt/ml/model', model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=2.0, overwrite_cache=False, overwrite_model_dir=False, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=500, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train='/opt/ml/input/data/train', warmup_steps=0, weight_decay=0.0)\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\u001b[0m\n",
      "\u001b[35m03/08/2020 06:15:14 - INFO - __main__ -   Creating features from dataset file at /opt/ml/input/data/train\n",
      "          )\u001b[0m\n",
      "\u001b[35m03/08/2020 06:15:15 - INFO - __main__ -   Saving features into cached file /opt/ml/input/data/train/bert_cached_lm_510_testfile.txt\n",
      "          (output): BertOutput(\u001b[0m\n",
      "\u001b[35m03/08/2020 06:15:15 - INFO - __main__ -   ***** Running training *****\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[35m03/08/2020 06:15:15 - INFO - __main__ -     Num examples = 110\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\u001b[0m\n",
      "\u001b[35m03/08/2020 06:15:15 - INFO - __main__ -     Num Epochs = 2\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\u001b[0m\n",
      "\u001b[35m03/08/2020 06:15:15 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
      "          )\u001b[0m\n",
      "\u001b[35m03/08/2020 06:15:15 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "        )\u001b[0m\n",
      "\u001b[35m03/08/2020 06:15:15 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "        (3): BertLayer(\u001b[0m\n",
      "\u001b[35m03/08/2020 06:15:15 - INFO - __main__ -     Total optimization steps = 56\n",
      "          (attention): BertAttention(\u001b[0m\n",
      "\u001b[35m#015Epoch:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "            (self): BertSelfAttention(\u001b[0m\n",
      "\u001b[35m#015Iteration:   0%|          | 0/28 [00:00<?, ?it/s]#033[A\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[35m#015Iteration:   4%|▎         | 1/28 [00:01<00:39,  1.47s/it]#033[A\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[35m#015Iteration:   7%|▋         | 2/28 [00:01<00:28,  1.10s/it]#033[A\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[35m#015Iteration:  11%|█         | 3/28 [00:01<00:20,  1.19it/s]#033[A\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\u001b[0m\n",
      "\u001b[35m#015Iteration:  14%|█▍        | 4/28 [00:02<00:15,  1.53it/s]#033[A\n",
      "            )\u001b[0m\n",
      "\u001b[35m#015Iteration:  18%|█▊        | 5/28 [00:02<00:12,  1.90it/s]#033[A\n",
      "            (output): BertSelfOutput(\u001b[0m\n",
      "\u001b[35m#015Iteration:  21%|██▏       | 6/28 [00:02<00:09,  2.30it/s]#033[A\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[35m#015Iteration:  25%|██▌       | 7/28 [00:02<00:07,  2.69it/s]#033[A\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\u001b[0m\n",
      "\u001b[35m#015Iteration:  29%|██▊       | 8/28 [00:03<00:06,  3.06it/s]#033[A\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\u001b[0m\n",
      "\u001b[35m#015Iteration:  32%|███▏      | 9/28 [00:03<00:05,  3.37it/s]#033[A\n",
      "            )\u001b[0m\n",
      "\u001b[35m#015Iteration:  36%|███▌      | 10/28 [00:03<00:04,  3.64it/s]#033[A\n",
      "          )\u001b[0m\n",
      "\u001b[35m#015Iteration:  39%|███▉      | 11/28 [00:03<00:04,  3.85it/s]#033[A\n",
      "          (intermediate): BertIntermediate(\u001b[0m\n",
      "\u001b[35m#015Iteration:  43%|████▎     | 12/28 [00:03<00:03,  4.01it/s]#033[A\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\u001b[0m\n",
      "\u001b[35m#015Iteration:  46%|████▋     | 13/28 [00:04<00:03,  4.13it/s]#033[A\n",
      "          )\u001b[0m\n",
      "\u001b[35m#015Iteration:  50%|█████     | 14/28 [00:04<00:03,  4.22it/s]#033[A\n",
      "          (output): BertOutput(\u001b[0m\n",
      "\u001b[35m#015Iteration:  54%|█████▎    | 15/28 [00:04<00:03,  4.29it/s]#033[A\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[35m#015Iteration:  57%|█████▋    | 16/28 [00:04<00:02,  4.33it/s]#033[A\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\u001b[0m\n",
      "\u001b[35m#015Iteration:  61%|██████    | 17/28 [00:05<00:02,  4.36it/s]#033[A\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\u001b[0m\n",
      "\u001b[35m#015Iteration:  64%|██████▍   | 18/28 [00:05<00:02,  4.38it/s]#033[A\n",
      "          )\u001b[0m\n",
      "\u001b[35m#015Iteration:  68%|██████▊   | 19/28 [00:05<00:02,  4.40it/s]#033[A\n",
      "        )\u001b[0m\n",
      "\u001b[35m#015Iteration:  71%|███████▏  | 20/28 [00:05<00:01,  4.42it/s]#033[A\n",
      "        (4): BertLayer(\u001b[0m\n",
      "\u001b[35m#015Iteration:  75%|███████▌  | 21/28 [00:05<00:01,  4.43it/s]#033[A\n",
      "          (attention): BertAttention(\u001b[0m\n",
      "\u001b[35m#015Iteration:  79%|███████▊  | 22/28 [00:06<00:01,  4.41it/s]#033[A\n",
      "            (self): BertSelfAttention(\u001b[0m\n",
      "\u001b[35m#015Iteration:  82%|████████▏ | 23/28 [00:06<00:01,  4.43it/s]#033[A\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[35m#015Iteration:  86%|████████▌ | 24/28 [00:06<00:00,  4.44it/s]#033[A\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[35m#015Iteration:  89%|████████▉ | 25/28 [00:06<00:00,  4.44it/s]#033[A\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[35m#015Iteration:  93%|█████████▎| 26/28 [00:07<00:00,  4.44it/s]#033[A\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\u001b[0m\n",
      "\u001b[35m#015Iteration:  96%|█████████▋| 27/28 [00:07<00:00,  4.45it/s]#033[A\n",
      "            )\u001b[0m\n",
      "\u001b[35m#015Iteration: 100%|██████████| 28/28 [00:07<00:00,  4.97it/s]#033[A#015Iteration: 100%|██████████| 28/28 [00:07<00:00,  3.75it/s]\n",
      "            (output): BertSelfOutput(\u001b[0m\n",
      "\u001b[35m#015Epoch:  50%|█████     | 1/2 [00:07<00:07,  7.47s/it]\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[35m#015Iteration:   0%|          | 0/28 [00:00<?, ?it/s]#033[A\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\u001b[0m\n",
      "\u001b[35m#015Iteration:   4%|▎         | 1/28 [00:00<00:06,  4.38it/s]#033[A\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\u001b[0m\n",
      "\u001b[35m#015Iteration:   7%|▋         | 2/28 [00:00<00:05,  4.40it/s]#033[A\n",
      "            )\u001b[0m\n",
      "\u001b[35m#015Iteration:  11%|█         | 3/28 [00:00<00:05,  4.41it/s]#033[A\n",
      "          )\u001b[0m\n",
      "\u001b[35m#015Iteration:  14%|█▍        | 4/28 [00:00<00:05,  4.41it/s]#033[A\n",
      "          (intermediate): BertIntermediate(\u001b[0m\n",
      "\u001b[35m#015Iteration:  18%|█▊        | 5/28 [00:01<00:05,  4.41it/s]#033[A\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\u001b[0m\n",
      "\u001b[35m#015Iteration:  21%|██▏       | 6/28 [00:01<00:04,  4.42it/s]#033[A\n",
      "          )\u001b[0m\n",
      "\u001b[35m#015Iteration:  25%|██▌       | 7/28 [00:01<00:04,  4.43it/s]#033[A\n",
      "          (output): BertOutput(\u001b[0m\n",
      "\u001b[35m#015Iteration:  29%|██▊       | 8/28 [00:01<00:04,  4.43it/s]#033[A\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[35m#015Iteration:  32%|███▏      | 9/28 [00:02<00:04,  4.41it/s]#033[A\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\u001b[0m\n",
      "\u001b[35m#015Iteration:  36%|███▌      | 10/28 [00:02<00:04,  4.42it/s]#033[A\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\u001b[0m\n",
      "\u001b[35m#015Iteration:  39%|███▉      | 11/28 [00:02<00:03,  4.42it/s]#033[A\n",
      "          )\u001b[0m\n",
      "\u001b[35m#015Iteration:  43%|████▎     | 12/28 [00:02<00:03,  4.42it/s]#033[A\n",
      "        )\u001b[0m\n",
      "\u001b[35m#015Iteration:  46%|████▋     | 13/28 [00:02<00:03,  4.42it/s]#033[A\n",
      "        (5): BertLayer(\u001b[0m\n",
      "\u001b[35m#015Iteration:  50%|█████     | 14/28 [00:03<00:03,  4.42it/s]#033[A\n",
      "          (attention): BertAttention(\u001b[0m\n",
      "\u001b[35m#015Iteration:  54%|█████▎    | 15/28 [00:03<00:02,  4.43it/s]#033[A\n",
      "            (self): BertSelfAttention(\u001b[0m\n",
      "\u001b[35m#015Iteration:  57%|█████▋    | 16/28 [00:03<00:02,  4.41it/s]#033[A\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[35m#015Iteration:  61%|██████    | 17/28 [00:03<00:02,  4.42it/s]#033[A\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[35m#015Iteration:  64%|██████▍   | 18/28 [00:04<00:02,  4.42it/s]#033[A\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[35m#015Iteration:  68%|██████▊   | 19/28 [00:04<00:02,  4.42it/s]#033[A\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\u001b[0m\n",
      "\u001b[35m#015Iteration:  71%|███████▏  | 20/28 [00:04<00:01,  4.43it/s]#033[A\n",
      "            )\u001b[0m\n",
      "\u001b[35m#015Iteration:  75%|███████▌  | 21/28 [00:04<00:01,  4.44it/s]#033[A\n",
      "            (output): BertSelfOutput(\u001b[0m\n",
      "\u001b[35m#015Iteration:  79%|███████▊  | 22/28 [00:04<00:01,  4.43it/s]#033[A\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[35m#015Iteration:  82%|████████▏ | 23/28 [00:05<00:01,  4.43it/s]#033[A\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\u001b[0m\n",
      "\u001b[35m#015Iteration:  86%|████████▌ | 24/28 [00:05<00:00,  4.43it/s]#033[A\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\u001b[0m\n",
      "\u001b[35m#015Iteration:  89%|████████▉ | 25/28 [00:05<00:00,  4.42it/s]#033[A\n",
      "            )\u001b[0m\n",
      "\u001b[35m#015Iteration:  93%|█████████▎| 26/28 [00:05<00:00,  4.42it/s]#033[A\n",
      "          )\u001b[0m\n",
      "\u001b[35m#015Iteration:  96%|█████████▋| 27/28 [00:06<00:00,  4.42it/s]#033[A\n",
      "          (intermediate): BertIntermediate(\u001b[0m\n",
      "\u001b[35m#015Iteration: 100%|██████████| 28/28 [00:06<00:00,  4.98it/s]#033[A#015Iteration: 100%|██████████| 28/28 [00:06<00:00,  4.48it/s]\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\u001b[0m\n",
      "\u001b[35m#015Epoch: 100%|██████████| 2/2 [00:13<00:00,  7.11s/it]#015Epoch: 100%|██████████| 2/2 [00:13<00:00,  6.86s/it]\n",
      "          )\u001b[0m\n",
      "\u001b[35m03/08/2020 06:15:28 - INFO - __main__ -    global_step = 56, average loss = 2.726954745394843\n",
      "          (output): BertOutput(\u001b[0m\n",
      "\u001b[35m03/08/2020 06:15:28 - INFO - __main__ -   Saving model checkpoint to /opt/ml/model\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[35m03/08/2020 06:15:28 - INFO - transformers.configuration_utils -   Configuration saved in /opt/ml/model/config.json\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\u001b[0m\n",
      "\u001b[35m03/08/2020 06:15:29 - INFO - transformers.modeling_utils -   Model weights saved in /opt/ml/model/pytorch_model.bin\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\u001b[0m\n",
      "\u001b[35m03/08/2020 06:15:29 - INFO - transformers.configuration_utils -   loading configuration file /opt/ml/model/config.json\n",
      "          )\u001b[0m\n",
      "\u001b[35m03/08/2020 06:15:29 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "        )\n",
      "  \"architectures\": [\n",
      "        (6): BertLayer(\n",
      "    \"BertForMaskedLM\"\n",
      "          (attention): BertAttention(\n",
      "  ],\n",
      "            (self): BertSelfAttention(\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  \"bos_token_id\": null,\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  \"do_sample\": false,\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  \"eos_token_ids\": null,\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "  \"finetuning_task\": null,\n",
      "            )\n",
      "  \"hidden_act\": \"gelu\",\n",
      "            (output): BertSelfOutput(\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  \"hidden_size\": 768,\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  \"id2label\": {\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "    \"0\": \"LABEL_0\",\n",
      "            )\n",
      "    \"1\": \"LABEL_1\"\n",
      "          )\n",
      "  },\n",
      "          (intermediate): BertIntermediate(\n",
      "  \"initializer_range\": 0.02,\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  \"intermediate_size\": 3072,\n",
      "          )\n",
      "  \"is_decoder\": false,\n",
      "          (output): BertOutput(\n",
      "  \"label2id\": {\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    \"LABEL_0\": 0,\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    \"LABEL_1\": 1\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "  },\n",
      "          )\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "        )\n",
      "  \"length_penalty\": 1.0,\n",
      "        (7): BertLayer(\n",
      "  \"max_length\": 20,\n",
      "          (attention): BertAttention(\n",
      "  \"max_position_embeddings\": 512,\n",
      "            (self): BertSelfAttention(\n",
      "  \"model_type\": \"bert\",\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  \"num_attention_heads\": 12,\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  \"num_beams\": 1,\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  \"num_hidden_layers\": 12,\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "  \"num_labels\": 2,\n",
      "            )\n",
      "  \"num_return_sequences\": 1,\n",
      "            (output): BertSelfOutput(\n",
      "  \"output_attentions\": false,\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  \"output_hidden_states\": false,\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  \"output_past\": true,\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "  \"pad_token_id\": null,\n",
      "            )\n",
      "  \"pruned_heads\": {},\n",
      "          )\n",
      "  \"repetition_penalty\": 1.0,\n",
      "          (intermediate): BertIntermediate(\n",
      "  \"temperature\": 1.0,\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  \"top_k\": 50,\n",
      "          )\n",
      "  \"top_p\": 1.0,\n",
      "          (output): BertOutput(\n",
      "  \"torchscript\": false,\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  \"type_vocab_size\": 2,\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  \"use_bfloat16\": false,\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "  \"vocab_size\": 30522\n",
      "          )\u001b[0m\n",
      "\u001b[35m}\n",
      "        )\n",
      "\n",
      "        (8): BertLayer(\u001b[0m\n",
      "\u001b[35m03/08/2020 06:15:29 - INFO - transformers.modeling_utils -   loading weights file /opt/ml/model/pytorch_model.bin\n",
      "          (attention): BertAttention(\u001b[0m\n",
      "\u001b[34mModel: BertForMaskedLM(\u001b[0m\n",
      "\u001b[34m03/08/2020 06:14:57 - INFO - filelock -   Lock 140702198572256 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685.lock\n",
      "  (bert): BertModel(\u001b[0m\n",
      "\u001b[34m03/08/2020 06:14:57 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmppad8xcxb\n",
      "    (embeddings): BertEmbeddings(\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/361 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 361/361 [00:00<00:00, 305kB/s]\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\u001b[0m\n",
      "\u001b[34m03/08/2020 06:14:57 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json in cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "      (position_embeddings): Embedding(512, 768)\u001b[0m\n",
      "\u001b[34m03/08/2020 06:14:57 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "      (token_type_embeddings): Embedding(2, 768)\u001b[0m\n",
      "\u001b[34m03/08/2020 06:14:57 - INFO - filelock -   Lock 140702198572256 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685.lock\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\u001b[0m\n",
      "\u001b[34m03/08/2020 06:14:57 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\u001b[0m\n",
      "\u001b[34m03/08/2020 06:14:57 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "    )\n",
      "  \"architectures\": [\n",
      "    (encoder): BertEncoder(\n",
      "    \"BertForMaskedLM\"\n",
      "      (layer): ModuleList(\n",
      "  ],\n",
      "        (0): BertLayer(\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "          (attention): BertAttention(\n",
      "  \"bos_token_id\": null,\n",
      "            (self): BertSelfAttention(\n",
      "  \"do_sample\": false,\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  \"eos_token_ids\": null,\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  \"finetuning_task\": null,\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  \"hidden_act\": \"gelu\",\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "            )\n",
      "  \"hidden_size\": 768,\n",
      "            (output): BertSelfOutput(\n",
      "  \"id2label\": {\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    \"0\": \"LABEL_0\",\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    \"1\": \"LABEL_1\"\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "  },\n",
      "            )\n",
      "  \"initializer_range\": 0.02,\n",
      "          )\n",
      "  \"intermediate_size\": 3072,\n",
      "          (intermediate): BertIntermediate(\n",
      "  \"is_decoder\": false,\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  \"label2id\": {\n",
      "          )\n",
      "    \"LABEL_0\": 0,\n",
      "          (output): BertOutput(\n",
      "    \"LABEL_1\": 1\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  },\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "  \"length_penalty\": 1.0,\n",
      "          )\n",
      "  \"max_length\": 20,\n",
      "        )\n",
      "  \"max_position_embeddings\": 512,\n",
      "        (1): BertLayer(\n",
      "  \"model_type\": \"bert\",\n",
      "          (attention): BertAttention(\n",
      "  \"num_attention_heads\": 12,\n",
      "            (self): BertSelfAttention(\n",
      "  \"num_beams\": 1,\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  \"num_hidden_layers\": 12,\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  \"num_labels\": 2,\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  \"num_return_sequences\": 1,\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "  \"output_attentions\": false,\n",
      "            )\n",
      "  \"output_hidden_states\": false,\n",
      "            (output): BertSelfOutput(\n",
      "  \"output_past\": true,\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  \"pad_token_id\": null,\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  \"pruned_heads\": {},\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "  \"repetition_penalty\": 1.0,\n",
      "            )\n",
      "  \"temperature\": 1.0,\n",
      "          )\n",
      "  \"top_k\": 50,\n",
      "          (intermediate): BertIntermediate(\n",
      "  \"top_p\": 1.0,\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  \"torchscript\": false,\n",
      "          )\n",
      "  \"type_vocab_size\": 2,\n",
      "          (output): BertOutput(\n",
      "  \"use_bfloat16\": false,\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  \"vocab_size\": 30522\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\u001b[0m\n",
      "\u001b[34m}\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "\n",
      "          )\u001b[0m\n",
      "\u001b[34m03/08/2020 06:14:58 - INFO - filelock -   Lock 140702207025504 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "        )\u001b[0m\n",
      "\u001b[34m03/08/2020 06:14:58 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp_5_qlhah\n",
      "        (2): BertLayer(\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]#015Downloading:  15%|█▌        | 34.8k/232k [00:00<00:00, 219kB/s]#015Downloading:  90%|█████████ | 209k/232k [00:00<00:00, 293kB/s] #015Downloading: 100%|██████████| 232k/232k [00:00<00:00, 813kB/s]\n",
      "          (attention): BertAttention(\u001b[0m\n",
      "\u001b[34m03/08/2020 06:14:58 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "            (self): BertSelfAttention(\u001b[0m\n",
      "\u001b[34m03/08/2020 06:14:58 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[34m03/08/2020 06:14:58 - INFO - filelock -   Lock 140702207025504 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[34m03/08/2020 06:14:58 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[34m03/08/2020 06:14:59 - INFO - filelock -   Lock 140702397415376 acquired on /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\u001b[0m\n",
      "\u001b[34m03/08/2020 06:14:59 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpq6230sbf\n",
      "            )\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]#015Downloading:   0%|          | 52.2k/440M [00:00<20:43, 354kB/s]#015Downloading:   0%|          | 296k/440M [00:00<15:50, 463kB/s] #015Downloading:   0%|          | 1.25M/440M [00:00<11:23, 642kB/s]#015Downloading:   1%|          | 3.49M/440M [00:00<08:02, 906kB/s]#015Downloading:   1%|          | 4.99M/440M [00:00<05:47, 1.25MB/s]#015Downloading:   2%|▏         | 7.72M/440M [00:00<04:08, 1.74MB/s]#015Downloading:   2%|▏         | 10.4M/440M [00:00<03:00, 2.39MB/s]#015Downloading:   3%|▎         | 13.1M/440M [00:01<02:12, 3.23MB/s]#015Downloading:   4%|▎         | 15.8M/440M [00:01<01:38, 4.29MB/s]#015Downloading:   4%|▍         | 18.5M/440M [00:01<01:15, 5.57MB/s]#015Downloading:   5%|▍         | 21.2M/440M [00:01<00:59, 7.04MB/s]#015Downloading:   5%|▌         | 23.9M/440M [00:01<00:48, 8.62MB/s]#015Downloading:   6%|▌         | 26.6M/440M [00:01<00:40, 10.2MB/s]#015Downloading:   7%|▋         | 29.3M/440M [00:02<00:34, 11.8MB/s]#015Downloading:   7%|▋         | 32.0M/440M [00:02<00:31, 13.2MB/s]#015Downloading:   8%|▊         | 34.7M/440M [00:02<00:28, 14.3MB/s]#015Downloading:   8%|▊         | 37.4M/440M [00:02<00:26, 15.4MB/s]#015Downloading:   9%|▉         | 40.1M/440M [00:02<00:24, 16.1MB/s]#015Downloading:  10%|▉         | 42.8M/440M [00:02<00:23, 16.6MB/s]#015Downloading:  10%|█         | 45.5M/440M [00:02<00:23, 17.0MB/s]#015Downloading:  11%|█         | 48.2M/440M [00:03<00:22, 17.3MB/s]#015Downloading:  12%|█▏        | 50.9M/440M [00:03<00:22, 17.6MB/s]#015Downloading:  12%|█▏        | 53.6M/440M [00:03<00:21, 17.8MB/s]#015Downloading:  13%|█▎        | 56.3M/440M [00:03<00:21, 17.9MB/s]#015Downloading:  13%|█▎        | 58.9M/440M [00:03<00:21, 17.9MB/s]#015Downloading:  14%|█▍        | 61.6M/440M [00:03<00:21, 18.0MB/s]#015Downloading:  15%|█▍        | 64.4M/440M [00:03<00:20, 18.2MB/s]#015Downloading:  15%|█▌        | 67.1M/440M [00:04<00:20, 18.2MB/s]#015Downloading:  16%|█▌        | 69.9M/440M [00:04<00:20, 18.3MB/s]#015Downloading:  16%|█▋        | 72.6M/440M [00:04<00:20, 18.3MB/s]#015Downloading:  17%|█▋        | 75.3M/440M [00:04<00:19, 18.4MB/s]#015Downloading:  18%|█▊        | 78.0M/440M [00:04<00:17, 20.3MB/s]#015Downloading:  18%|█▊        | 80.1M/440M [00:04<00:19, 18.5MB/s]#015Downloading:  19%|█▊        | 82.4M/440M [00:04<00:19, 18.1MB/s]#015Downloading:  19%|█▉        | 84.9M/440M [00:04<00:18, 19.7MB/s]#015Downloading:  20%|█▉        | 86.9M/440M [00:05<00:19, 17.9MB/s]#015Downloading:  20%|██        | 89.4M/440M [00:05<00:19, 18.0MB/s]#015Downloading:  21%|██        | 92.2M/440M [00:05<00:17, 20.2MB/s]#015Downloading:  21%|██▏       | 94.3M/440M [00:05<00:18, 18.8MB/s]#015Downloading:  22%|██▏       | 97.1M/440M [00:05<00:17, 19.1MB/s]#015Downloading:  23%|██▎       | 100M/440M [00:05<00:16, 21.2MB/s] #015Downloading:  23%|██▎       | 102M/440M [00:05<00:17, 19.6MB/s]#015Downloading:  24%|██▍       | 105M/440M [00:06<00:17, 19.4MB/s]#015Downloading:  24%|██▍       | 107M/440M [00:06<00:15, 21.1MB/s]#015Downloading:  25%|██▍       | 110M/440M [00:06<00:16, 19.6MB/s]#015Downloading:  25%|██▌       | 112M/440M [00:06<00:15, 21.2MB/s]#015Downloading:  26%|██▌       | 115M/440M [00:06<00:16, 19.3MB/s]#015Downloading:  26%|██▋       | 117M/440M [00:06<00:18, 17.6MB/s]#015Downloading:  27%|██▋       | 119M/440M [00:06<00:18, 17.4MB/s]#015Downloading:  28%|██▊       | 122M/440M [00:06<00:17, 17.7MB/s]#015Downloading:  28%|██▊       | 124M/440M [00:07<00:16, 18.8MB/s]#015Downloading:  29%|██▊       | 126M/440M [00:07<00:16, 19.3MB/s]#015Downloading:  29%|██▉       | 129M/440M [00:07<00:15, 20.0MB/s]#015Downloading:  30%|██▉       | 131M/440M [00:07<00:15, 20.1MB/s]#015Downloading:  30%|███       | 134M/440M [00:07<00:14, 20.6MB/s]#015Downloading:  31%|███       | 136M/440M [00:07<00:14, 20.6MB/s]#015Downloading:  31%|███▏      | 138M/440M [00:07<00:14, 20.9MB/s]#015Downloading:  32%|███▏      | 141M/440M [00:07<00:14, 20.8MB/s]#015Downloading:  33%|███▎      | 143M/440M [00:07<00:14, 21.1MB/s]#015Downloading:  33%|███▎      | 145M/440M [00:08<00:14, 21.0MB/s]#015Downloading:  34%|███▎      | 148M/440M [00:08<00:13, 21.1MB/s]#015Downloading:  34%|███▍      | 150M/440M [00:08<00:13, 21.0MB/s]#015Downloading:  35%|███▍      | 153M/440M [00:08<00:13, 21.2MB/s]#015Downloading:  35%|███▌      | 155M/440M [00:08<00:13, 21.1MB/s]#015Downloading:  36%|███▌      | 157M/440M [00:08<00:13, 21.2MB/s]#015Downloading:  36%|███▌      | 159M/440M [00:08<00:13, 21.0MB/s]#015Downloading:  37%|███▋      | 162M/440M [00:08<00:13, 21.2MB/s]#015Downloading:  37%|███▋      | 164M/440M [00:08<00:13, 21.1MB/s]#015Downloading:  38%|███▊      | 167M/440M [00:09<00:12, 21.2MB/s]#015Downloading:  38%|███▊      | 169M/440M [00:09<00:12, 21.1MB/s]#015Downloading:  39%|███▉      | 171M/440M [00:09<00:12, 21.2MB/s]#015Downloading:  39%|███▉      | 174M/440M [00:09<00:12, 21.0MB/s]#015Downloading:  40%|████      | 176M/440M [00:09<00:12, 21.2MB/s]#015Downloading:  40%|████      | 178M/440M [00:09<00:12, 21.1MB/s]#015Downloading:  41%|████      | 181M/440M [00:09<00:12, 21.2MB/s]#015Downloading:  42%|████▏     | 183M/440M [00:09<00:12, 21.0MB/s]#015Downloading:  42%|████▏     | 186M/440M [00:09<00:12, 21.2MB/s]#015Downloading:  43%|████▎     | 188M/440M [00:10<00:12, 21.0MB/s]#015Downloading:  43%|████▎     | 190M/440M [00:10<00:11, 21.2MB/s]#015Downloading:  44%|████▎     | 192M/440M [00:10<00:11, 21.0MB/s]#015Downloading:  44%|████▍     | 195M/440M [00:10<00:11, 21.2MB/s]#015Downloading:  45%|████▍     | 197M/440M [00:10<00:11, 21.0MB/s]#015Downloading:  45%|████▌     | 200M/440M [00:10<00:11, 21.2MB/s]#015Downloading:  46%|████▌     | 202M/440M [00:10<00:11, 21.0MB/s]#015Downloading:  46%|████▋     | 205M/440M [00:10<00:11, 21.2MB/s]#015Downloading:  47%|████▋     | 207M/440M [00:10<00:11, 21.0MB/s]#015Downloading:  48%|████▊     | 209M/440M [00:11<00:10, 21.2MB/s]#015Downloading:  48%|████▊     | 211M/440M [00:11<00:10, 21.0MB/s]#015Downloading:  49%|████▊     | 214M/440M [00:11<00:10, 21.3MB/s]#015Downloading:  49%|████▉     | 216M/440M [00:11<00:10, 21.0MB/s]#015Downloading:  50%|████▉     | 219M/440M [00:11<00:10, 21.2MB/s]#015Downloading:  50%|█████     | 221M/440M [00:11<00:10, 21.0MB/s]#015Downloading:  51%|█████     | 223M/440M [00:11<00:10, 21.3MB/s]#015Downloading:  51%|█████     | 226M/440M [00:11<00:10, 21.0MB/s]#015Downloading:  52%|█████▏    | 228M/440M [00:11<00:09, 21.3MB/s]#015Downloading:  52%|█████▏    | 230M/440M [00:12<00:10, 21.0MB/s]#015Downloading:  53%|█████▎    | 233M/440M [00:12<00:09, 21.3MB/s]#015Downloading:  53%|█████▎    | 235M/440M [00:12<00:09, 21.0MB/s]#015Downloading:  54%|█████▍    | 238M/440M [00:12<00:09, 21.2MB/s]#015Downloading:  54%|█████▍    | 240M/440M [00:12<00:09, 21.0MB/s]#015Downloading:  55%|█████▍    | 242M/440M [00:12<00:09, 21.2MB/s]#015Downloading:  55%|█████▌    | 244M/440M [00:12<00:09, 21.0MB/s]#015Downloading:  56%|█████▌    | 247M/440M [00:12<00:09, 21.3MB/s]#015Downloading:  57%|█████▋    | 249M/440M [00:12<00:09, 20.9MB/s]#015Downloading:  57%|█████▋    | 252M/440M [00:13<00:08, 21.3MB/s]#015Downloading:  58%|█████▊    | 254M/440M [00:13<00:08, 20.9MB/s]#015Downloading:  58%|█████▊    | 256M/440M [00:13<00:08, 21.3MB/s]#015Downloading:  59%|█████▊    | 259M/440M [00:13<00:08, 21.0MB/s]#015Downloading:  59%|█████▉    | 261M/440M [00:13<00:08, 21.3MB/s]#015Downloading:  60%|█████▉    | 263M/440M [00:13<00:08, 21.0MB/s]#015Downloading:  60%|██████    | 266M/440M [00:13<00:08, 21.3MB/s]#015Downloading:  61%|██████    | 268M/440M [00:13<00:08, 21.0MB/s]#015Downloading:  61%|██████▏   | 271M/440M [00:13<00:07, 21.3MB/s]#015Downloading:  62%|██████▏   | 273M/440M [00:14<00:08, 21.0MB/s]#015Downloading:  62%|██████▏   | 275M/440M [00:14<00:07, 21.3MB/s]#015Downloading:  63%|██████▎   | 277M/440M [00:14<00:07, 21.0MB/s]#015Downloading:  64%|██████▎   | 280M/440M [00:14<00:07, 21.3MB/s]#015Downloading:  64%|██████▍   | 282M/440M [00:14<00:07, 21.0MB/s]#015Downloading:  65%|██████▍   | 285M/440M [00:14<00:07, 21.3MB/s]#015Downloading:  65%|██████▌   | 287M/440M [00:14<00:07, 21.0MB/s]#015Downloading:  66%|██████▌   | 289M/440M [00:14<00:07, 21.3MB/s]#015Downloading:  66%|██████▌   | 292M/440M [00:14<00:07, 21.0MB/s]#015Downloading:  67%|██████▋   | 294M/440M [00:15<00:06, 21.3MB/s]#015Downloading:  67%|██████▋   | 296M/440M [00:15<00:06, 21.0MB/s]#015Downloading:  68%|██████▊   | 299M/440M [00:15<00:06, 21.3MB/s]#015Downloading:  68%|██████▊   | 301M/440M [00:15<00:06, 21.0MB/s]#015Downloading:  69%|██████▉   | 304M/440M [00:15<00:06, 21.2MB/s]#015Downloading:  69%|██████▉   | 306M/440M [00:15<00:06, 21.0MB/s]#015Downloading:  70%|██████▉   | 308M/440M [00:15<00:06, 21.3MB/s]#015Downloading:  70%|███████   | 310M/440M [00:15<00:06, 21.0MB/s]#015Downloading:  71%|███████   | 313M/440M [00:15<00:06, 21.2MB/s]#015Downloading:  72%|███████▏  | 315M/440M [00:16<00:05, 21.0MB/s]#015Downloading:  72%|███████▏  | 318M/440M [00:16<00:05, 21.2MB/s]#015Downloading:  73%|███████▎  | 320M/440M [00:16<00:05, 21.0MB/s]#015Downloading:  73%|███████▎  | 322M/440M [00:16<00:05, 21.2MB/s]#015Downloading:  74%|███████▎  | 325M/440M [00:16<00:05, 21.0MB/s]#015Downloading:  74%|███████▍  | 327M/440M [00:16<00:05, 21.2MB/s]#015Downloading:  75%|███████▍  | 329M/440M [00:16<00:05, 21.1MB/s]#015Downloading:  75%|███████▌  | 332M/440M [00:16<00:04, 22.2MB/s]#015Downloading:  76%|███████▌  | 334M/440M [00:16<00:05, 20.8MB/s]#015Downloading:  76%|███████▋  | 337M/440M [00:17<00:04, 20.9MB/s]#015Downloading:  77%|███████▋  | 339M/440M [00:17<00:04, 20.8MB/s]#015Downloading:  77%|███████▋  | 341M/440M [00:17<00:04, 22.1MB/s]#015Downloading:  78%|███████▊  | 343M/440M [00:17<00:04, 20.7MB/s]#015Downloading:  79%|███████▊  | 346M/440M [00:17<00:04, 20.9MB/s]#015Downloading:  79%|███████▉  | 348M/440M [00:17<00:04, 20.8MB/s]#015Downloading:  80%|███████▉  | 351M/440M [00:17<00:04, 22.0MB/s]#015Downloading:  80%|████████  | 353M/440M [00:17<00:04, 20.6MB/s]#015Downloading:  81%|████████  | 355M/440M [00:17<00:04, 20.9MB/s]#015Downloading:  81%|████████  | 358M/440M [00:18<00:03, 20.7MB/s]#015Downloading:  82%|████████▏ | 360M/440M [00:18<00:03, 22.1MB/s]#015Downloading:  82%|████████▏ | 362M/440M [00:18<00:03, 20.7MB/s]#015Downloading:  83%|████████▎ | 365M/440M [00:18<00:03, 20.8MB/s]#015Downloading:  83%|████████▎ | 367M/440M [00:18<00:03, 20.7MB/s]#015Downloading:  84%|████████▍ | 370M/440M [00:18<00:03, 21.0MB/s]#015Downloading:  84%|████████▍ | 372M/440M [00:18<00:03, 20.9MB/s]#015Downloading:  85%|████████▍ | 374M/440M [00:18<00:03, 21.1MB/s]#015Downloading:  85%|████████▌ | 376M/440M [00:18<00:03, 20.9MB/s]#015Downloading:  86%|████████▌ | 379M/440M [00:19<00:02, 21.2MB/s]#015Downloading:  87%|████████▋ | 381M/440M [00:19<00:02, 21.0MB/s]#015Downloading:  87%|████████▋ | 384M/440M [00:19<00:02, 21.2MB/s]#015Downloading:  88%|████████▊ | 386M/440M [00:19<00:02, 21.0MB/s]#015Downloading:  88%|████████▊ | 388M/440M [00:19<00:02, 21.2MB/s]#015Downloading:  89%|████████▊ | 391M/440M [00:19<00:02, 21.0MB/s]#015Downloading:  89%|████████▉ | 393M/440M [00:19<00:02, 21.2MB/s]#015Downloading:  90%|████████▉ | 395M/440M [00:19<00:02, 21.0MB/s]#015Downloading:  90%|█████████ | 398M/440M [00:19<00:02, 21.3MB/s]#015Downloading:  91%|█████████ | 400M/440M [00:20<00:01, 21.0MB/s]#015Downloading:  91%|█████████▏| 403M/440M [00:20<00:01, 21.2MB/s]#015Downloading:  92%|█████████▏| 405M/440M [00:20<00:01, 21.0MB/s]#015Downloading:  92%|█████████▏| 407M/440M [00:20<00:01, 21.2MB/s]#015Downloading:  93%|█████████▎| 409M/440M [00:20<00:01, 21.0MB/s]#015Downloading:  94%|█████████▎| 412M/440M [00:20<00:01, 21.2MB/s]#015Downloading:  94%|█████████▍| 414M/440M [00:20<00:01, 21.0MB/s]#015Downloading:  95%|█████████▍| 417M/440M [00:20<00:01, 22.2MB/s]#015Downloading:  95%|█████████▌| 419M/440M [00:20<00:01, 20.8MB/s]#015Downloading:  96%|█████████▌| 421M/440M [00:21<00:00, 20.9MB/s]#015Downloading:  96%|█████████▌| 424M/440M [00:21<00:00, 20.8MB/s]#015Downloading:  97%|█████████▋| 426M/440M [00:21<00:00, 22.0MB/s]#015Downloading:  97%|█████████▋| 428M/440M [00:21<00:00, 20.6MB/s]#015Downloading:  98%|█████████▊| 431M/440M [00:21<00:00, 20.9MB/s]#015Downloading:  98%|█████████▊| 433M/440M [00:21<00:00, 20.8MB/s]#015Downloading:  99%|█████████▉| 436M/440M [00:21<00:00, 22.0MB/s]#015Downloading:  99%|█████████▉| 438M/440M [00:21<00:00, 20.6MB/s]#015Downloading: 100%|█████████▉| 440M/440M [00:21<00:00, 21.8MB/s]#015Downloading: 100%|██████████| 440M/440M [00:21<00:00, 20.1MB/s]\n",
      "            (output): BertSelfOutput(\u001b[0m\n",
      "\u001b[34m03/08/2020 06:15:21 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin in cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[34m03/08/2020 06:15:21 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\u001b[0m\n",
      "\u001b[34m03/08/2020 06:15:21 - INFO - filelock -   Lock 140702397415376 released on /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\u001b[0m\n",
      "\u001b[34m03/08/2020 06:15:21 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "            )\u001b[0m\n",
      "\u001b[34m03/08/2020 06:15:24 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "          )\u001b[0m\n",
      "\u001b[34m03/08/2020 06:15:24 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "          (intermediate): BertIntermediate(\u001b[0m\n",
      "\u001b[34m03/08/2020 06:15:29 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=False, do_train=True, eval_all_checkpoints=False, eval_data_file=None, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=0.0001, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=True, mlm_probability=0.15, model_dir='/opt/ml/model', model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=2.0, overwrite_cache=False, overwrite_model_dir=False, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=500, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train='/opt/ml/input/data/train', warmup_steps=0, weight_decay=0.0)\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\u001b[0m\n",
      "\u001b[34m03/08/2020 06:15:29 - INFO - __main__ -   Creating features from dataset file at /opt/ml/input/data/train\n",
      "          )\u001b[0m\n",
      "\u001b[34m03/08/2020 06:15:30 - INFO - __main__ -   Saving features into cached file /opt/ml/input/data/train/bert_cached_lm_510_testfile.txt\n",
      "          (output): BertOutput(\u001b[0m\n",
      "\u001b[34m03/08/2020 06:15:30 - INFO - __main__ -   ***** Running training *****\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[34m03/08/2020 06:15:30 - INFO - __main__ -     Num examples = 110\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\u001b[0m\n",
      "\u001b[34m03/08/2020 06:15:30 - INFO - __main__ -     Num Epochs = 2\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\u001b[0m\n",
      "\u001b[34m03/08/2020 06:15:30 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
      "          )\u001b[0m\n",
      "\u001b[34m03/08/2020 06:15:30 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "        )\u001b[0m\n",
      "\u001b[34m03/08/2020 06:15:30 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "        (3): BertLayer(\u001b[0m\n",
      "\u001b[34m03/08/2020 06:15:30 - INFO - __main__ -     Total optimization steps = 56\n",
      "          (attention): BertAttention(\u001b[0m\n",
      "\u001b[34m#015Epoch:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "            (self): BertSelfAttention(\u001b[0m\n",
      "\u001b[34m#015Iteration:   0%|          | 0/28 [00:00<?, ?it/s]#033[A\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[34m#015Iteration:   4%|▎         | 1/28 [00:01<00:31,  1.17s/it]#033[A\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[34m#015Iteration:   7%|▋         | 2/28 [00:01<00:23,  1.13it/s]#033[A\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[35m03/08/2020 06:15:32 - INFO - transformers.tokenization_utils -   Model name '/opt/ml/model' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '/opt/ml/model' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "            (self): BertSelfAttention(\u001b[0m\n",
      "\u001b[35m03/08/2020 06:15:32 - INFO - transformers.tokenization_utils -   Didn't find file /opt/ml/model/added_tokens.json. We won't load it.\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[35m03/08/2020 06:15:32 - INFO - transformers.tokenization_utils -   loading file /opt/ml/model/vocab.txt\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[35m03/08/2020 06:15:32 - INFO - transformers.tokenization_utils -   loading file None\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[35m03/08/2020 06:15:32 - INFO - transformers.tokenization_utils -   loading file /opt/ml/model/special_tokens_map.json\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\u001b[0m\n",
      "\u001b[35m03/08/2020 06:15:32 - INFO - transformers.tokenization_utils -   loading file /opt/ml/model/tokenizer_config.json\n",
      "            )\u001b[0m\n",
      "\u001b[34m#015Iteration:  11%|█         | 3/28 [00:01<00:17,  1.45it/s]#033[A\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\u001b[0m\n",
      "\u001b[34m#015Iteration:  14%|█▍        | 4/28 [00:01<00:13,  1.82it/s]#033[A\n",
      "            )\u001b[0m\n",
      "\u001b[34m#015Iteration:  18%|█▊        | 5/28 [00:02<00:10,  2.21it/s]#033[A\n",
      "            (output): BertSelfOutput(\u001b[0m\n",
      "\u001b[34m#015Iteration:  21%|██▏       | 6/28 [00:02<00:08,  2.61it/s]#033[A\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[34m#015Iteration:  25%|██▌       | 7/28 [00:02<00:07,  2.97it/s]#033[A\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\u001b[0m\n",
      "\u001b[35m            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (cls): BertOnlyMLMHead(\n",
      "    (predictions): BertLMPredictionHead(\n",
      "      (transform): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[35m)\u001b[0m\n",
      "\u001b[35margs.train: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[35mfile_path: /opt/ml/input/data/train/testfile.txt\u001b[0m\n",
      "\u001b[35mLoaded and Cached examples\u001b[0m\n",
      "\u001b[35mIterating through training data\u001b[0m\n",
      "\u001b[35mBatch number: 0\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  8036,   103,  ..., 12365,  2607,   102],\n",
      "        [  101, 27954,  1012,  ...,  7632,   103,   102],\n",
      "        [  101,  1035,  1035,  ...,  2155,  3392,   102],\n",
      "        [  101,  1996,   103,  ...,  8370, 18704,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, 1997,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, 2595, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, 4125,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:15.390 algo-2:52 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:15.390 algo-2:52 INFO hook.py:152] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:15.390 algo-2:52 INFO hook.py:197] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:15.405 algo-2:52 INFO hook.py:326] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.370 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.370 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.370 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.371 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.0.attention NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.378 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.378 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.378 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.381 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.381 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.381 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.382 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.1.attention NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.384 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.384 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.384 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.386 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.386 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.386 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.387 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.2.attention NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.389 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.389 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.389 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.392 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.392 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.392 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.392 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.3.attention NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.394 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.395 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[35m2020-03-08 06:15:33,108 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.395 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.397 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.397 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.397 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.398 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.4.attention NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.400 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.400 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.400 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.403 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.403 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.403 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.403 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.5.attention NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.405 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.406 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.406 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.408 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.408 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.408 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.409 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.6.attention NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.411 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.411 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.411 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.414 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.414 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.414 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.414 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.7.attention NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.416 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.416 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.416 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.419 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.419 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.419 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.420 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.8.attention NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.422 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.422 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.422 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.425 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.425 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.425 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.425 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.9.attention NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.427 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.427 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.427 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.430 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.430 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.430 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.431 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.10.attention NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.433 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.433 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.433 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.436 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.436 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.436 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.437 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.11.attention NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.439 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.439 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:16.439 algo-2:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(3.3193, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 1\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  1012,  1045,  ...,  1037,   103,   102],\n",
      "        [  101,  2150,  1037,  ...,  3066,  2007,   102],\n",
      "        [  101,  7662,  1037,  ...,  2581,  2683,   102],\n",
      "        [  101,  1996, 18293,  ...,   103, 18333,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[ -100,  -100,  -100,  ...,  -100, 29500,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  2007,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  2024,  -100,  -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(3.4851, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 2\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  1996,   103,  ...,  2149,  1517,   102],\n",
      "        [  101,  2942,  2916,  ...,  1523, 14092,   102],\n",
      "        [  101,  2624,   103,  ...,  7795,  4968,   102],\n",
      "        [  101, 14570,  2075,  ...,  2111,  2000,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[ -100,  -100, 13950,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  2624,  5348,  ...,  -100,  2013,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(3.1198, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 3\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101, 11293,  7550,  ...,  1025,  2684,   102],\n",
      "        [  101,   103,  2000,  ...,  2003,  2006,   102],\n",
      "        [  101,  3571,  1012,  ...,   103,  2033,   102],\n",
      "        [  101,  1996,  2536,  ...,   103,  1010,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, 3266, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., 2000, -100, -100],\n",
      "        [-100, -100, -100,  ..., 1015, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.8702, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 4\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101, 15983,  2005,  ...,  2243, 13214,   102],\n",
      "        [  101,  1998,  2019,  ...,  2385,  1012,   102],\n",
      "        [  101,  2038,  3653,  ...,  6905,  1010,   102],\n",
      "        [  101,  8144,  2089,  ...,  6687,  9021,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[ -100,  4455,  -100,  ...,  -100, 12155,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.8757, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 5\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101, 11378,  2996,  ...,  2188,  2130,   102],\n",
      "        [  101,  2199,  2670,  ...,  2037,  4752,   102],\n",
      "        [  101,  1998,   103,  ...,  3305,  1996,   102],\n",
      "        [  101,  1996,  9647,  ...,   103,  1012,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, 1996,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., 3969, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(3.1159, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 6\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[ 101, 3544, 1010,  ..., 1010, 2018,  102],\n",
      "        [ 101, 1996, 2158,  ..., 1010,  103,  102],\n",
      "        [ 101, 2094, 9759,  ..., 2029, 2003,  102],\n",
      "        [ 101, 1998, 2256,  ..., 1012, 1045,  102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, 1010,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, 2009, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.5341, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 7\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  6666,   103,  ...,  1996,  2307,   102],\n",
      "        [  101,  2044,   103,  ..., 20242,  2008,   102],\n",
      "        [  101, 11591,  4094,  ...,  1010, 12170,   102],\n",
      "        [  101,  1012,   103,  ...,  7988,  1998,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, 1997,  ..., -100, -100, -100],\n",
      "        [-100, -100, 1010,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, 2009,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(3.0337, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 8\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  2003,  1037,  ...,  1012,  2017,   102],\n",
      "        [  101,  1996,  9556,  ...,  2050,  2004,   102],\n",
      "        [  101, 26302,  2618,  ...,  2005,  1996,   102],\n",
      "        [  101,  1999, 15842,  ...,  3454,   103,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, 1999, -100,  ..., -100, 1025, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.5598, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 9\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  2578,  1012,  ..., 29483,  1012,   102],\n",
      "        [  101,  1012, 12216,  ...,  1033,  2030,   102],\n",
      "        [  101,  1997,  1996,  ..., 28844,  2098,   102],\n",
      "        [  101,  3370,  1010,  ...,  1996, 11932,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, -100,  ..., 2193, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.9207, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 10\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  2007,  8991,  ...,  2384,  1010,   102],\n",
      "        [  101,   103,  2003,  ..., 18921,  5092,   102],\n",
      "        [  101,  2009,   103,  ...,  2649,   103,   102],\n",
      "        [  101,  1996,   103,  ...,  2664,  1997,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  2009,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100, 19676,  ...,  -100,  1037,  -100],\n",
      "        [ -100,  -100,  4664,  ...,  -100,  -100,  -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.9829, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 11\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  1996, 16409,  ...,  2149,   103,   102],\n",
      "        [  101,  3695,  1010,  ...,  3695,  3449,   102],\n",
      "        [  101,  1997,  9185,  ...,  9647,  1012,   102],\n",
      "        [  101,  2111,  3404,  ...,  9450,  1012,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, -100,  ..., -100, 2000, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, 1012, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(3.2377, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 12\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,   103,  1007,  ..., 21924,  1998,   102],\n",
      "        [  101,  2445,  6121,  ...,  3050,   103,   102],\n",
      "        [  101,   103,  2137,  ...,   103,  6342,   102],\n",
      "        [  101,  1061, 11477,  ...,  9092,  3406,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[ -100,  2324,  -100,  ...,  3343,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  8273,  -100],\n",
      "        [ -100,  2005,  -100,  ..., 13316,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.9753, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 13\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101, 25088,  2334,  ...,  1010,  3847,   102],\n",
      "        [  101, 27346,  1997,  ...,  2059, 28960,   102],\n",
      "        [  101,  1996,  4512,  ...,  1037,  5257,   102],\n",
      "        [  101,   103, 26097,  ...,  1524,  2057,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, 3867, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, 3419, 4168,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.9549, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 14\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  2174,  1010,  ...,  2735,   103,   102],\n",
      "        [  101,  2010, 10638,  ...,  4208,  2006,   102],\n",
      "        [  101,  9144,  1010,  ...,   103, 13970,   102],\n",
      "        [  101,  3151,   103,  ...,   103,  2704,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, -100,  ..., -100, 2000, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., 1996, -100, -100],\n",
      "        [-100, -100, 2326,  ..., 2555, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(3.0881, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 15\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  1997, 27113,  ...,  2771,  2704,   102],\n",
      "        [  101,   103,  1996,  ...,  5951,  5846,   102],\n",
      "        [  101, 18752, 14045,  ...,  2025,  2146,   102],\n",
      "        [  101,  2597,  2000,  ...,  1010,  2009,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[ -100,  -100, 27113,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  2293,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(3.1707, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 16\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  2442,  2079,  ...,  1517,   103,   102],\n",
      "        [  101,   103,  3283,  ...,  2015, 29542,   102],\n",
      "        [  101,   103,  5368,  ...,  2455,  1025,   102],\n",
      "        [  101,  7316,  1012,  ..., 11493, 14192,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, -100,  ..., -100, 4319, -100],\n",
      "        [-100, 4372, -100,  ..., -100, -100, -100],\n",
      "        [-100, 4207, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, 1012,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.6861, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 17\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  3842,  2008,  ..., 12087,   103,   102],\n",
      "        [  101,  2210,  4495,  ...,  2721, 10446,   102],\n",
      "        [  101,  1523,  9038,  ...,  8320,  1999,   102],\n",
      "        [  101,  3288,  2017,  ...,  1037, 24185,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, -100,  ..., -100, 4447, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.9124, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 18\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  3008,  1998,  ...,   103,  1998,   102],\n",
      "        [  101,  3740,   103,  ...,   103,  3438,   102],\n",
      "        [  101,  1007,  1007,  ..., 24431,  1998,   102],\n",
      "        [  101,  1010,  2043,  ...,  1010,  2014,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, -100,  ..., 2155, -100, -100],\n",
      "        [-100, -100, 6687,  ..., 2005, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.5258, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 19\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[ 101, 2000, 2360,  ..., 1037,  103,  102],\n",
      "        [ 101, 3837, 1524,  ..., 4963, 1998,  102],\n",
      "        [ 101, 2003, 1523,  ..., 5635, 1998,  102],\n",
      "        [ 101, 3425, 2003,  ..., 1035, 1035,  102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, -100,  ..., -100, 2051, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, 2003, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(3.0243, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 20\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  2004,  1045,  ...,  1010,  2643,   102],\n",
      "        [  101,  2047,  2259,  ...,  1996,  2647,   102],\n",
      "        [  101,  2025,  2205,  ...,  2052,  1037,   102],\n",
      "        [  101, 19575,  2015,  ..., 14312,  1996,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.9322, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 21\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  1996,  2899,  ...,  8083,  1997,   102],\n",
      "        [  101, 13724,  4869,  ...,  2167, 16596,   102],\n",
      "        [  101, 11295,  2964,  ...,  1997,  2010,   102],\n",
      "        [  101, 10479,  1997,  ...,  1517,  1999,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, 1010, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(3.0763, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 22\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  6526,   103,  ...,  1517,  2416,   102],\n",
      "        [  101, 17063,  2139,  ...,  2401, 24536,   102],\n",
      "        [  101, 10506,  1010,  ...,  2058,  2040,   102],\n",
      "        [  101,  2658,  2147,  ...,  2515,  2025,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, 2000,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.7884, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 23\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[ 101, 2003,  103,  ..., 2055,  103,  102],\n",
      "        [ 101, 1521, 1055,  ..., 7795, 1010,  102],\n",
      "        [ 101, 4667, 1010,  ..., 4034, 1999,  102],\n",
      "        [ 101, 2477, 2089,  ..., 1010, 6772,  102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, 7453,  ..., -100, 1996, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.1241, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 24\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[ 101, 5738, 1996,  ..., 1029, 1524,  102],\n",
      "        [ 101, 1010, 9686,  ..., 3449, 2704,  102],\n",
      "        [ 101, 1999, 2561,  ..., 1011, 2729,  102],\n",
      "        [ 101, 2272, 2046,  ..., 3310, 2013,  102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.9495, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 25\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101, 17903,  1996,  ...,   103,  2007,   102],\n",
      "        [  101,  2064,  1521,  ...,  1010,  2004,   102],\n",
      "        [  101,  2028,  1997,  ...,  2000, 29525,   102],\n",
      "        [  101,   103,  2045,  ...,  4618,  2036,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, -100,  ..., 8398, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, 1523, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(3.3001, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 26\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[ 101,  103, 1996,  ..., 1998, 1996,  102],\n",
      "        [ 101, 2733, 1997,  ..., 2004, 2256,  102],\n",
      "        [ 101, 1998, 2191,  ..., 2389, 3325,  102],\n",
      "        [ 101, 3695,  103,  ..., 2712,  999,  102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, 2425, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, 9530,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.6808, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 27\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101, 26053,  2137,  ...,  2145,  1010,   102],\n",
      "        [  101,  2040,  2003,  ...,  2163,  1012,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(3.0144, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mIterating through training data\u001b[0m\n",
      "\u001b[35mBatch number: 0\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  1523,  2045,  ...,  4618,  2036,   102],\n",
      "        [  101,  3571,  1012,  ...,  2000,  2033,   102],\n",
      "        [  101,  1012, 21210,  ...,  7988,  1998,   102],\n",
      "        [  101,  1997,  1996,  ..., 28844,  2098,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, 2009,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.6321, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 1\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[ 101, 2174,  103,  ..., 2735, 2000,  102],\n",
      "        [ 101, 2624, 5348,  ...,  103, 2013,  102],\n",
      "        [ 101, 7662, 1037,  ..., 2581, 2683,  102],\n",
      "        [ 101, 5738, 1996,  ..., 1029, 1524,  102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, 1010,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., 7795, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.3265, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 2\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,   103,  1997,  ...,  2000, 15850,   102],\n",
      "        [  101,  2064,  1521,  ...,  1010,   103,   102],\n",
      "        [  101,   103,  1010,  ...,  4034,  1999,   102],\n",
      "        [  101,   103,  1010,  ...,  1010,   103,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[ -100,  2028,  -100,  ...,  -100, 29525,  -100],\n",
      "        [ -100,  2064,  -100,  ...,  -100,  2004,  -100],\n",
      "        [ -100,  4667,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  3544,  -100,  ...,  -100,  2018,  -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.4529, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 3\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  1996,  4664,  ...,  2664,  1997,   102],\n",
      "        [  101,  3695,  9530,  ...,  2712,   999,   102],\n",
      "        [  101,   103, 10638,  ...,  4208,  2006,   102],\n",
      "        [  101, 26302,  2618,  ...,  2005,  1996,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, 2010, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.4338, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 4\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  1996, 16409,  ...,  2149,  2000,   102],\n",
      "        [  101,   103,  2008,  ..., 12087,  4447,   102],\n",
      "        [  101,  2047,  2259,  ...,  1996,  2647,   102],\n",
      "        [  101,  2445,  6121,  ...,  3050,  8273,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, 3842, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.9144, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 5\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  1996, 13950,  ...,  2149,  1517,   102],\n",
      "        [  101, 17063,  2139,  ...,  2401,   103,   102],\n",
      "        [  101, 27954,  1012,  ...,  7632,  2595,   102],\n",
      "        [  101,  1521,  1055,  ...,  7795,  1010,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100, 24536,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  7795,  -100,  -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.4352, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 6\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  2040,  2003,  ...,  2163,  1012,   102],\n",
      "        [  101,  3288,  2017,  ...,  1037, 24185,   102],\n",
      "        [  101, 25088,  2334,  ...,  1010,   103,   102],\n",
      "        [  101,   103,   103,  ...,  2555,  2704,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, 3847, -100],\n",
      "        [-100, 3151, 2326,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.8088, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 7\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  2007,  8991,  ...,  2384,  1010,   102],\n",
      "        [  101,  1996,  2536,  ...,  1015,  1010,   102],\n",
      "        [  101,  4207,  5368,  ...,  2455,  1025,   102],\n",
      "        [  101, 18752,  2911,  ...,   103,   103,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100, 14045,  ...,  2025,  2146,  -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.6439, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 8\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,   103,   103,  ...,  1524,  2057,   102],\n",
      "        [  101,  3867,   103,  ...,  2059, 28960,   102],\n",
      "        [  101, 11293,  7550,  ...,   103,  2684,   102],\n",
      "        [  101, 11295,   103,  ...,  1997,  2010,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, 3419, 4168,  ..., -100, -100, -100],\n",
      "        [-100, -100, 1997,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., 1025, -100, -100],\n",
      "        [-100, -100, 2964,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.2768, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 9\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  2442,  2079,  ...,  1517,  4319,   102],\n",
      "        [  101,  2009,   103,  ..., 18921,  5092,   102],\n",
      "        [  101,  3740,  6687,  ...,   103,  3438,   102],\n",
      "        [  101,  1010,   103,  ...,   103,  2014,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, 2003,  ..., -100, 5092, -100],\n",
      "        [-100, -100, -100,  ..., 2005, -100, -100],\n",
      "        [-100, -100, 2043,  ..., 1010, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.7074, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 10\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  1007,  1007,  ..., 24431,  1998,   102],\n",
      "        [  101,  1998,  2256,  ...,  1012,  1045,   102],\n",
      "        [  101,  2733,   103,  ...,  2004,  2256,   102],\n",
      "        [  101,  2272,  2046,  ...,  3310,  2013,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, 1997,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.4888, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 11\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  9144,  1010,  ...,  1996, 13970,   102],\n",
      "        [  101,  2003,  1523,  ...,  5635,  1998,   102],\n",
      "        [  101,  1012,  1045,  ...,  1037, 29500,   102],\n",
      "        [  101,  8036,  1997,  ..., 12365,  2607,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.4339, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 12\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,   103,  1035,  ...,   103,  3392,   102],\n",
      "        [  101,  2597,  2000,  ...,  1010,   103,   102],\n",
      "        [  101,  1996,  9647,  ...,  3969,  1012,   102],\n",
      "        [  101,   103,  4125,  ...,  8370, 18704,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, 1035, -100,  ..., 2155, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, 2009, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, 1996, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.9952, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 13\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,   103,  2996,  ...,  2188,  2130,   102],\n",
      "        [  101,  1012, 12216,  ...,  1033,  2030,   102],\n",
      "        [  101,  6666,  1997,  ...,  1996,  2307,   102],\n",
      "        [  101,  2005,  2137,  ..., 13316,  6342,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[ -100, 11378,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ..., 13316,  -100,  -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.2592, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 14\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  1996,  4512,  ...,  1037, 14465,   102],\n",
      "        [  101,  2111,  3404,  ...,  9450,  1012,   102],\n",
      "        [  101,  4372,  3283,  ...,  2015, 29542,   102],\n",
      "        [  101,   103,  2003,  ...,  1035,  1035,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, -100,  ..., -100, 5257, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, 3425, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.3861, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 15\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  2942,   103,  ...,   103, 14092,   102],\n",
      "        [  101,   103,  1996,  ...,  8398,  2007,   102],\n",
      "        [  101,  2578,  1012,  ...,  2193,  1012,   102],\n",
      "        [  101,  1010,  4869,  ...,   103,   103,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[ -100,  -100,  2916,  ...,  1523,  -100,  -100],\n",
      "        [ -100, 17903,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  2167, 16596,  -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.5381, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 16\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101, 10506,  1010,  ...,  2058,  2040,   102],\n",
      "        [  101,  1061, 11477,  ...,   103,  3406,   102],\n",
      "        [  101,  2210,   103,  ...,   103, 10446,   102],\n",
      "        [  101,  1999, 15842,  ...,  3454,  1025,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., 9092, -100, -100],\n",
      "        [-100, -100, 4495,  ..., 2721, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.6361, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 17\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101, 19575,   103,  ...,   103,  1996,   102],\n",
      "        [  101, 14570,   103,  ...,   103,  2000,   102],\n",
      "        [  101,  2025,  2205,  ...,  2052,  1037,   102],\n",
      "        [  101,  2324,  1007,  ...,  3343,  1998,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[ -100,  -100,  2015,  ..., 14312,  -100,  -100],\n",
      "        [ -100,  -100,  2075,  ...,  2111,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  3343,  -100,  -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.6869, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 18\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[ 101, 1996,  103,  ..., 8083, 1997,  102],\n",
      "        [ 101, 1996, 2158,  ..., 1010, 2009,  102],\n",
      "        [ 101, 2003,  103,  ..., 1012, 2017,  102],\n",
      "        [ 101,  103, 1996,  ..., 1998,  103,  102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, 2899,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, 1037,  ..., -100, -100, -100],\n",
      "        [-100, 2425, -100,  ..., -100, 1996, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.4382, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 19\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  2000,  2360,  ...,  1037,  2051,   102],\n",
      "        [  101,   103,  1997,  ...,  1517,  1999,   102],\n",
      "        [  101,  1998,  2191,  ...,  2389,  3325,   102],\n",
      "        [  101,  1997, 27113,  ...,  2771,  2704,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100, 10479,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.3073, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 20\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  3008,  1998,  ...,   103,  1998,   102],\n",
      "        [  101, 26053,  2137,  ...,  2145,  1010,   102],\n",
      "        [  101,  2293,  1996,  ...,  5951,   103,   102],\n",
      "        [  101,  2199,  2670,  ...,  2037,  4752,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, -100,  ..., 2155, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, 5846, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.4075, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 21\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  1523,  9038,  ...,  8320,  1999,   102],\n",
      "        [  101,  1997,  9185,  ...,  9647,  1012,   102],\n",
      "        [  101,  1999,  2561,  ...,  1011,   103,   102],\n",
      "        [  101,  2658,  2147,  ...,  2515, 26330,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, 2729, -100],\n",
      "        [-100, -100, -100,  ..., -100, 2025, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.3446, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 22\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  4455,  2005,  ...,  2243, 12155,   102],\n",
      "        [  101,  6526,  2000,  ...,   103,  2416,   102],\n",
      "        [  101,  2044,  1010,  ..., 20242,  2008,   102],\n",
      "        [  101,  1996, 18293,  ...,  2024,   103,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  1517,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100, 18333,  -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.5275, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 23\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  1998,  2019,  ...,  2385,  1012,   102],\n",
      "        [  101,  2004,  1045,  ...,  1010,  2643,   102],\n",
      "        [  101,  7316,  1012,  ..., 11493, 14192,   102],\n",
      "        [  101, 11591,  4094,  ...,  1010, 12170,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.3598, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 24\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[ 101, 8144, 2089,  ..., 6687, 9021,  102],\n",
      "        [ 101, 2094, 9759,  ..., 2029, 2003,  102],\n",
      "        [ 101, 2038, 3653,  ..., 6905, 1010,  102],\n",
      "        [ 101, 1010, 9686,  ..., 3449, 2704,  102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.4646, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 25\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  2003,   103,  ...,  2055,  1996,   102],\n",
      "        [  101,   103,  1037,  ...,   103,  2007,   102],\n",
      "        [  101, 20627,  1010,  ...,  3695,  3449,   102],\n",
      "        [  101,  3837,   103,  ...,  4963,  1998,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, 7453,  ..., -100, -100, -100],\n",
      "        [-100, 2150, -100,  ..., 3066, -100, -100],\n",
      "        [-100, 3695, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, 1524,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.6093, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 26\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  2477,  2089,  ...,  1010,  6772,   102],\n",
      "        [  101,  3370,  1010,  ...,  1996, 11932,   102],\n",
      "        [  101,  2009, 19676,  ...,  2649,  1037,   102],\n",
      "        [  101,  1998,  1996,  ...,  3305,  1996,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.7404, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mBatch number: 27\u001b[0m\n",
      "\u001b[35msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[35mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[35mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[35margs.device: cuda\u001b[0m\n",
      "\u001b[35mTraining mode\u001b[0m\n",
      "\u001b[35margs.mlm True\u001b[0m\n",
      "\u001b[35minputs tensor([[  101,  3266,  2000,  ...,  2003,  2006,   102],\n",
      "        [  101,  1996,  9556,  ...,  2050, 19835,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, 2004, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[35mGet outputs\u001b[0m\n",
      "\u001b[35mloss: tensor(2.1958, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[35mSave the model\u001b[0m\n",
      "\u001b[35m[2020-03-08 06:15:32.711 algo-2:52 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m#015Iteration:  29%|██▊       | 8/28 [00:02<00:06,  3.30it/s]#033[A\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\u001b[0m\n",
      "\u001b[34m#015Iteration:  32%|███▏      | 9/28 [00:02<00:05,  3.58it/s]#033[A\n",
      "            )\u001b[0m\n",
      "\u001b[34m#015Iteration:  36%|███▌      | 10/28 [00:03<00:04,  3.81it/s]#033[A\n",
      "          )\u001b[0m\n",
      "\u001b[34m#015Iteration:  39%|███▉      | 11/28 [00:03<00:04,  3.98it/s]#033[A\n",
      "          (intermediate): BertIntermediate(\u001b[0m\n",
      "\u001b[34m#015Iteration:  43%|████▎     | 12/28 [00:03<00:03,  4.12it/s]#033[A\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\u001b[0m\n",
      "\u001b[34m#015Iteration:  46%|████▋     | 13/28 [00:03<00:03,  4.22it/s]#033[A\n",
      "          )\u001b[0m\n",
      "\u001b[34m#015Iteration:  50%|█████     | 14/28 [00:04<00:03,  4.29it/s]#033[A\n",
      "          (output): BertOutput(\u001b[0m\n",
      "\u001b[34m#015Iteration:  54%|█████▎    | 15/28 [00:04<00:02,  4.34it/s]#033[A\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[34m#015Iteration:  57%|█████▋    | 16/28 [00:04<00:02,  4.37it/s]#033[A\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\u001b[0m\n",
      "\u001b[34m#015Iteration:  61%|██████    | 17/28 [00:04<00:02,  4.39it/s]#033[A\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\u001b[0m\n",
      "\u001b[34m#015Iteration:  64%|██████▍   | 18/28 [00:04<00:02,  4.41it/s]#033[A\n",
      "          )\u001b[0m\n",
      "\u001b[34m#015Iteration:  68%|██████▊   | 19/28 [00:05<00:02,  4.43it/s]#033[A\n",
      "        )\u001b[0m\n",
      "\u001b[34m#015Iteration:  71%|███████▏  | 20/28 [00:05<00:01,  4.45it/s]#033[A\n",
      "        (4): BertLayer(\u001b[0m\n",
      "\u001b[34m#015Iteration:  75%|███████▌  | 21/28 [00:05<00:01,  4.45it/s]#033[A\n",
      "          (attention): BertAttention(\u001b[0m\n",
      "\u001b[34m#015Iteration:  79%|███████▊  | 22/28 [00:05<00:01,  4.46it/s]#033[A\n",
      "            (self): BertSelfAttention(\u001b[0m\n",
      "\u001b[34m#015Iteration:  82%|████████▏ | 23/28 [00:06<00:01,  4.45it/s]#033[A\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[34m#015Iteration:  86%|████████▌ | 24/28 [00:06<00:00,  4.45it/s]#033[A\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[34m#015Iteration:  89%|████████▉ | 25/28 [00:06<00:00,  4.44it/s]#033[A\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[34m#015Iteration:  93%|█████████▎| 26/28 [00:06<00:00,  4.44it/s]#033[A\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\u001b[0m\n",
      "\u001b[34m#015Iteration:  96%|█████████▋| 27/28 [00:07<00:00,  4.44it/s]#033[A\n",
      "            )\u001b[0m\n",
      "\u001b[34m#015Iteration: 100%|██████████| 28/28 [00:07<00:00,  4.96it/s]#033[A#015Iteration: 100%|██████████| 28/28 [00:07<00:00,  3.91it/s]\n",
      "            (output): BertSelfOutput(\u001b[0m\n",
      "\u001b[34m#015Epoch:  50%|█████     | 1/2 [00:07<00:07,  7.16s/it]\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[34m#015Iteration:   0%|          | 0/28 [00:00<?, ?it/s]#033[A\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\u001b[0m\n",
      "\u001b[34m#015Iteration:   4%|▎         | 1/28 [00:00<00:06,  4.42it/s]#033[A\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\u001b[0m\n",
      "\u001b[34m#015Iteration:   7%|▋         | 2/28 [00:00<00:05,  4.42it/s]#033[A\n",
      "            )\u001b[0m\n",
      "\u001b[34m#015Iteration:  11%|█         | 3/28 [00:00<00:05,  4.43it/s]#033[A\n",
      "          )\u001b[0m\n",
      "\u001b[34m#015Iteration:  14%|█▍        | 4/28 [00:00<00:05,  4.43it/s]#033[A\n",
      "          (intermediate): BertIntermediate(\u001b[0m\n",
      "\u001b[34m#015Iteration:  18%|█▊        | 5/28 [00:01<00:05,  4.43it/s]#033[A\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\u001b[0m\n",
      "\u001b[34m#015Iteration:  21%|██▏       | 6/28 [00:01<00:04,  4.44it/s]#033[A\n",
      "          )\u001b[0m\n",
      "\n",
      "2020-03-08 06:15:50 Uploading - Uploading generated training model\u001b[34m#015Iteration:  25%|██▌       | 7/28 [00:01<00:04,  4.44it/s]#033[A\n",
      "          (output): BertOutput(\u001b[0m\n",
      "\u001b[34m#015Iteration:  29%|██▊       | 8/28 [00:01<00:04,  4.44it/s]#033[A\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[34m#015Iteration:  32%|███▏      | 9/28 [00:02<00:04,  4.44it/s]#033[A\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\u001b[0m\n",
      "\u001b[34m#015Iteration:  36%|███▌      | 10/28 [00:02<00:04,  4.44it/s]#033[A\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\u001b[0m\n",
      "\u001b[34m#015Iteration:  39%|███▉      | 11/28 [00:02<00:03,  4.45it/s]#033[A\n",
      "          )\u001b[0m\n",
      "\u001b[34m#015Iteration:  43%|████▎     | 12/28 [00:02<00:03,  4.44it/s]#033[A\n",
      "        )\u001b[0m\n",
      "\u001b[34m#015Iteration:  46%|████▋     | 13/28 [00:02<00:03,  4.41it/s]#033[A\n",
      "        (5): BertLayer(\u001b[0m\n",
      "\u001b[34m#015Iteration:  50%|█████     | 14/28 [00:03<00:03,  4.41it/s]#033[A\n",
      "          (attention): BertAttention(\u001b[0m\n",
      "\u001b[34m#015Iteration:  54%|█████▎    | 15/28 [00:03<00:02,  4.42it/s]#033[A\n",
      "            (self): BertSelfAttention(\u001b[0m\n",
      "\u001b[34m#015Iteration:  57%|█████▋    | 16/28 [00:03<00:02,  4.43it/s]#033[A\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[34m#015Iteration:  61%|██████    | 17/28 [00:03<00:02,  4.43it/s]#033[A\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[34m#015Iteration:  64%|██████▍   | 18/28 [00:04<00:02,  4.44it/s]#033[A\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[34m#015Iteration:  68%|██████▊   | 19/28 [00:04<00:02,  4.45it/s]#033[A\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\u001b[0m\n",
      "\u001b[34m#015Iteration:  71%|███████▏  | 20/28 [00:04<00:01,  4.45it/s]#033[A\n",
      "            )\u001b[0m\n",
      "\u001b[34m#015Iteration:  75%|███████▌  | 21/28 [00:04<00:01,  4.44it/s]#033[A\n",
      "            (output): BertSelfOutput(\u001b[0m\n",
      "\u001b[34m#015Iteration:  79%|███████▊  | 22/28 [00:04<00:01,  4.45it/s]#033[A\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[34m#015Iteration:  82%|████████▏ | 23/28 [00:05<00:01,  4.45it/s]#033[A\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\u001b[0m\n",
      "\u001b[34m#015Iteration:  86%|████████▌ | 24/28 [00:05<00:00,  4.45it/s]#033[A\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\u001b[0m\n",
      "\u001b[34m#015Iteration:  89%|████████▉ | 25/28 [00:05<00:00,  4.44it/s]#033[A\n",
      "            )\u001b[0m\n",
      "\u001b[34m#015Iteration:  93%|█████████▎| 26/28 [00:05<00:00,  4.44it/s]#033[A\n",
      "          )\u001b[0m\n",
      "\u001b[34m#015Iteration:  96%|█████████▋| 27/28 [00:06<00:00,  4.45it/s]#033[A\n",
      "          (intermediate): BertIntermediate(\u001b[0m\n",
      "\u001b[34m#015Iteration: 100%|██████████| 28/28 [00:06<00:00,  5.02it/s]#033[A#015Iteration: 100%|██████████| 28/28 [00:06<00:00,  4.50it/s]\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\u001b[0m\n",
      "\u001b[34m#015Epoch: 100%|██████████| 2/2 [00:13<00:00,  6.88s/it]#015Epoch: 100%|██████████| 2/2 [00:13<00:00,  6.69s/it]\n",
      "          )\u001b[0m\n",
      "\u001b[34m03/08/2020 06:15:43 - INFO - __main__ -    global_step = 56, average loss = 2.726954745394843\n",
      "          (output): BertOutput(\u001b[0m\n",
      "\u001b[34m03/08/2020 06:15:43 - INFO - __main__ -   Saving model checkpoint to /opt/ml/model\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[34m03/08/2020 06:15:43 - INFO - transformers.configuration_utils -   Configuration saved in /opt/ml/model/config.json\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\u001b[0m\n",
      "\u001b[34m03/08/2020 06:15:44 - INFO - transformers.modeling_utils -   Model weights saved in /opt/ml/model/pytorch_model.bin\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\u001b[0m\n",
      "\u001b[34m03/08/2020 06:15:44 - INFO - transformers.configuration_utils -   loading configuration file /opt/ml/model/config.json\n",
      "          )\u001b[0m\n",
      "\u001b[34m03/08/2020 06:15:44 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "        )\n",
      "  \"architectures\": [\n",
      "        (6): BertLayer(\n",
      "    \"BertForMaskedLM\"\n",
      "          (attention): BertAttention(\n",
      "  ],\n",
      "            (self): BertSelfAttention(\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  \"bos_token_id\": null,\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  \"do_sample\": false,\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  \"eos_token_ids\": null,\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "  \"finetuning_task\": null,\n",
      "            )\n",
      "  \"hidden_act\": \"gelu\",\n",
      "            (output): BertSelfOutput(\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  \"hidden_size\": 768,\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  \"id2label\": {\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "    \"0\": \"LABEL_0\",\n",
      "            )\n",
      "    \"1\": \"LABEL_1\"\n",
      "          )\n",
      "  },\n",
      "          (intermediate): BertIntermediate(\n",
      "  \"initializer_range\": 0.02,\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  \"intermediate_size\": 3072,\n",
      "          )\n",
      "  \"is_decoder\": false,\n",
      "          (output): BertOutput(\n",
      "  \"label2id\": {\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    \"LABEL_0\": 0,\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    \"LABEL_1\": 1\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "  },\n",
      "          )\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "        )\n",
      "  \"length_penalty\": 1.0,\n",
      "        (7): BertLayer(\n",
      "  \"max_length\": 20,\n",
      "          (attention): BertAttention(\n",
      "  \"max_position_embeddings\": 512,\n",
      "            (self): BertSelfAttention(\n",
      "  \"model_type\": \"bert\",\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  \"num_attention_heads\": 12,\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  \"num_beams\": 1,\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  \"num_hidden_layers\": 12,\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "  \"num_labels\": 2,\n",
      "            )\n",
      "  \"num_return_sequences\": 1,\n",
      "            (output): BertSelfOutput(\n",
      "  \"output_attentions\": false,\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  \"output_hidden_states\": false,\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  \"output_past\": true,\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "  \"pad_token_id\": null,\n",
      "            )\n",
      "  \"pruned_heads\": {},\n",
      "          )\n",
      "  \"repetition_penalty\": 1.0,\n",
      "          (intermediate): BertIntermediate(\n",
      "  \"temperature\": 1.0,\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  \"top_k\": 50,\n",
      "          )\n",
      "  \"top_p\": 1.0,\n",
      "          (output): BertOutput(\n",
      "  \"torchscript\": false,\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  \"type_vocab_size\": 2,\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  \"use_bfloat16\": false,\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "  \"vocab_size\": 30522\n",
      "          )\u001b[0m\n",
      "\u001b[34m}\n",
      "        )\n",
      "\n",
      "        (8): BertLayer(\u001b[0m\n",
      "\u001b[34m03/08/2020 06:15:44 - INFO - transformers.modeling_utils -   loading weights file /opt/ml/model/pytorch_model.bin\n",
      "          (attention): BertAttention(\u001b[0m\n",
      "\u001b[34m03/08/2020 06:15:47 - INFO - transformers.tokenization_utils -   Model name '/opt/ml/model' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '/opt/ml/model' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "            (self): BertSelfAttention(\u001b[0m\n",
      "\u001b[34m03/08/2020 06:15:47 - INFO - transformers.tokenization_utils -   Didn't find file /opt/ml/model/added_tokens.json. We won't load it.\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[34m03/08/2020 06:15:47 - INFO - transformers.tokenization_utils -   loading file /opt/ml/model/vocab.txt\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[34m03/08/2020 06:15:47 - INFO - transformers.tokenization_utils -   loading file None\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\u001b[0m\n",
      "\u001b[34m03/08/2020 06:15:47 - INFO - transformers.tokenization_utils -   loading file /opt/ml/model/special_tokens_map.json\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\u001b[0m\n",
      "\u001b[34m03/08/2020 06:15:47 - INFO - transformers.tokenization_utils -   loading file /opt/ml/model/tokenizer_config.json\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (cls): BertOnlyMLMHead(\n",
      "    (predictions): BertLMPredictionHead(\n",
      "      (transform): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34margs.train: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mfile_path: /opt/ml/input/data/train/testfile.txt\u001b[0m\n",
      "\u001b[34mLoaded and Cached examples\u001b[0m\n",
      "\u001b[34mIterating through training data\u001b[0m\n",
      "\u001b[34mBatch number: 0\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  8036,   103,  ..., 12365,  2607,   102],\n",
      "        [  101, 27954,  1012,  ...,  7632,   103,   102],\n",
      "        [  101,  1035,  1035,  ...,  2155,  3392,   102],\n",
      "        [  101,  1996,   103,  ...,  8370, 18704,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, 1997,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, 2595, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, 4125,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:30.705 algo-1:52 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:30.705 algo-1:52 INFO hook.py:152] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:30.705 algo-1:52 INFO hook.py:197] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:30.723 algo-1:52 INFO hook.py:326] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.466 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.466 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.466 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.467 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.0.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.472 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.472 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.472 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.475 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.475 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.475 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.475 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.1.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.477 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.478 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.478 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.480 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.480 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.480 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.481 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.2.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.483 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.483 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.483 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.485 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.485 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.485 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.486 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.3.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.488 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.488 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.488 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.491 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.491 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.491 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.491 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.4.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.493 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.493 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.493 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.496 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.496 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.496 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.497 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.5.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.499 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.499 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.499 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.501 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.501 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.501 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.502 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.6.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.504 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.504 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.504 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.507 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.507 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.507 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.507 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.7.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.509 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.509 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.509 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.512 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.512 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.512 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.513 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.8.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.515 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.515 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.515 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.517 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.517 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.517 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.518 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.9.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.520 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.520 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.520 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.523 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.523 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.523 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.523 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.10.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.525 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.526 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.526 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.528 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m2020-03-08 06:15:48,106 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.528 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.528 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.529 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.11.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.531 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.531 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:31.531 algo-1:52 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(3.3193, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 1\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  1012,  1045,  ...,  1037,   103,   102],\n",
      "        [  101,  2150,  1037,  ...,  3066,  2007,   102],\n",
      "        [  101,  7662,  1037,  ...,  2581,  2683,   102],\n",
      "        [  101,  1996, 18293,  ...,   103, 18333,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[ -100,  -100,  -100,  ...,  -100, 29500,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  2007,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  2024,  -100,  -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(3.4851, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 2\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  1996,   103,  ...,  2149,  1517,   102],\n",
      "        [  101,  2942,  2916,  ...,  1523, 14092,   102],\n",
      "        [  101,  2624,   103,  ...,  7795,  4968,   102],\n",
      "        [  101, 14570,  2075,  ...,  2111,  2000,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[ -100,  -100, 13950,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  2624,  5348,  ...,  -100,  2013,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(3.1198, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 3\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101, 11293,  7550,  ...,  1025,  2684,   102],\n",
      "        [  101,   103,  2000,  ...,  2003,  2006,   102],\n",
      "        [  101,  3571,  1012,  ...,   103,  2033,   102],\n",
      "        [  101,  1996,  2536,  ...,   103,  1010,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, 3266, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., 2000, -100, -100],\n",
      "        [-100, -100, -100,  ..., 1015, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.8702, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 4\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101, 15983,  2005,  ...,  2243, 13214,   102],\n",
      "        [  101,  1998,  2019,  ...,  2385,  1012,   102],\n",
      "        [  101,  2038,  3653,  ...,  6905,  1010,   102],\n",
      "        [  101,  8144,  2089,  ...,  6687,  9021,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[ -100,  4455,  -100,  ...,  -100, 12155,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.8757, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 5\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101, 11378,  2996,  ...,  2188,  2130,   102],\n",
      "        [  101,  2199,  2670,  ...,  2037,  4752,   102],\n",
      "        [  101,  1998,   103,  ...,  3305,  1996,   102],\n",
      "        [  101,  1996,  9647,  ...,   103,  1012,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, 1996,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., 3969, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(3.1159, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 6\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[ 101, 3544, 1010,  ..., 1010, 2018,  102],\n",
      "        [ 101, 1996, 2158,  ..., 1010,  103,  102],\n",
      "        [ 101, 2094, 9759,  ..., 2029, 2003,  102],\n",
      "        [ 101, 1998, 2256,  ..., 1012, 1045,  102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, 1010,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, 2009, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.5341, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 7\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  6666,   103,  ...,  1996,  2307,   102],\n",
      "        [  101,  2044,   103,  ..., 20242,  2008,   102],\n",
      "        [  101, 11591,  4094,  ...,  1010, 12170,   102],\n",
      "        [  101,  1012,   103,  ...,  7988,  1998,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, 1997,  ..., -100, -100, -100],\n",
      "        [-100, -100, 1010,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, 2009,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(3.0337, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 8\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  2003,  1037,  ...,  1012,  2017,   102],\n",
      "        [  101,  1996,  9556,  ...,  2050,  2004,   102],\n",
      "        [  101, 26302,  2618,  ...,  2005,  1996,   102],\n",
      "        [  101,  1999, 15842,  ...,  3454,   103,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, 1999, -100,  ..., -100, 1025, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.5598, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 9\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  2578,  1012,  ..., 29483,  1012,   102],\n",
      "        [  101,  1012, 12216,  ...,  1033,  2030,   102],\n",
      "        [  101,  1997,  1996,  ..., 28844,  2098,   102],\n",
      "        [  101,  3370,  1010,  ...,  1996, 11932,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, -100,  ..., 2193, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.9207, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 10\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  2007,  8991,  ...,  2384,  1010,   102],\n",
      "        [  101,   103,  2003,  ..., 18921,  5092,   102],\n",
      "        [  101,  2009,   103,  ...,  2649,   103,   102],\n",
      "        [  101,  1996,   103,  ...,  2664,  1997,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  2009,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100, 19676,  ...,  -100,  1037,  -100],\n",
      "        [ -100,  -100,  4664,  ...,  -100,  -100,  -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.9829, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 11\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  1996, 16409,  ...,  2149,   103,   102],\n",
      "        [  101,  3695,  1010,  ...,  3695,  3449,   102],\n",
      "        [  101,  1997,  9185,  ...,  9647,  1012,   102],\n",
      "        [  101,  2111,  3404,  ...,  9450,  1012,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, -100,  ..., -100, 2000, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, 1012, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(3.2377, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 12\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,   103,  1007,  ..., 21924,  1998,   102],\n",
      "        [  101,  2445,  6121,  ...,  3050,   103,   102],\n",
      "        [  101,   103,  2137,  ...,   103,  6342,   102],\n",
      "        [  101,  1061, 11477,  ...,  9092,  3406,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[ -100,  2324,  -100,  ...,  3343,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  8273,  -100],\n",
      "        [ -100,  2005,  -100,  ..., 13316,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.9753, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 13\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101, 25088,  2334,  ...,  1010,  3847,   102],\n",
      "        [  101, 27346,  1997,  ...,  2059, 28960,   102],\n",
      "        [  101,  1996,  4512,  ...,  1037,  5257,   102],\n",
      "        [  101,   103, 26097,  ...,  1524,  2057,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, 3867, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, 3419, 4168,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.9549, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 14\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  2174,  1010,  ...,  2735,   103,   102],\n",
      "        [  101,  2010, 10638,  ...,  4208,  2006,   102],\n",
      "        [  101,  9144,  1010,  ...,   103, 13970,   102],\n",
      "        [  101,  3151,   103,  ...,   103,  2704,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, -100,  ..., -100, 2000, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., 1996, -100, -100],\n",
      "        [-100, -100, 2326,  ..., 2555, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(3.0881, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 15\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  1997, 27113,  ...,  2771,  2704,   102],\n",
      "        [  101,   103,  1996,  ...,  5951,  5846,   102],\n",
      "        [  101, 18752, 14045,  ...,  2025,  2146,   102],\n",
      "        [  101,  2597,  2000,  ...,  1010,  2009,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[ -100,  -100, 27113,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  2293,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(3.1707, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 16\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  2442,  2079,  ...,  1517,   103,   102],\n",
      "        [  101,   103,  3283,  ...,  2015, 29542,   102],\n",
      "        [  101,   103,  5368,  ...,  2455,  1025,   102],\n",
      "        [  101,  7316,  1012,  ..., 11493, 14192,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, -100,  ..., -100, 4319, -100],\n",
      "        [-100, 4372, -100,  ..., -100, -100, -100],\n",
      "        [-100, 4207, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, 1012,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.6861, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 17\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  3842,  2008,  ..., 12087,   103,   102],\n",
      "        [  101,  2210,  4495,  ...,  2721, 10446,   102],\n",
      "        [  101,  1523,  9038,  ...,  8320,  1999,   102],\n",
      "        [  101,  3288,  2017,  ...,  1037, 24185,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, -100,  ..., -100, 4447, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.9124, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 18\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  3008,  1998,  ...,   103,  1998,   102],\n",
      "        [  101,  3740,   103,  ...,   103,  3438,   102],\n",
      "        [  101,  1007,  1007,  ..., 24431,  1998,   102],\n",
      "        [  101,  1010,  2043,  ...,  1010,  2014,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, -100,  ..., 2155, -100, -100],\n",
      "        [-100, -100, 6687,  ..., 2005, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.5258, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 19\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[ 101, 2000, 2360,  ..., 1037,  103,  102],\n",
      "        [ 101, 3837, 1524,  ..., 4963, 1998,  102],\n",
      "        [ 101, 2003, 1523,  ..., 5635, 1998,  102],\n",
      "        [ 101, 3425, 2003,  ..., 1035, 1035,  102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, -100,  ..., -100, 2051, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, 2003, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(3.0243, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 20\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  2004,  1045,  ...,  1010,  2643,   102],\n",
      "        [  101,  2047,  2259,  ...,  1996,  2647,   102],\n",
      "        [  101,  2025,  2205,  ...,  2052,  1037,   102],\n",
      "        [  101, 19575,  2015,  ..., 14312,  1996,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.9322, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 21\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  1996,  2899,  ...,  8083,  1997,   102],\n",
      "        [  101, 13724,  4869,  ...,  2167, 16596,   102],\n",
      "        [  101, 11295,  2964,  ...,  1997,  2010,   102],\n",
      "        [  101, 10479,  1997,  ...,  1517,  1999,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, 1010, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(3.0763, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 22\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  6526,   103,  ...,  1517,  2416,   102],\n",
      "        [  101, 17063,  2139,  ...,  2401, 24536,   102],\n",
      "        [  101, 10506,  1010,  ...,  2058,  2040,   102],\n",
      "        [  101,  2658,  2147,  ...,  2515,  2025,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, 2000,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.7884, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 23\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[ 101, 2003,  103,  ..., 2055,  103,  102],\n",
      "        [ 101, 1521, 1055,  ..., 7795, 1010,  102],\n",
      "        [ 101, 4667, 1010,  ..., 4034, 1999,  102],\n",
      "        [ 101, 2477, 2089,  ..., 1010, 6772,  102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, 7453,  ..., -100, 1996, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.1241, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 24\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[ 101, 5738, 1996,  ..., 1029, 1524,  102],\n",
      "        [ 101, 1010, 9686,  ..., 3449, 2704,  102],\n",
      "        [ 101, 1999, 2561,  ..., 1011, 2729,  102],\n",
      "        [ 101, 2272, 2046,  ..., 3310, 2013,  102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.9495, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 25\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101, 17903,  1996,  ...,   103,  2007,   102],\n",
      "        [  101,  2064,  1521,  ...,  1010,  2004,   102],\n",
      "        [  101,  2028,  1997,  ...,  2000, 29525,   102],\n",
      "        [  101,   103,  2045,  ...,  4618,  2036,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, -100,  ..., 8398, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, 1523, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(3.3001, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 26\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[ 101,  103, 1996,  ..., 1998, 1996,  102],\n",
      "        [ 101, 2733, 1997,  ..., 2004, 2256,  102],\n",
      "        [ 101, 1998, 2191,  ..., 2389, 3325,  102],\n",
      "        [ 101, 3695,  103,  ..., 2712,  999,  102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, 2425, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, 9530,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.6808, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 27\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101, 26053,  2137,  ...,  2145,  1010,   102],\n",
      "        [  101,  2040,  2003,  ...,  2163,  1012,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(3.0144, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mIterating through training data\u001b[0m\n",
      "\u001b[34mBatch number: 0\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  1523,  2045,  ...,  4618,  2036,   102],\n",
      "        [  101,  3571,  1012,  ...,  2000,  2033,   102],\n",
      "        [  101,  1012, 21210,  ...,  7988,  1998,   102],\n",
      "        [  101,  1997,  1996,  ..., 28844,  2098,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, 2009,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.6321, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 1\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[ 101, 2174,  103,  ..., 2735, 2000,  102],\n",
      "        [ 101, 2624, 5348,  ...,  103, 2013,  102],\n",
      "        [ 101, 7662, 1037,  ..., 2581, 2683,  102],\n",
      "        [ 101, 5738, 1996,  ..., 1029, 1524,  102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, 1010,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., 7795, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.3265, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 2\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,   103,  1997,  ...,  2000, 15850,   102],\n",
      "        [  101,  2064,  1521,  ...,  1010,   103,   102],\n",
      "        [  101,   103,  1010,  ...,  4034,  1999,   102],\n",
      "        [  101,   103,  1010,  ...,  1010,   103,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[ -100,  2028,  -100,  ...,  -100, 29525,  -100],\n",
      "        [ -100,  2064,  -100,  ...,  -100,  2004,  -100],\n",
      "        [ -100,  4667,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  3544,  -100,  ...,  -100,  2018,  -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.4529, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 3\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  1996,  4664,  ...,  2664,  1997,   102],\n",
      "        [  101,  3695,  9530,  ...,  2712,   999,   102],\n",
      "        [  101,   103, 10638,  ...,  4208,  2006,   102],\n",
      "        [  101, 26302,  2618,  ...,  2005,  1996,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, 2010, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.4338, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 4\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  1996, 16409,  ...,  2149,  2000,   102],\n",
      "        [  101,   103,  2008,  ..., 12087,  4447,   102],\n",
      "        [  101,  2047,  2259,  ...,  1996,  2647,   102],\n",
      "        [  101,  2445,  6121,  ...,  3050,  8273,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, 3842, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.9144, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 5\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  1996, 13950,  ...,  2149,  1517,   102],\n",
      "        [  101, 17063,  2139,  ...,  2401,   103,   102],\n",
      "        [  101, 27954,  1012,  ...,  7632,  2595,   102],\n",
      "        [  101,  1521,  1055,  ...,  7795,  1010,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100, 24536,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  7795,  -100,  -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.4352, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 6\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  2040,  2003,  ...,  2163,  1012,   102],\n",
      "        [  101,  3288,  2017,  ...,  1037, 24185,   102],\n",
      "        [  101, 25088,  2334,  ...,  1010,   103,   102],\n",
      "        [  101,   103,   103,  ...,  2555,  2704,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, 3847, -100],\n",
      "        [-100, 3151, 2326,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.8088, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 7\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  2007,  8991,  ...,  2384,  1010,   102],\n",
      "        [  101,  1996,  2536,  ...,  1015,  1010,   102],\n",
      "        [  101,  4207,  5368,  ...,  2455,  1025,   102],\n",
      "        [  101, 18752,  2911,  ...,   103,   103,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100, 14045,  ...,  2025,  2146,  -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.6439, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 8\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,   103,   103,  ...,  1524,  2057,   102],\n",
      "        [  101,  3867,   103,  ...,  2059, 28960,   102],\n",
      "        [  101, 11293,  7550,  ...,   103,  2684,   102],\n",
      "        [  101, 11295,   103,  ...,  1997,  2010,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, 3419, 4168,  ..., -100, -100, -100],\n",
      "        [-100, -100, 1997,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., 1025, -100, -100],\n",
      "        [-100, -100, 2964,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.2768, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 9\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  2442,  2079,  ...,  1517,  4319,   102],\n",
      "        [  101,  2009,   103,  ..., 18921,  5092,   102],\n",
      "        [  101,  3740,  6687,  ...,   103,  3438,   102],\n",
      "        [  101,  1010,   103,  ...,   103,  2014,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, 2003,  ..., -100, 5092, -100],\n",
      "        [-100, -100, -100,  ..., 2005, -100, -100],\n",
      "        [-100, -100, 2043,  ..., 1010, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.7074, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 10\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  1007,  1007,  ..., 24431,  1998,   102],\n",
      "        [  101,  1998,  2256,  ...,  1012,  1045,   102],\n",
      "        [  101,  2733,   103,  ...,  2004,  2256,   102],\n",
      "        [  101,  2272,  2046,  ...,  3310,  2013,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, 1997,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.4888, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 11\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  9144,  1010,  ...,  1996, 13970,   102],\n",
      "        [  101,  2003,  1523,  ...,  5635,  1998,   102],\n",
      "        [  101,  1012,  1045,  ...,  1037, 29500,   102],\n",
      "        [  101,  8036,  1997,  ..., 12365,  2607,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.4339, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 12\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,   103,  1035,  ...,   103,  3392,   102],\n",
      "        [  101,  2597,  2000,  ...,  1010,   103,   102],\n",
      "        [  101,  1996,  9647,  ...,  3969,  1012,   102],\n",
      "        [  101,   103,  4125,  ...,  8370, 18704,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, 1035, -100,  ..., 2155, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, 2009, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, 1996, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.9952, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 13\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,   103,  2996,  ...,  2188,  2130,   102],\n",
      "        [  101,  1012, 12216,  ...,  1033,  2030,   102],\n",
      "        [  101,  6666,  1997,  ...,  1996,  2307,   102],\n",
      "        [  101,  2005,  2137,  ..., 13316,  6342,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[ -100, 11378,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ..., 13316,  -100,  -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.2592, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 14\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  1996,  4512,  ...,  1037, 14465,   102],\n",
      "        [  101,  2111,  3404,  ...,  9450,  1012,   102],\n",
      "        [  101,  4372,  3283,  ...,  2015, 29542,   102],\n",
      "        [  101,   103,  2003,  ...,  1035,  1035,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, -100,  ..., -100, 5257, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, 3425, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.3861, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 15\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  2942,   103,  ...,   103, 14092,   102],\n",
      "        [  101,   103,  1996,  ...,  8398,  2007,   102],\n",
      "        [  101,  2578,  1012,  ...,  2193,  1012,   102],\n",
      "        [  101,  1010,  4869,  ...,   103,   103,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[ -100,  -100,  2916,  ...,  1523,  -100,  -100],\n",
      "        [ -100, 17903,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  2167, 16596,  -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.5381, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 16\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101, 10506,  1010,  ...,  2058,  2040,   102],\n",
      "        [  101,  1061, 11477,  ...,   103,  3406,   102],\n",
      "        [  101,  2210,   103,  ...,   103, 10446,   102],\n",
      "        [  101,  1999, 15842,  ...,  3454,  1025,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., 9092, -100, -100],\n",
      "        [-100, -100, 4495,  ..., 2721, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.6361, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 17\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101, 19575,   103,  ...,   103,  1996,   102],\n",
      "        [  101, 14570,   103,  ...,   103,  2000,   102],\n",
      "        [  101,  2025,  2205,  ...,  2052,  1037,   102],\n",
      "        [  101,  2324,  1007,  ...,  3343,  1998,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[ -100,  -100,  2015,  ..., 14312,  -100,  -100],\n",
      "        [ -100,  -100,  2075,  ...,  2111,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  3343,  -100,  -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.6869, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 18\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[ 101, 1996,  103,  ..., 8083, 1997,  102],\n",
      "        [ 101, 1996, 2158,  ..., 1010, 2009,  102],\n",
      "        [ 101, 2003,  103,  ..., 1012, 2017,  102],\n",
      "        [ 101,  103, 1996,  ..., 1998,  103,  102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, 2899,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, 1037,  ..., -100, -100, -100],\n",
      "        [-100, 2425, -100,  ..., -100, 1996, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.4382, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 19\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  2000,  2360,  ...,  1037,  2051,   102],\n",
      "        [  101,   103,  1997,  ...,  1517,  1999,   102],\n",
      "        [  101,  1998,  2191,  ...,  2389,  3325,   102],\n",
      "        [  101,  1997, 27113,  ...,  2771,  2704,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100, 10479,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.3073, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 20\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  3008,  1998,  ...,   103,  1998,   102],\n",
      "        [  101, 26053,  2137,  ...,  2145,  1010,   102],\n",
      "        [  101,  2293,  1996,  ...,  5951,   103,   102],\n",
      "        [  101,  2199,  2670,  ...,  2037,  4752,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, -100,  ..., 2155, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, 5846, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.4075, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 21\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  1523,  9038,  ...,  8320,  1999,   102],\n",
      "        [  101,  1997,  9185,  ...,  9647,  1012,   102],\n",
      "        [  101,  1999,  2561,  ...,  1011,   103,   102],\n",
      "        [  101,  2658,  2147,  ...,  2515, 26330,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, 2729, -100],\n",
      "        [-100, -100, -100,  ..., -100, 2025, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.3446, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 22\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  4455,  2005,  ...,  2243, 12155,   102],\n",
      "        [  101,  6526,  2000,  ...,   103,  2416,   102],\n",
      "        [  101,  2044,  1010,  ..., 20242,  2008,   102],\n",
      "        [  101,  1996, 18293,  ...,  2024,   103,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  1517,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100, 18333,  -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.5275, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 23\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  1998,  2019,  ...,  2385,  1012,   102],\n",
      "        [  101,  2004,  1045,  ...,  1010,  2643,   102],\n",
      "        [  101,  7316,  1012,  ..., 11493, 14192,   102],\n",
      "        [  101, 11591,  4094,  ...,  1010, 12170,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.3598, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 24\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[ 101, 8144, 2089,  ..., 6687, 9021,  102],\n",
      "        [ 101, 2094, 9759,  ..., 2029, 2003,  102],\n",
      "        [ 101, 2038, 3653,  ..., 6905, 1010,  102],\n",
      "        [ 101, 1010, 9686,  ..., 3449, 2704,  102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.4646, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 25\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  2003,   103,  ...,  2055,  1996,   102],\n",
      "        [  101,   103,  1037,  ...,   103,  2007,   102],\n",
      "        [  101, 20627,  1010,  ...,  3695,  3449,   102],\n",
      "        [  101,  3837,   103,  ...,  4963,  1998,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, 7453,  ..., -100, -100, -100],\n",
      "        [-100, 2150, -100,  ..., 3066, -100, -100],\n",
      "        [-100, 3695, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, 1524,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.6093, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 26\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  2477,  2089,  ...,  1010,  6772,   102],\n",
      "        [  101,  3370,  1010,  ...,  1996, 11932,   102],\n",
      "        [  101,  2009, 19676,  ...,  2649,  1037,   102],\n",
      "        [  101,  1998,  1996,  ...,  3305,  1996,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.7404, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mBatch number: 27\u001b[0m\n",
      "\u001b[34msteps_trained_in_current_epoch: 0\u001b[0m\n",
      "\u001b[34mCheckpoint 2 NK\u001b[0m\n",
      "\u001b[34mPytorch version: 1.3.1\u001b[0m\n",
      "\u001b[34margs.device: cuda\u001b[0m\n",
      "\u001b[34mTraining mode\u001b[0m\n",
      "\u001b[34margs.mlm True\u001b[0m\n",
      "\u001b[34minputs tensor([[  101,  3266,  2000,  ...,  2003,  2006,   102],\n",
      "        [  101,  1996,  9556,  ...,  2050, 19835,   102]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mlabels tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, 2004, -100]], device='cuda:0')\u001b[0m\n",
      "\u001b[34mGet outputs\u001b[0m\n",
      "\u001b[34mloss: tensor(2.1958, device='cuda:0', grad_fn=<NllLossBackward>)\u001b[0m\n",
      "\u001b[34mSave the model\u001b[0m\n",
      "\u001b[34m[2020-03-08 06:15:47.722 algo-1:52 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\n",
      "2020-03-08 06:17:30 Completed - Training job completed\n",
      "Training seconds: 536\n",
      "Billable seconds: 536\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'train': 's3://sagemaker-us-west-2-496641494145/L1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
