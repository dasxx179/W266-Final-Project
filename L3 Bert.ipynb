{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based off tutorial https://mccormickml.com/2019/07/22/BERT-fine-tuning/#3-tokenization--input-formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'transformers' already exists and is not an empty directory.\n",
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2.5.1)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (1.12.8)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (4.38.0)\n",
      "Requirement already satisfied: sentencepiece in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.1.85)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.0.38)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (3.0.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2020.2.20)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (1.15.4)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.8 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->transformers) (1.15.8)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2.6)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (1.23)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2019.9.11)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.11.0)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (6.7)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.8->boto3->transformers) (2.7.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.8->boto3->transformers) (0.14)\n",
      "\u001b[31mfastai 1.0.59 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/transformers\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun this\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "import os\n",
    "import sagemaker\n",
    "import math\n",
    "from transformers import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker.pytorch import PyTorch\n",
    "# from transformers import BertTokenizer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from time import gmtime, strftime \n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "from sagemaker.pytorch import PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d150eced1a46c0a8828b754ad47a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_widâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role() # we are using the notebook instance role for training in this example\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket() # you can specify a bucket name here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7 WAYS IMMIGRANTS ENRICH OUR ECONOMY AND SOCIE...</td>\n",
       "      <td>pro-immigration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A report by the National Academies of Sciences...</td>\n",
       "      <td>pro-immigration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Immigrants are also entrepreneurs who create j...</td>\n",
       "      <td>pro-immigration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>According to a study of the bipartisan immigra...</td>\n",
       "      <td>pro-immigration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Given that the average immigrant-owned busines...</td>\n",
       "      <td>pro-immigration</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text         category\n",
       "0  7 WAYS IMMIGRANTS ENRICH OUR ECONOMY AND SOCIE...  pro-immigration\n",
       "1  A report by the National Academies of Sciences...  pro-immigration\n",
       "2  Immigrants are also entrepreneurs who create j...  pro-immigration\n",
       "3  According to a study of the bipartisan immigra...  pro-immigration\n",
       "4  Given that the average immigrant-owned busines...  pro-immigration"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L3_df = pd.read_csv('./data/train/supervised/L3train_nonOneHot.csv')\n",
    "L3_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'pro-immigration':0, 'pro-guns':1, 'anti-immigration':2,\n",
    "       'oppose-medicare':3, 'pro-medicare':4, 'anti-guns':5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "L3_df['category'] = L3_df.category.apply(lambda x: label_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3565</th>\n",
       "      <td>This reflects the decline of hunting as a spor...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3589</th>\n",
       "      <td>(Conversely, a 2014 Quinnipiac survey showed j...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>Even though the evidence of immigration assimi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2376</th>\n",
       "      <td>It Will Make Wait Times Worse Medicare for All...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>(These inconvenient facts never get mentioned,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  category\n",
       "3565  This reflects the decline of hunting as a spor...         5\n",
       "3589  (Conversely, a 2014 Quinnipiac survey showed j...         5\n",
       "388   Even though the evidence of immigration assimi...         0\n",
       "2376  It Will Make Wait Times Worse Medicare for All...         3\n",
       "1136  (These inconvenient facts never get mentioned,...         1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L3_df = shuffle(L3_df,random_state = 100)\n",
    "L3_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>alpha</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3565</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>a</td>\n",
       "      <td>This reflects the decline of hunting as a spor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3589</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>a</td>\n",
       "      <td>(Conversely, a 2014 Quinnipiac survey showed j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>Even though the evidence of immigration assimi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2376</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>It Will Make Wait Times Worse Medicare for All...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>(These inconvenient facts never get mentioned,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  label alpha                                               text\n",
       "3565   0      5     a  This reflects the decline of hunting as a spor...\n",
       "3589   1      5     a  (Conversely, a 2014 Quinnipiac survey showed j...\n",
       "388    2      0     a  Even though the evidence of immigration assimi...\n",
       "2376   3      3     a  It Will Make Wait Times Worse Medicare for All...\n",
       "1136   4      1     a  (These inconvenient facts never get mentioned,..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L3_df = pd.DataFrame({\n",
    "    'id':range(len(L3_df)),\n",
    "    'label':L3_df['category'],\n",
    "    'alpha':['a']*L3_df.shape[0],\n",
    "    'text': L3_df['text']\n",
    "})\n",
    "\n",
    "L3_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3690\n"
     ]
    }
   ],
   "source": [
    "cutoff = math.floor(len(L3_df) * 0.9)\n",
    "print(cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in Training: 3690\n",
      "Rows in Dev: 411\n"
     ]
    }
   ],
   "source": [
    "train_df = L3_df[:cutoff]\n",
    "dev_df = L3_df[cutoff:]\n",
    "print('Rows in Training:', train_df.shape[0])\n",
    "print('Rows in Dev:', dev_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/train/supervised/clean' # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir): # Make sure that the folder exists\n",
    "    print(\"Making folder\")\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(os.path.join(data_dir, 'train.tsv'), sep='\\t', header=True, index=False, columns=train_df.columns)\n",
    "dev_df.to_csv(os.path.join(data_dir, 'dev.tsv'), sep='\\t', header=True, index=False, columns=dev_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3\n",
    "input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=\"L3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-496641494145/L3'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test(candidate):\n",
    "    df = pd.read_csv('./data/test/%s_test.csv' % candidate)\n",
    "    df.columns = ['id','text']\n",
    "    df.to_csv('./data/test/cleaned/%s.tsv' % candidate, sep='\\t', header=True, index=False, columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir = './data/test/cleaned'\n",
    "preprocess_test('Biden')\n",
    "preprocess_test('Sanders')\n",
    "preprocess_test('Buttigieg')\n",
    "preprocess_test('Klobuchar')\n",
    "preprocess_test('Warren')\n",
    "preprocess_test('Yang')\n",
    "\n",
    "# Upload to S3\n",
    "input_data = sagemaker_session.upload_data(path=test_data_dir, bucket=bucket, key_prefix=\"L3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>alpha</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>a</td>\n",
       "      <td>This reflects the decline of hunting as a spor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>a</td>\n",
       "      <td>(Conversely, a 2014 Quinnipiac survey showed j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>Even though the evidence of immigration assimi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>It Will Make Wait Times Worse Medicare for All...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>(These inconvenient facts never get mentioned,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>If only the Holocaust was the only example of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>Related to the welfare argument is the argumen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>a</td>\n",
       "      <td>Then I looked at the actual empirical research...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>This does not take into account all the jobs t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>Use the health care data that is available to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>a</td>\n",
       "      <td>Itâ€™s not perfect by any means â€” this is a toug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>a</td>\n",
       "      <td>So just out of curiosity, I decided to look at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>The largest study to date of global comparison...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>Because, as I discovered recently, my mother n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>a</td>\n",
       "      <td>But covering all Americans through a single-pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>Family separation was largely thought of as a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>a</td>\n",
       "      <td>Not surprisingly, since more people own guns f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>a</td>\n",
       "      <td>That is the essence of democracy, and our demo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>a</td>\n",
       "      <td>This is what all credible analyses make clear:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>Why â€˜Medicare for Allâ€™ Would Be Bad News for E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>$11.74 billion a year in state and local taxes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>Successive generations also see massive gains ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>While we are far from the 2020 Democratic Conv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>a</td>\n",
       "      <td>It exploited a largely uncontested field to ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>a</td>\n",
       "      <td>But the drop in suicides was statistically sig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>During the hearings on H.R.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>Moreover, it made Tennessee a magnet for illeg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>Reimbursement rates for healthcare providers: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>Instead of doctors and nurses spending a signi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>The manner in which sheâ€™s handled the policy h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3660</th>\n",
       "      <td>3660</td>\n",
       "      <td>4</td>\n",
       "      <td>a</td>\n",
       "      <td>To them, thereâ€™s only one response to the ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3661</th>\n",
       "      <td>3661</td>\n",
       "      <td>5</td>\n",
       "      <td>a</td>\n",
       "      <td>Thereâ€™s also a more progressive version of thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3662</th>\n",
       "      <td>3662</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>The first piece of research is the National Ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3663</th>\n",
       "      <td>3663</td>\n",
       "      <td>4</td>\n",
       "      <td>a</td>\n",
       "      <td>Outrageous drug prices get negotiated down to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3664</th>\n",
       "      <td>3664</td>\n",
       "      <td>4</td>\n",
       "      <td>a</td>\n",
       "      <td>Then, Warren shows how a single-payer system, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3665</th>\n",
       "      <td>3665</td>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>However, the Justice Department, under the cur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3666</th>\n",
       "      <td>3666</td>\n",
       "      <td>5</td>\n",
       "      <td>a</td>\n",
       "      <td>Rural red states have huge power in the Senate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3667</th>\n",
       "      <td>3667</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>Economists Michael Clemens and Lant Pritchett ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3668</th>\n",
       "      <td>3668</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>Now that a new tragedy has reared its ugly hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3669</th>\n",
       "      <td>3669</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>New research reports that U.S. counties that s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3670</th>\n",
       "      <td>3670</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>In 2008, Bill Gates stated before Congress tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3671</th>\n",
       "      <td>3671</td>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>Medicare is the most popular health care progr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3672</th>\n",
       "      <td>3672</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>Are they continually updating their submitted ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3673</th>\n",
       "      <td>3673</td>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>Former vice president Joe Biden distinguished ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3674</th>\n",
       "      <td>3674</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>Just two weeks later, the House Judiciary Comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3675</th>\n",
       "      <td>3675</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>You are, in fact, three times more likely to b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3676</th>\n",
       "      <td>3676</td>\n",
       "      <td>5</td>\n",
       "      <td>a</td>\n",
       "      <td>To combat this behavior, some states have inst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3677</th>\n",
       "      <td>3677</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>I mean foolish stuff like preventing the CDC f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3678</th>\n",
       "      <td>3678</td>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>The law would allow state and local law enforc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3679</th>\n",
       "      <td>3679</td>\n",
       "      <td>4</td>\n",
       "      <td>a</td>\n",
       "      <td>â€œInsurance companies are not watching the stor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3680</th>\n",
       "      <td>3680</td>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>Securing our border isnâ€™t political theater; i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3681</th>\n",
       "      <td>3681</td>\n",
       "      <td>5</td>\n",
       "      <td>a</td>\n",
       "      <td>To fill in those gaping holes in what American...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3682</th>\n",
       "      <td>3682</td>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>Just remember, the last expansion of governmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3683</th>\n",
       "      <td>3683</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>But you brought it up.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3684</th>\n",
       "      <td>3684</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>It seems as though nothing has changed, save t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3685</th>\n",
       "      <td>3685</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>Despite this surge in gun ownership, the overa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3686</th>\n",
       "      <td>3686</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>At the same time, it would be a mistake to see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3687</th>\n",
       "      <td>3687</td>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>Anti-Immigrant Arguments Against Immigration R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3688</th>\n",
       "      <td>3688</td>\n",
       "      <td>5</td>\n",
       "      <td>a</td>\n",
       "      <td>Itâ€™s worth adding that the anti-gun position i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3689</th>\n",
       "      <td>3689</td>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>In 2020, NHE is expected to be about $3.5 tril...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3690 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  label alpha                                               text\n",
       "0        0      5     a  This reflects the decline of hunting as a spor...\n",
       "1        1      5     a  (Conversely, a 2014 Quinnipiac survey showed j...\n",
       "2        2      0     a  Even though the evidence of immigration assimi...\n",
       "3        3      3     a  It Will Make Wait Times Worse Medicare for All...\n",
       "4        4      1     a  (These inconvenient facts never get mentioned,...\n",
       "5        5      1     a  If only the Holocaust was the only example of ...\n",
       "6        6      0     a  Related to the welfare argument is the argumen...\n",
       "7        7      5     a  Then I looked at the actual empirical research...\n",
       "8        8      0     a  This does not take into account all the jobs t...\n",
       "9        9      3     a  Use the health care data that is available to ...\n",
       "10      10      5     a  Itâ€™s not perfect by any means â€” this is a toug...\n",
       "11      11      4     a  So just out of curiosity, I decided to look at...\n",
       "12      12      3     a  The largest study to date of global comparison...\n",
       "13      13      0     a  Because, as I discovered recently, my mother n...\n",
       "14      14      4     a  But covering all Americans through a single-pa...\n",
       "15      15      0     a  Family separation was largely thought of as a ...\n",
       "16      16      5     a  Not surprisingly, since more people own guns f...\n",
       "17      17      5     a  That is the essence of democracy, and our demo...\n",
       "18      18      4     a  This is what all credible analyses make clear:...\n",
       "19      19      3     a  Why â€˜Medicare for Allâ€™ Would Be Bad News for E...\n",
       "20      20      0     a  $11.74 billion a year in state and local taxes...\n",
       "21      21      0     a  Successive generations also see massive gains ...\n",
       "22      22      3     a  While we are far from the 2020 Democratic Conv...\n",
       "23      23      5     a  It exploited a largely uncontested field to ad...\n",
       "24      24      5     a  But the drop in suicides was statistically sig...\n",
       "25      25      1     a                        During the hearings on H.R.\n",
       "26      26      2     a  Moreover, it made Tennessee a magnet for illeg...\n",
       "27      27      3     a  Reimbursement rates for healthcare providers: ...\n",
       "28      28      3     a  Instead of doctors and nurses spending a signi...\n",
       "29      29      3     a  The manner in which sheâ€™s handled the policy h...\n",
       "...    ...    ...   ...                                                ...\n",
       "3660  3660      4     a  To them, thereâ€™s only one response to the ques...\n",
       "3661  3661      5     a  Thereâ€™s also a more progressive version of thi...\n",
       "3662  3662      0     a  The first piece of research is the National Ac...\n",
       "3663  3663      4     a  Outrageous drug prices get negotiated down to ...\n",
       "3664  3664      4     a  Then, Warren shows how a single-payer system, ...\n",
       "3665  3665      2     a  However, the Justice Department, under the cur...\n",
       "3666  3666      5     a  Rural red states have huge power in the Senate...\n",
       "3667  3667      0     a  Economists Michael Clemens and Lant Pritchett ...\n",
       "3668  3668      1     a  Now that a new tragedy has reared its ugly hea...\n",
       "3669  3669      0     a  New research reports that U.S. counties that s...\n",
       "3670  3670      0     a  In 2008, Bill Gates stated before Congress tha...\n",
       "3671  3671      3     a  Medicare is the most popular health care progr...\n",
       "3672  3672      1     a  Are they continually updating their submitted ...\n",
       "3673  3673      3     a  Former vice president Joe Biden distinguished ...\n",
       "3674  3674      1     a  Just two weeks later, the House Judiciary Comm...\n",
       "3675  3675      1     a  You are, in fact, three times more likely to b...\n",
       "3676  3676      5     a  To combat this behavior, some states have inst...\n",
       "3677  3677      1     a  I mean foolish stuff like preventing the CDC f...\n",
       "3678  3678      2     a  The law would allow state and local law enforc...\n",
       "3679  3679      4     a  â€œInsurance companies are not watching the stor...\n",
       "3680  3680      2     a  Securing our border isnâ€™t political theater; i...\n",
       "3681  3681      5     a  To fill in those gaping holes in what American...\n",
       "3682  3682      3     a  Just remember, the last expansion of governmen...\n",
       "3683  3683      1     a                             But you brought it up.\n",
       "3684  3684      1     a  It seems as though nothing has changed, save t...\n",
       "3685  3685      1     a  Despite this surge in gun ownership, the overa...\n",
       "3686  3686      0     a  At the same time, it would be a mistake to see...\n",
       "3687  3687      2     a  Anti-Immigrant Arguments Against Immigration R...\n",
       "3688  3688      5     a  Itâ€™s worth adding that the anti-gun position i...\n",
       "3689  3689      3     a  In 2020, NHE is expected to be about $3.5 tril...\n",
       "\n",
       "[3690 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(os.path.join(data_dir, 'train.tsv'), sep = \"\\t\", header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = L3_df.label.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tokenized_input = L3_df.text.apply(lambda x: tokenizer.encode(x,add_special_tokens = True, max_length = 128, pad_to_max_length = True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = tokenized_input[0]\n",
    "# l.extend([0] * (128 - len(l)))\n",
    "# len(l)\n",
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 1021,\n",
       " 3971,\n",
       " 7489,\n",
       " 4372,\n",
       " 13149,\n",
       " 2256,\n",
       " 4610,\n",
       " 1998,\n",
       " 2554,\n",
       " 2755,\n",
       " 1024,\n",
       " 7489,\n",
       " 9002,\n",
       " 2000,\n",
       " 3105,\n",
       " 4325,\n",
       " 1012,\n",
       " 102,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min([len(p) for p in tokenized_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attn_masks = []\n",
    "\n",
    "for sent in tokenized_input:\n",
    "    att_mask = [int(token > 0) for token in sent]\n",
    "    attn_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(x) for x in attn_masks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_ids = L3_df.id.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "\n",
    "all_input_ids = torch.tensor(all_input_ids)\n",
    "attn_masks = torch.tensor(attn_masks)\n",
    "tokenized_input = torch.tensor(tokenized_input)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "dataset = TensorDataset(tokenized_input, attn_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(tokenized_input, labels, random_state=2018, test_size=0.1)\n",
    "# Do the same for the masks.\n",
    "train_masks, validation_masks, _, _ = train_test_split(attn_masks, labels, random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 790, 4: 615, 3: 571, 1: 688, 2: 351, 5: 675})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 34, 1: 71, 5: 70, 0: 110, 3: 68, 4: 58})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  app.launch_new_instance()\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "# Convert to Tensors\n",
    "attn_masks = torch.tensor(attn_masks, dtype=torch.int32)\n",
    "tokenized_input = torch.tensor(tokenized_input, dtype=torch.int32)\n",
    "labels = torch.tensor(labels, dtype=torch.int32)\n",
    "    \n",
    "dataset = TensorDataset(tokenized_input, attn_masks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "sampler = RandomSampler(dataset)\n",
    "dataloader = DataLoader(dataset, sampler=sampler, batch_size=32)\n",
    "for step, batch in enumerate(dataloader):\n",
    "    print(batch[0].size()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Doing the Remainder of Postprocessing + Training in a .py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data path for the SageMaker PyTorch container. We don't need to create an own container. \n",
    "container_data_dir = '/opt/ml/input/data/training'\n",
    "container_model_dir = '/opt/ml/model'\n",
    "\n",
    "# input arguments for the training script and initial values for some hyperparameters\n",
    "parameters = {\n",
    "    'model_type': 'bert',\n",
    "    'model_name_or_path' : 'bert-base-uncased',\n",
    "#     'task_name': task_name,\n",
    "    'data_dir': container_data_dir,\n",
    "    'model_dir': container_model_dir,\n",
    "#     'num_train_epochs': 1,\n",
    "#     'per_gpu_train_batch_size': 64,\n",
    "#     'per_gpu_eval_batch_size': 64,\n",
    "#     'save_steps': 150,\n",
    "#     'logging_steps': 150,\n",
    "    'do_train': True,\n",
    "    'num_labels': 6\n",
    "#     'do_eval': True,\n",
    "#     'do_lower_case': True\n",
    "    # you can add more input arguments here\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazon SageMaker PyTorch framework\n",
    "\n",
    "train_instance_type = 'ml.p3.2xlarge'\n",
    "\n",
    "estimator = PyTorch(entry_point='L3_rebuild.py',\n",
    "                    source_dir = './train_scripts/', # the local directory stores all relevant scripts for modeling\n",
    "                    hyperparameters=parameters,\n",
    "                    role=role,\n",
    "                    framework_version='1.1.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type=train_instance_type\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-05 08:39:56 Starting - Starting the training job...\n",
      "2020-03-05 08:39:58 Starting - Launching requested ML instances...\n",
      "2020-03-05 08:40:56 Starting - Preparing the instances for training.........\n",
      "2020-03-05 08:42:16 Downloading - Downloading input data\n",
      "2020-03-05 08:42:16 Training - Downloading the training image...\n",
      "2020-03-05 08:42:45 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-03-05 08:42:46,571 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-03-05 08:42:46,595 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-03-05 08:42:46,596 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-03-05 08:42:46,827 sagemaker-containers INFO     Module L3_rebuild does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-03-05 08:42:46,827 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-03-05 08:42:46,827 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-03-05 08:42:46,827 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mCollecting tensorboardX (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\u001b[0m\n",
      "\u001b[34mCollecting scikit-learn (from -r requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/d8/312e03adf4c78663e17d802fe2440072376fee46cada1404f1727ed77a32/scikit_learn-0.22.2.post1-cp36-cp36m-manylinux1_x86_64.whl (7.1MB)\u001b[0m\n",
      "\u001b[34mCollecting transformers (from -r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (0.25.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r requirements.txt (line 1)) (1.16.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r requirements.txt (line 1)) (1.12.0)\u001b[0m\n",
      "\u001b[34mCollecting protobuf>=3.8.0 (from tensorboardX->-r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/57/02/5432412c162989260fab61fa65e0a490c1872739eb91a659896e4d554b26/protobuf-3.11.3-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mCollecting joblib>=0.11 (from scikit-learn->-r requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/ed/36/fd20c656fb4a4fbe8db367ea274c3465b81cb2e01ffc57b9980f0578e131/regex-2020.2.20-cp36-cp36m-manylinux1_x86_64.whl (690kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers->-r requirements.txt (line 3)) (4.33.0)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers==0.5.2 (from transformers->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers->-r requirements.txt (line 3)) (1.9.209)\u001b[0m\n",
      "\u001b[34mCollecting filelock (from transformers->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/93/83/71a2ee6158bb9f39a90c0dea1637f81d5eef866e188e1971a1b1ab01a35a/filelock-3.0.12-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers->-r requirements.txt (line 3)) (2.22.0)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece (from transformers->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses (from transformers->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 4)) (2019.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 4)) (2.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX->-r requirements.txt (line 1)) (41.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.13.0,>=1.12.209 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers->-r requirements.txt (line 3)) (1.12.209)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers->-r requirements.txt (line 3)) (0.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers->-r requirements.txt (line 3)) (0.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers->-r requirements.txt (line 3)) (2019.6.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers->-r requirements.txt (line 3)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers->-r requirements.txt (line 3)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers->-r requirements.txt (line 3)) (1.25.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers->-r requirements.txt (line 3)) (7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.209->boto3->transformers->-r requirements.txt (line 3)) (0.15.2)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: L3-rebuild, sacremoses\n",
      "  Running setup.py bdist_wheel for L3-rebuild: started\n",
      "  Running setup.py bdist_wheel for L3-rebuild: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-l_e44tft/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\n",
      "  Running setup.py bdist_wheel for sacremoses: started\u001b[0m\n",
      "\u001b[34m  Running setup.py bdist_wheel for sacremoses: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\u001b[0m\n",
      "\u001b[34mSuccessfully built L3-rebuild sacremoses\u001b[0m\n",
      "\u001b[34mInstalling collected packages: protobuf, tensorboardX, joblib, scikit-learn, regex, tokenizers, filelock, sentencepiece, sacremoses, transformers, L3-rebuild\u001b[0m\n",
      "\u001b[34mSuccessfully installed L3-rebuild-1.0.0 filelock-3.0.12 joblib-0.14.1 protobuf-3.11.3 regex-2020.2.20 sacremoses-0.0.38 scikit-learn-0.22.2.post1 sentencepiece-0.1.85 tensorboardX-2.0 tokenizers-0.5.2 transformers-2.5.1\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.0.2 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-03-05 08:42:54,345 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"model_type\": \"bert\",\n",
      "        \"do_train\": true,\n",
      "        \"model_dir\": \"/opt/ml/model\",\n",
      "        \"num_labels\": 6,\n",
      "        \"model_name_or_path\": \"bert-base-uncased\",\n",
      "        \"data_dir\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-pytorch-2020-03-05-08-39-56-385\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-496641494145/sagemaker-pytorch-2020-03-05-08-39-56-385/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"L3_rebuild\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"L3_rebuild.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"data_dir\":\"/opt/ml/input/data/training\",\"do_train\":true,\"model_dir\":\"/opt/ml/model\",\"model_name_or_path\":\"bert-base-uncased\",\"model_type\":\"bert\",\"num_labels\":6}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=L3_rebuild.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=L3_rebuild\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-496641494145/sagemaker-pytorch-2020-03-05-08-39-56-385/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"data_dir\":\"/opt/ml/input/data/training\",\"do_train\":true,\"model_dir\":\"/opt/ml/model\",\"model_name_or_path\":\"bert-base-uncased\",\"model_type\":\"bert\",\"num_labels\":6},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-pytorch-2020-03-05-08-39-56-385\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-496641494145/sagemaker-pytorch-2020-03-05-08-39-56-385/source/sourcedir.tar.gz\",\"module_name\":\"L3_rebuild\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"L3_rebuild.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--data_dir\",\"/opt/ml/input/data/training\",\"--do_train\",\"True\",\"--model_dir\",\"/opt/ml/model\",\"--model_name_or_path\",\"bert-base-uncased\",\"--model_type\",\"bert\",\"--num_labels\",\"6\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_TYPE=bert\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_LABELS=6\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=bert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_DATA_DIR=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m L3_rebuild --data_dir /opt/ml/input/data/training --do_train True --model_dir /opt/ml/model --model_name_or_path bert-base-uncased --model_type bert --num_labels 6\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mPreprocessing Goes here\u001b[0m\n",
      "\u001b[34mDirectory: /opt/ml/input/data/training\n",
      "   id  label alpha                                               text\u001b[0m\n",
      "\u001b[34m0   0      5     a  This reflects the decline of hunting as a spor...\u001b[0m\n",
      "\u001b[34m1   1      5     a  (Conversely, a 2014 Quinnipiac survey showed j...\u001b[0m\n",
      "\u001b[34m2   2      0     a  Even though the evidence of immigration assimi...\u001b[0m\n",
      "\u001b[34m3   3      3     a  It Will Make Wait Times Worse Medicare for All...\u001b[0m\n",
      "\u001b[34m4   4      1     a  (These inconvenient facts never get mentioned,...\u001b[0m\n",
      "\u001b[34mLoading BERT tokenizer...\u001b[0m\n",
      "\u001b[34mCreating Attention Masks\u001b[0m\n",
      "\u001b[34mCreated train dataset and loader\u001b[0m\n",
      "\u001b[34mPreprocessing Goes here\u001b[0m\n",
      "\u001b[34mDirectory: /opt/ml/input/data/training\n",
      "     id  label alpha                                               text\u001b[0m\n",
      "\u001b[34m0  3690      5     a  As Cruz wrote, â€œThe Framers were understandabl...\u001b[0m\n",
      "\u001b[34m1  3691      2     a  But given the scope of Third World poverty, ma...\u001b[0m\n",
      "\u001b[34m2  3692      0     a  Of course they can, as that is the essence of ...\u001b[0m\n",
      "\u001b[34m3  3693      0     a  The Netherlands is a great place to live, but ...\u001b[0m\n",
      "\u001b[34m4  3694      4     a  Hudson worries that even a small out-of-pocket...\u001b[0m\n",
      "\u001b[34mLoading BERT tokenizer...\u001b[0m\n",
      "\u001b[34mCreating Attention Masks\u001b[0m\n",
      "\u001b[34mCreated dev dataset and loader\u001b[0m\n",
      "\u001b[34mTraining Goes here\n",
      "\u001b[0m\n",
      "\u001b[34m======== Epoch 1 / 4 ========\u001b[0m\n",
      "\u001b[34mTraining...\u001b[0m\n",
      "\u001b[34m  Batch    40  of    116.    Elapsed: 0:00:11.\u001b[0m\n",
      "\u001b[34m  Batch    80  of    116.    Elapsed: 0:00:22.\u001b[0m\n",
      "\u001b[34m  Average training loss: 1.26\n",
      "  Training epcoh took: 0:00:31\n",
      "\u001b[0m\n",
      "\u001b[34mRunning Validation...\u001b[0m\n",
      "\u001b[34m  Accuracy: 0.59\n",
      "  Validation took: 0:00:01\n",
      "\u001b[0m\n",
      "\u001b[34m======== Epoch 2 / 4 ========\u001b[0m\n",
      "\u001b[34mTraining...\u001b[0m\n",
      "\u001b[34m  Batch    40  of    116.    Elapsed: 0:00:11.\u001b[0m\n",
      "\u001b[34m  Batch    80  of    116.    Elapsed: 0:00:22.\u001b[0m\n",
      "\u001b[34m  Average training loss: 0.82\n",
      "  Training epcoh took: 0:00:31\n",
      "\u001b[0m\n",
      "\u001b[34mRunning Validation...\u001b[0m\n",
      "\u001b[34m  Accuracy: 0.64\n",
      "  Validation took: 0:00:01\n",
      "\u001b[0m\n",
      "\u001b[34m======== Epoch 3 / 4 ========\u001b[0m\n",
      "\u001b[34mTraining...\u001b[0m\n",
      "\u001b[34m  Batch    40  of    116.    Elapsed: 0:00:11.\u001b[0m\n",
      "\u001b[34m  Batch    80  of    116.    Elapsed: 0:00:22.\u001b[0m\n",
      "\u001b[34m  Average training loss: 0.65\n",
      "  Training epcoh took: 0:00:31\n",
      "\u001b[0m\n",
      "\u001b[34mRunning Validation...\u001b[0m\n",
      "\u001b[34m  Accuracy: 0.65\n",
      "  Validation took: 0:00:01\n",
      "\u001b[0m\n",
      "\u001b[34m======== Epoch 4 / 4 ========\u001b[0m\n",
      "\u001b[34mTraining...\u001b[0m\n",
      "\u001b[34m  Batch    40  of    116.    Elapsed: 0:00:11.\u001b[0m\n",
      "\u001b[34m  Batch    80  of    116.    Elapsed: 0:00:22.\u001b[0m\n",
      "\u001b[34m  Average training loss: 0.54\n",
      "  Training epcoh took: 0:00:31\n",
      "\u001b[0m\n",
      "\u001b[34mRunning Validation...\u001b[0m\n",
      "\u001b[34m  Accuracy: 0.65\n",
      "  Validation took: 0:00:01\n",
      "\u001b[0m\n",
      "\u001b[34mTraining complete!\u001b[0m\n",
      "\u001b[34msaved model path\u001b[0m\n",
      "\u001b[34mEvaluate on Test Dataset!\u001b[0m\n",
      "\u001b[34mTokenize testing, like training\u001b[0m\n",
      "\u001b[34mNow create attention masks too\u001b[0m\n",
      "\u001b[34mCreated test dataset and loader\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m[126, 166, 88, 174, 42, 27, 24, 79, 46, 124, 135, 124, 85, 135, 118, 132, 162, 150, 180, 171, 33, 43, 126, 55, 24, 112, 76, 129, 28, 42, 18, 0, 187, 159, 157, 39, 96, 175, 9, 45, 22, 64, 39, 106, 43, 97, 40, 94, 147, 22]\u001b[0m\n",
      "\u001b[34mBiden : stance on immigration: 1.0 .... Relative Importance: 0.02\u001b[0m\n",
      "\u001b[34mBiden :stance on guns: 0.5 .... Relative Importance: 0.0\u001b[0m\n",
      "\u001b[34mBiden : stance on medicare: 0.5 .... Relative Importance: 0.0\u001b[0m\n",
      "\u001b[34mEvaluate on Test Dataset!\u001b[0m\n",
      "\u001b[34mTokenize testing, like training\u001b[0m\n",
      "\u001b[34mNow create attention masks too\u001b[0m\n",
      "\u001b[34mCreated test dataset and loader\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m[181, 0, 0, 126, 29, 18, 71, 1, 30, 72, 59, 127, 64, 0, 179, 124, 168, 141, 78, 114, 54, 64, 126, 36, 175, 53, 101, 24, 41, 153, 130, 121, 153, 6, 124, 70, 163, 36]\u001b[0m\n",
      "\u001b[34mSanders : stance on immigration: 0.75 .... Relative Importance: 0.10526315789473684\u001b[0m\n",
      "\u001b[34mSanders :stance on guns: 0.5 .... Relative Importance: 0.0\u001b[0m\n",
      "\u001b[34mSanders : stance on medicare: 0.5 .... Relative Importance: 0.0\u001b[0m\n",
      "\u001b[34mEvaluate on Test Dataset!\u001b[0m\n",
      "\u001b[34mTokenize testing, like training\u001b[0m\n",
      "\u001b[34mNow create attention masks too\u001b[0m\n",
      "\u001b[34mCreated test dataset and loader\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m[172, 186, 3, 132, 168, 60, 78, 22, 120, 175, 48, 94, 160, 34, 60, 75, 90, 24, 129, 45, 55, 54, 3, 144, 36, 105, 162, 0, 129, 87, 54]\u001b[0m\n",
      "\u001b[34mWarren : stance on immigration: 1.0 .... Relative Importance: 0.03225806451612903\u001b[0m\n",
      "\u001b[34mWarren :stance on guns: 0.0 .... Relative Importance: 0.06451612903225806\u001b[0m\n",
      "\u001b[34mWarren : stance on medicare: 0.5 .... Relative Importance: 0.0\u001b[0m\n",
      "\u001b[34mEvaluate on Test Dataset!\u001b[0m\n",
      "\u001b[34mTokenize testing, like training\u001b[0m\n",
      "\u001b[34mNow create attention masks too\u001b[0m\n",
      "\u001b[34mCreated test dataset and loader\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m[72, 102, 78, 121, 126, 96, 0, 12, 21, 184, 162, 138, 13, 36, 24, 21, 78, 22, 18, 31, 78, 12, 142, 174, 162, 6, 138, 60, 42, 156, 162, 52]\u001b[0m\n",
      "\u001b[34mYang : stance on immigration: 1.0 .... Relative Importance: 0.03125\u001b[0m\n",
      "\u001b[34mYang :stance on guns: 0.5 .... Relative Importance: 0.0\u001b[0m\n",
      "\u001b[34mYang : stance on medicare: 0.5 .... Relative Importance: 0.0\u001b[0m\n",
      "\u001b[34mEvaluate on Test Dataset!\u001b[0m\n",
      "\u001b[34mTokenize testing, like training\u001b[0m\n",
      "\u001b[34mNow create attention masks too\u001b[0m\n",
      "\u001b[34mCreated test dataset and loader\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m[76, 103, 66, 186, 167, 105, 39, 162, 157, 20, 30, 168, 138, 84, 99, 0, 94, 31, 151, 37, 85, 132, 84, 109, 174, 55, 75, 106, 48, 15, 84, 52]\u001b[0m\n",
      "\u001b[34mButtigieg : stance on immigration: 1.0 .... Relative Importance: 0.03125\u001b[0m\n",
      "\u001b[34mButtigieg :stance on guns: 0.5 .... Relative Importance: 0.0\u001b[0m\n",
      "\u001b[34mButtigieg : stance on medicare: 0.5 .... Relative Importance: 0.0\u001b[0m\n",
      "\u001b[34mEvaluate on Test Dataset!\u001b[0m\n",
      "\u001b[34mTokenize testing, like training\u001b[0m\n",
      "\u001b[34mNow create attention masks too\u001b[0m\n",
      "\u001b[34mCreated test dataset and loader\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34m[115, 108, 55, 144, 54, 72, 133, 139, 139, 97, 150, 157, 79, 102, 46, 1, 15, 99, 115, 13, 46, 132, 6, 12, 30, 73, 129, 51, 107, 36, 150, 113, 19, 37, 191, 138, 121, 60]\u001b[0m\n",
      "\u001b[34mKlobuchar : stance on immigration: 0.0 .... Relative Importance: 0.02631578947368421\u001b[0m\n",
      "\u001b[34mKlobuchar :stance on guns: 0.5 .... Relative Importance: 0.0\u001b[0m\n",
      "\u001b[34mKlobuchar : stance on medicare: 0.5 .... Relative Importance: 0.0\u001b[0m\n",
      "\u001b[34m2020-03-05 08:46:04,765 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-03-05 08:46:07 Uploading - Uploading generated training model\n",
      "2020-03-05 08:47:04 Completed - Training job completed\n",
      "Training seconds: 301\n",
      "Billable seconds: 301\n"
     ]
    }
   ],
   "source": [
    "# launch model training job\n",
    "input_data = 's3://sagemaker-us-west-2-496641494145/L3'\n",
    "estimator.fit({'training': input_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make some Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int32"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_input.dtype\n",
    "labels.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example  = dev_df[['id','alpha','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\n<title>500 Internal Server Error</title>\n<h1>Internal Server Error</h1>\n<p>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.</p>\n\". See https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/sagemaker-pytorch-2020-03-04-06-29-51-608 in account 496641494145 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-5b1f9468316c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'alpha'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args, target_model)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mrequest_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_request_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    315\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\n<title>500 Internal Server Error</title>\n<h1>Internal Server Error</h1>\n<p>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.</p>\n\". See https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/sagemaker-pytorch-2020-03-04-06-29-51-608 in account 496641494145 for more information."
     ]
    }
   ],
   "source": [
    "predictor.predict(dev_df[['id','alpha','text']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out how to run this on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = [0.59, 0.64, 0.65, 0.65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax([1,2,34,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
